<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Zu3zz.github.io</id>
    <title>zz失乐园</title>
    <updated>2019-09-13T11:58:16.840Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Zu3zz.github.io"/>
    <link rel="self" href="https://Zu3zz.github.io/atom.xml"/>
    <subtitle>Everyone Can (Not) Comprehend.</subtitle>
    <logo>https://Zu3zz.github.io/images/avatar.png</logo>
    <icon>https://Zu3zz.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, zz失乐园</rights>
    <entry>
        <title type="html"><![CDATA[HDFS系列(1)----安装]]></title>
        <id>https://Zu3zz.github.io/post/hdfs-3</id>
        <link href="https://Zu3zz.github.io/post/hdfs-3">
        </link>
        <updated>2019-09-13T10:54:04.000Z</updated>
        <summary type="html"><![CDATA[<p>这篇文章咱们就来讲讲如何安装HDFS</p>
]]></summary>
        <content type="html"><![CDATA[<p>这篇文章咱们就来讲讲如何安装HDFS</p>
<!-- more -->
<h1 id="hdfs-概述">HDFS 概述</h1>
<ol>
<li>分布式</li>
<li>commodity hardware</li>
<li>fault-tolerant 容错</li>
<li>high throughput</li>
<li>large data sets</li>
</ol>
<h2 id="hdfs-是一个分布式的文件系统">HDFS 是一个分布式的文件系统</h2>
<ul>
<li>文件系统：Linux、Windows、Mac....
<ul>
<li>目录结构: C /</li>
<li>存放的是文件或者文件夹</li>
<li>对外提供服务：创建、修改、删除、查看、移动等等</li>
<li>普通文件系统 vs 分布式文件系统
<ul>
<li>单机</li>
<li>分布式文件系统能够横跨 N 个机器</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="hdfs-前提和设计目标">HDFS 前提和设计目标</h2>
<ul>
<li>Hardware Failure 硬件错误
<ul>
<li>每个机器只存储文件的部分数据，blocksize=128M</li>
<li>block 存放在不同的机器上的，由于容错，HDFS 默认采用 3 副本机制</li>
</ul>
</li>
<li>Streaming Data Access 流式数据访问
<ul>
<li>The emphasis is on high throughput of data access</li>
<li>rather than low latency of data access.</li>
</ul>
</li>
<li>Large Data Sets 大规模数据集</li>
<li>Moving Computation is Cheaper than Moving Data 移动计算比移动数据更划算</li>
</ul>
<h2 id="hdfs-的架构">HDFS 的架构</h2>
<ol>
<li>NameNode(master) and DataNodes(slave)</li>
<li>master/slave 的架构</li>
<li>NN:</li>
</ol>
<ul>
<li>the file system namespace
<ul>
<li>/home/hadoop/software</li>
<li>/home/hadoop/app</li>
<li>regulates access to files by clients</li>
</ul>
</li>
</ul>
<ol start="4">
<li>
<p>DN：storage</p>
</li>
<li>
<p>HDFS exposes a file system namespace and allows user data to be stored in files.</p>
</li>
<li>
<p>a file is split into one or more blocks</p>
<ul>
<li>blocksize: 128M</li>
<li>150M 拆成 2 个 block</li>
</ul>
</li>
<li>
<p>blocks are stored in a set of DataNodes</p>
<ul>
<li>为什么？ 容错！！！</li>
</ul>
</li>
<li>
<p>NameNode executes file system namespace operations：CRUD</p>
</li>
<li>
<p>determines the mapping of blocks to DataNodes</p>
<ul>
<li>
<p>a.txt 150M blocksize=128M</p>
</li>
<li>
<p>a.txt 拆分成 2 个 block 一个是 block1：128M 另一个是 block2：22M<br>
block1 存放在哪个 DN？block2 存放在哪个 DN？</p>
</li>
<li>
<p>a.txt<br>
_ block1：128M, 192.168.199.1<br>
_ block2：22M, 192.168.199.2<br>
_ get a.txt<br>
_ 这个过程对于用户来说是不感知的</p>
</li>
</ul>
</li>
<li>
<p>通常情况下：1 个 Node 部署一个组件</p>
</li>
</ol>
<h2 id="课程环境介绍">课程环境介绍：</h2>
<h3 id="本课程录制的系统是-mac所以我采用的-linux-客户端是-mac-自带的-shell">本课程录制的系统是 Mac，所以我采用的 linux 客户端是 mac 自带的 shell</h3>
<ul>
<li>如果你们是 win：xshell、crt</li>
<li>服务器/linux 地址：192.168.199.233</li>
<li>连接到 linux 环境<br>
_ 登陆：ssh hadoop@192.168.199.233<br>
_ 登陆成功以后：[hadoop@hadoop000 ~]$
<ul>
<li>linux机器：用户名hadoop、密码123456、hostname是* * hadoop000</li>
<li>创建课程中所需要的目录（合适的文件存放在合适的目录）
<ol>
<li>[hadoop@hadoop000 ~]$ mkdir software 存放课程所使用的软件安装包 2. [hadoop@hadoop000 ~]$ mkdir app       存放课程所有软件的安装目录</li>
<li>[hadoop@hadoop000 ~]$ mkdir data 存放课程中使用的数据 4. [hadoop@hadoop000 ~]$ mkdir lib       存放课程中开发过的作业jar存放的目录</li>
<li>[hadoop@hadoop000 ~]$ mkdir shell 存放课程中相关的脚本 6. [hadoop@hadoop000 ~]$ mkdir maven_resp 存放课程中使用到的 maven 依赖包存放的目录</li>
</ol>
</li>
</ul>
</li>
<li>学员问：root 密码 1. 切换 hadoop 到 root 用户：[hadoop@hadoop000 ~]$ sudo -i 2. 切换 root 到 hadoop 用户：[root@hadoop000 ~]# su hadoop 3. 我 OOTB 环境中创建的 hadoop 用户是有 sudo 权限：sudo vi /etc/hosts</li>
<li>Linux 版本： * 以前的课程是 centos6.4，本次课程升级成 centos7</li>
</ul>
<h3 id="hadoop-环境搭建">Hadoop 环境搭建</h3>
<ul>
<li>使用的 Hadoop 相关版本：CDH</li>
<li>CDH 相关软件包下载地址：http://archive.cloudera.com/cdh5/cdh/5/</li>
<li>Hadoop 使用版本：hadoop-2.6.0-cdh5.15.1</li>
<li>Hadoop 下载：wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</li>
<li>Hive 使用版本：hive-1.1.0-cdh5.15.1</li>
</ul>
<h3 id="hadoophivespark-相关框架的学习">Hadoop/Hive/Spark 相关框架的学习：</h3>
<ul>
<li>
<p>使用单机版足够</p>
</li>
<li>
<p>如果使用集群学习会导致：从入门到放弃</p>
</li>
<li>
<p>使用 Linux/Mac 学习</p>
</li>
<li>
<p>一定不要使用 Windows 搭建 Hadoop 环境</p>
</li>
<li>
<p>所以 Linux 基础是要会的</p>
</li>
</ul>
<h3 id="hadoop-安装前置要求">Hadoop 安装前置要求</h3>
<ul>
<li>Java 1.8+</li>
<li>ssh</li>
</ul>
<h3 id="安装-java">安装 Java</h3>
<ul>
<li>拷贝本地软件包到服务器：scp jdk-8u91-linux-x64.tar.gz hadoop@192.168.199.233:~/software/</li>
<li>解压 jdk 到~/app/：tar -zvxf jdk-8u91-linux-x64.tar.gz -C ~/app/</li>
<li>把 jdk 配置系统环境变量中： ~/.bash_profile - export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91 - export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mi>A</mi><mi>V</mi><msub><mi>A</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">JAVA_HOME/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH</li>
<li>使得配置修改生效：source .bash_profile</li>
<li>验证：java -version</li>
</ul>
<h3 id="安装-ssh-无密码登陆">安装 ssh 无密码登陆</h3>
<ul>
<li>
<p>ls</p>
</li>
<li>
<p>ls -a</p>
</li>
<li>
<p>ls -la 并没有发现一个.ssh 的文件夹</p>
</li>
<li>
<p>ssh-keygen -t rsa 一路回车</p>
</li>
<li>
<p>cd ~/.ssh</p>
</li>
<li>
<p>[hadoop@hadoop000 .ssh]$ ll</p>
</li>
<li>
<p>总用量 12</p>
<pre><code class="language-shell">-rw------- 1 hadoop hadoop 1679 10月 15 02:54 id_rsa  私钥
-rw-r--r-- 1 hadoop hadoop  398 10月 15 02:54 id_rsa.pub 公钥
-rw-r--r-- 1 hadoop hadoop  358 10月 15 02:54 known_hosts
cat id_rsa.pub &gt;&gt; authorized_keys
chmod 600 authorized_keys
</code></pre>
</li>
</ul>
<h3 id="hadoophdfs安装">Hadoop(HDFS)安装</h3>
<ul>
<li>
<p>下载<br>
_ 解压：~/app<br>
_ 添加 HADOOP_HOME/bin 到系统环境变量<br>
_ 修改 Hadoop 配置文件<br>
_ hadoop-env.sh<br>
_ export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91<br>
_ core-site.xml</p>
<pre><code class="language-xml">  &lt;property&gt;
      &lt;name&gt;fs.defaultFS&lt;/name&gt;
      &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
  &lt;/property&gt;

  hdfs-site.xml
  &lt;property&gt;
      &lt;name&gt;dfs.replication&lt;/name&gt;
      &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
<ul>
<li>slaves<br>
_ hadoop000<br>
_ 启动 HDFS：<br>
_ 第一次执行的时候一定要格式化文件系统，不要重复执行: hdfs namenode -format<br>
_ 启动集群：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mi>A</mi><mi>D</mi><mi>O</mi><mi>O</mi><msub><mi>P</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi><mi mathvariant="normal">/</mi><mi>s</mi><mi>b</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">/</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi><mo>−</mo><mi>d</mi><mi>f</mi><mi>s</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mo>∗</mo><mi mathvariant="normal">验</mi><mi mathvariant="normal">证</mi><mo>:</mo><mn>1.</mn><mo>[</mo><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">@</mi><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mn>000</mn><mi>s</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">HADOOP_HOME/sbin/start-dfs.sh
	* 验证:
		1. [hadoop@hadoop000 sbin]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">s</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord cjk_fallback">验</span><span class="mord cjk_fallback">证</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mopen">[</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">@</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord mathdefault">s</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span> jps 2. 60002 DataNode 3. 60171 SecondaryNameNode 4. 59870 NameNode 5. http://192.168.199.233:50070 6. 如果发现 jps ok，但是浏览器不 OK？ 十有八九是防火墙问题 7. 查看防火墙状态：sudo firewall-cmd --state 8. 关闭防火墙: sudo systemctl stop firewalld.service 9. 进制防火墙开机启动：</li>
</ul>
</li>
</ul>
<h3 id="hadoop-软件包常见目录说明">hadoop 软件包常见目录说明</h3>
<ul>
<li>bin：hadoop 客户端名单</li>
<li>etc/hadoop：hadoop 相关的配置文件存放目录</li>
<li>sbin：启动 hadoop 相关进程的脚本</li>
<li>share：常用例子</li>
</ul>
<h3 id="注意">注意：</h3>
<ul>
<li>
<p>start/stop-dfs.sh 与 hadoop-daemons.sh 的关系</p>
</li>
<li>
<p>start-dfs.sh</p>
<pre><code class="language-shell">hadoop-daemons.sh start namenode
hadoop-daemons.sh start datanode
hadoop-daemons.sh start secondarynamenode
</code></pre>
</li>
<li>
<p>stop-dfs.sh =</p>
</li>
<li>
<p>hadoop 常用命令：</p>
<ol>
<li>hadoop fs -ls /</li>
<li>hadoop fs -put</li>
<li>hadoop fs -copyFromLocal</li>
<li>hadoop fs -moveFromLocal</li>
<li>hadoop fs -cat</li>
<li>hadoop fs -text</li>
<li>hadoop fs -get</li>
<li>hadoop fs -mkdir</li>
<li>hadoop fs -mv 移动/改名</li>
<li>hadoop fs -getmerge</li>
<li>hadoop fs -rm</li>
<li>hadoop fs -rmdir</li>
<li>hadoop fs -rm -r</li>
</ol>
</li>
</ul>
<h3 id="hdfs-存储扩展">HDFS 存储扩展：</h3>
<ul>
<li>put: 1file ==&gt; 1...n block ==&gt; 存放在不同的节点上的</li>
<li>get: 去 nn 上查找这个 file 对应的元数据信息</li>
<li>了解底层的存储机制这才是我们真正要学习的东西，掌握 API 那是毛毛雨</li>
</ul>
<h3 id="使用-hdfs-api-的方式来操作-hdfs-文件系统">使用 HDFS API 的方式来操作 HDFS 文件系统</h3>
<ul>
<li>IDEA/Eclipse</li>
<li>Java
<ul>
<li>使用 Maven 来管理项目</li>
<li>拷贝 jar 包</li>
<li>我的所有课程都是使用 maven 来进行管理的</li>
</ul>
</li>
</ul>
<pre><code class="language-shell">Caused by: org.apache.hadoop.ipc.RemoteException
(org.apache.hadoop.security.AccessControlException):
Permission denied: user=rocky, access=WRITE,
inode=&quot;/&quot;:hadoop:supergroup:drwxr-xr-x
</code></pre>
<h3 id="hdfs-操作shell-java-api">HDFS 操作：shell + Java API</h3>
<ul>
<li>综合性的 HDFS 实战：使用 HDFS Java API 才完成 HDFS 文件系统上的文件的词频统计</li>
<li>词频统计：wordcount
<ul>
<li>/path/1.txt</li>
<li>hello world hello</li>
<li>/path/2.txt</li>
<li>hello world hello
<ul>
<li>==&gt; (hello,4) (world,2)</li>
</ul>
</li>
</ul>
</li>
<li>将统计完的结果输出到 HDFS 上去。</li>
</ul>
<h3 id="假设有的小伙伴了解过-mr-spark-等等觉得这个操作很简单">假设：有的小伙伴了解过 mr、spark 等等，觉得这个操作很简单</h3>
<h3 id="本实战的要求只允许使用-hdfs-api-进行操作">本实战的要求：只允许使用 HDFS API 进行操作</h3>
<h3 id="目的">目的</h3>
<ol>
<li>掌握 HDFS API 的操作</li>
<li>通过这个案例，让你们对于后续要学习的 mr 有一个比较好的认识</li>
</ol>
<h3 id="硬编码-非常忌讳的">硬编码 ： 非常忌讳的</h3>
<h3 id="可配置">==&gt; 可配置</h3>
<h3 id="可插拔的开发管理方式-plugin">可插拔的开发/管理方式 plugin</h3>
<h3 id="副本摆放策略">副本摆放策略</h3>
<ul>
<li>
<p>1-本 rack 的一个节点上</p>
</li>
<li>
<p>2-另外一个 rack 的节点上</p>
</li>
<li>
<p>3-与 2 相同的 rack 的另外一个节点上</p>
</li>
<li>
<p>1-本 rack 的一个节点上</p>
</li>
<li>
<p>2-本 rack 的另外一个节点上</p>
</li>
<li>
<p>3-不同 rack 的一个节点上</p>
</li>
</ul>
<h3 id="hdfs-的元数据管理">HDFS 的元数据管理</h3>
<ul>
<li>元数据：HDFS 的目录结构以及每个文件的 BLOCK 信息(id，副本系数、block 存放在哪个 DN 上)</li>
<li>存在什么地方：对应配置 ${hadoop.tmp.dir}/name/......</li>
<li>元数据存放在文件中：</li>
</ul>
<pre><code class="language-shell">/test1
/test1/a.txt
/test2
/test2/1.txt
/test2/2.txt
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yarn系列(1)----基础、介绍、流程]]></title>
        <id>https://Zu3zz.github.io/post/yarn-1</id>
        <link href="https://Zu3zz.github.io/post/yarn-1">
        </link>
        <updated>2019-09-10T07:31:48.000Z</updated>
        <summary type="html"><![CDATA[<p>简单介绍一下资源调度工具YARN<br>
Yet Another Resource Negotiator</p>
]]></summary>
        <content type="html"><![CDATA[<p>简单介绍一下资源调度工具YARN<br>
Yet Another Resource Negotiator</p>
<!-- more -->
<h1 id="yarn-产生背景">YARN 产生背景</h1>
<ul>
<li>
<p>MapReduce1.x ==&gt; MapReduce2.x</p>
<ul>
<li>master/slave : JobTracker/TaskTracker</li>
<li>JobTracker：单点、压力大</li>
<li>仅仅只能够支持 mapreduce 作业</li>
</ul>
</li>
<li>
<p>资源利用率</p>
<ul>
<li>所有的计算框架运行在一个集群中，共享一个集群的资源，按需分配！</li>
</ul>
</li>
</ul>
<pre><code class="language-txt">master: resource management：ResourceManager (RM)
job scheduling/monitoring：per-application ApplicationMaster (AM)
slave: NodeManager (NM)
</code></pre>
<h2 id="yarn-架构">YARN 架构</h2>
<ul>
<li>Client、ResourceManager、NodeManager、ApplicationMaster</li>
<li>master/slave: RM/NM</li>
</ul>
<h2 id="client-向-rm-提交任务-杀死任务等">Client: 向 RM 提交任务、杀死任务等</h2>
<ul>
<li>
<p>ApplicationMaster：</p>
<ul>
<li>每个应用程序对应一个 AM</li>
<li>AM 向 RM 申请资源用于在 NM 上启动对应的 Task</li>
<li>数据切分</li>
<li>为每个 task 向 RM 申请资源（container）</li>
<li>NodeManager 通信</li>
<li>任务的监控</li>
</ul>
</li>
<li>
<p>NodeManager： 多个</p>
<ul>
<li>干活</li>
<li>向 RM 发送心跳信息、任务的执行情况</li>
<li>接收来自 RM 的请求来启动任务</li>
<li>处理来自 AM 的命令</li>
</ul>
</li>
<li>
<p>ResourceManager:集群中同一时刻对外提供服务的只有 1 个，负责资源相关</p>
<ul>
<li>处理来自客户端的请求：提交、杀死</li>
<li>启动/监控 AM</li>
<li>监控 NM</li>
<li>资源相关</li>
</ul>
</li>
<li>
<p>container：任务的运行抽象</p>
<ul>
<li>memory、cpu....</li>
<li>task 是运行在 container 里面的</li>
<li>可以运行 am、也可以运行 map/reduce task</li>
</ul>
</li>
</ul>
<h2 id="yarn-执行流程">yarn 执行流程</h2>
<p><img src="https://Zu3zz.github.io/post-images/1568100871914.png" alt="yarn执行流程"></p>
<h2 id="提交自己开发的-mr-作业到-yarn-上运行的步骤">提交自己开发的 MR 作业到 YARN 上运行的步骤：</h2>
<ol>
<li>mvn clean package -DskipTests 打包 jar 包<br>
windows/Mac/Linux ==&gt; Maven</li>
<li>把编译出来的 jar 包(项目根目录/target/...jar)以及测试数据上传到服务器<br>
scp xxxx hadoop@hostname:directory</li>
<li>把数据上传到 HDFS<br>
hadoop fs -put xxx hdfspath</li>
<li>执行作业<br>
hadoop jar xxx.jar 完整的类名(包名+类名) args.....</li>
<li>到 YARN UI(8088) 上去观察作业的运行情况</li>
<li>到输出目录去查看对应的输出结果</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hadoop系列(1)----多集群配置流程]]></title>
        <id>https://Zu3zz.github.io/post/hadoop-1</id>
        <link href="https://Zu3zz.github.io/post/hadoop-1">
        </link>
        <updated>2019-09-09T13:12:27.000Z</updated>
        <summary type="html"><![CDATA[<p>终于到了激动人心的伪(划掉分布式环节 如何将HDFS配置在多机器的集群上<br>
这篇文章会详细告诉你~</p>
]]></summary>
        <content type="html"><![CDATA[<p>终于到了激动人心的伪(划掉分布式环节 如何将HDFS配置在多机器的集群上<br>
这篇文章会详细告诉你~</p>
<!-- more -->
<h1 id="hadoop-集群规划">Hadoop 集群规划</h1>
<ul>
<li>
<p>HDFS: NN DN</p>
</li>
<li>
<p>YARN: RM NM</p>
</li>
<li>
<p>hadoop000 192.168.199.234</p>
<ul>
<li>NN RM</li>
<li>DN NM</li>
</ul>
</li>
<li>
<p>hadoop001 192.168.199.235</p>
<ul>
<li>DN NM</li>
</ul>
</li>
<li>
<p>hadoop002 192.168.199.236</p>
<ul>
<li>DN NM</li>
</ul>
</li>
</ul>
<h2 id="详细步骤">详细步骤</h2>
<ul>
<li>
<p>对每台机器</p>
<ul>
<li>
<p>修改 host 配置</p>
<ul>
<li>
<p>在/etc/hostname 下修改 hostname(hadoop000/hadoop001/hadoop002)</p>
</li>
<li>
<p>在/etc/hosts 下修改 ip 和 hostname 的映射关系</p>
<ol>
<li>192.168.199.234 hadoop000</li>
<li>192.168.199.235 hadoop001</li>
<li>192.168.199.236 hadoop002</li>
<li>192.168.199.23x localhost(试机器而定)</li>
</ol>
</li>
</ul>
</li>
<li>
<p>前置安装 ssh 进行免密码登录操作</p>
</li>
</ul>
<pre><code class="language-shell">ssh-keygen -t rsa
</code></pre>
<p>在每台 hadoop 机器上进行操作</p>
<pre><code class="language-shell">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002
</code></pre>
<ul>
<li>
<p>安装 java jkd</p>
<ul>
<li>首先在每台 hadoop 集群机器上部署 jdk</li>
<li>将 jkd bin 配置到系统环境变量(~/.bash_profile)</li>
<li>将 jdk 以及环境变量配置拷贝到其他节点上去(start with hadoop000)</li>
</ul>
<pre><code class="language-shell">scp -r jdk1.8.0_91 hadoop@hadoop001:~/app/
scp -r jdk1.8.0_91 hadoop@hadoop002:~/app/

scp ~/.bash_profile hadoop@hadoop001:~/
scp ~/.bash_profile hadoop@hadoop002:~/
</code></pre>
</li>
<li>
<p>Hadoop 部署</p>
<ul>
<li>hadoop-env.sh 配置: JAVA_HOME</li>
<li>hdfs-site.xml 配置:</li>
</ul>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/app/tmp/dfs/name&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/app/tmp/dfs/data&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>yarn-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;!--只在hadoop000机器中进行配置--&gt;
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;hadoop000&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>mapred-site.xml: 这个文件只有模板 需要自己创建</li>
</ul>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
  &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>配置 slaves</li>
<li>分发 hadoop 到其他机器</li>
</ul>
<pre><code class="language-shell">scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:~/app/
scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:~/app/

scp ~/.bash_profile hadoop@hadoop001:~/
scp ~/.bash_profile hadoop@hadoop002:~/
</code></pre>
<ul>
<li>NN 格式化</li>
</ul>
<pre><code class="language-shell">hadoop namenode -format
</code></pre>
<ul>
<li>在每个机器上启动 HDFS</li>
<li>在每个机器上启动 YRAN</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hive系列(2)----内部表、外部表、实战]]></title>
        <id>https://Zu3zz.github.io/post/hive-2</id>
        <link href="https://Zu3zz.github.io/post/hive-2">
        </link>
        <updated>2019-09-09T07:48:10.000Z</updated>
        <summary type="html"><![CDATA[<p>这节咱们来点真实的<br>
hive如何创建表以及读取数据</p>
]]></summary>
        <content type="html"><![CDATA[<p>这节咱们来点真实的<br>
hive如何创建表以及读取数据</p>
<!-- more -->
<h1 id="hive-外部表-内部表">Hive 外部表、内部表</h1>
<h2 id="内部表">内部表</h2>
<p>可以通过 formatted 查看表的属性</p>
<pre><code class="language-sql">desc formattede mp2

可以看到一系列属性 其中有属性如下

Table Type: MANAGED_TABLE

MANAGED_TABLE就代emp2是一个内部表

删除emp2
drop table emp2;
</code></pre>
<p>删除表: HDFS 上的数据被删除 &amp; Meta 也被删除</p>
<h2 id="外部表">外部表</h2>
<p>创建外部表</p>
<pre><code class="language-sql">CREATE EXTERNNAL TABLE emp_external(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
location '/external/emp/';

此时表是空表 还需要加载数据
LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' ONVERWRITE INTO TABLE emp_external

此时通过desc查看表的属性
Table Type: EXTERNAL_TABLE
是一个外部表

drop table emp_external
</code></pre>
<p>删除表: HDFS 上的数据不被删除 &amp; Meta 上被删除<br>
安全性更好</p>
<h2 id="分区表">分区表</h2>
<p>使用分区表创建</p>
<pre><code class="language-sql">CREATE EXTERNAL TABLE track_info(
ip string,
country string,
province string,
city string,
url string,
time string,
page string
) partitioned by (day string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
location '/project/trackinfo/';
</code></pre>
<p>此时通过执行 jar 包生成 ETL 文件</p>
<p>将这个文件存储到 hive 中</p>
<pre><code class="language-sql">LOAD DATA INPATH 'hdfs://localhost:8020/project/input/etl'
OVERWRITE INTO TABLE track_info partition(day='2013-07-21');

统计总数
select count(*) from track_info where day='2013-07-21';

统计省份的个数
select province,count(*) from track_info where day='2013-07-21' group by province;

为了方便展示 创建一张表用来存储省份的信息
create table track_info_province_stat(
province string,
cnt bigint
) partitioned by (day string)
row format delimited fields terminated by '\t';

通过sql语句直接写入数据
insert overwrite table track_info_province_stat partition(day='2013-07-21') select province, count(*) as cnt from track_info where day='2013-07-21' group by province;

统计页面访问情况
select page,count(*) from track_info where day = '2013-07-21' group by page;

创建一张表用来存储页面访问信息
create table track_info_page_stat(
province string,
cnt bigint
) partitioned by (day string)
row format delimited fields terminated by '\t';

写入数据
insert overwrite table track_info_page_stat partition(day='2013-07-21') select page, count(*) as cnt from track_info where day='2013-07-21' group by page;
</code></pre>
<p>到现在为止，我们统计的数据已经在 Hive 表 track_info_province_stat<br>
而且这个表是一个分区表，后续统计报表的数据可以直接从这个表中查询<br>
也可以将 hive 表的数据导出到 RDBMS（sqoop<br>
总结一下所有的操作</p>
<ol>
<li>ETL</li>
<li>把 ETL 输出的数据加载到 track_info 分区表里</li>
<li>各个维度统计结果的数据输出到各自维度的表里 （如 track_info_province_stat）</li>
<li>将数据导出 (optional)</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hive系列(1)----概念、部署、语法]]></title>
        <id>https://Zu3zz.github.io/post/hive-1</id>
        <link href="https://Zu3zz.github.io/post/hive-1">
        </link>
        <updated>2019-09-07T12:35:32.000Z</updated>
        <summary type="html"><![CDATA[<p>慢慢学习路 一步一脚印<br>
今天带来hive学习笔记系列的第一篇</p>
]]></summary>
        <content type="html"><![CDATA[<p>慢慢学习路 一步一脚印<br>
今天带来hive学习笔记系列的第一篇</p>
<!-- more -->
<h1 id="hive-笔记">Hive 笔记</h1>
<h2 id="hive-概念">Hive 概念</h2>
<ul>
<li>
<p>统一元数据管理:</p>
<ul>
<li>Hive 数据是存放在 HDFS</li>
<li>元数据信息(记录数据的数据)是存放在 MySQL 中</li>
<li>SQL on Hadoop： Hive、Spark SQL、impala....</li>
</ul>
</li>
<li>
<p>Hive 体系架构</p>
<ul>
<li>client: shell、thrift/jdbc(server/jdbc)、WebUI(HUE/Zeppelin)</li>
<li>metastore：==&gt; MySQL<br>
database：name、location、owner....<br>
table：name、location、owner、column name/type ....</li>
</ul>
</li>
<li>
<p>Hive 部署</p>
<ol>
<li>下载（官网）</li>
<li>解压到~/app</li>
<li>添加 HIVE_HOME 到系统环境变量</li>
<li>修改配置<br>
hive-env.sh<br>
hive-site.xml</li>
<li>拷贝 MySQL 驱动包 mysql-java-connector.jar 包到$HIVE_HOME/lib 下</li>
<li>前提是要准备安装一个 MySQL 数据库，利用 yum install 安装一个 MySQL 数据库 https://www.cnblogs.com/julyme/p/5969626.html</li>
</ol>
<pre><code class="language-xml">&lt;!--hive-site.xml配置--&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://hadoop000:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;我是用户名&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;我是密码&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
</li>
</ul>
<h2 id="hive-的-sql-语言">Hive 的 sql 语言</h2>
<h3 id="ddl">DDL</h3>
<p>Hive Data Definition Language</p>
<pre><code class="language-shell">create、delete、alter...
</code></pre>
<p>Hive 数据抽象/结构:</p>
<pre><code class="language-ASCII">+-- database  HDFS一个目录
|   +-- table HDFS一个目录
    |   +-- data  文件
    |   +-- partition 分区表  HDFS一个文件
        |   +-- data  文件
        |   +-- bucket  分桶  HDFS一个文件
</code></pre>
<p>Hive 具体 ddl 操作</p>
<h3 id="创建数据库">创建数据库</h3>
<pre><code class="language-sql">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

CREATE DATABASE IF NOT EXISTS hive;

CREATE DATABASE IF NOT EXISTS hive2 LOCATION '/test/location';

CREATE DATABASE IF NOT EXISTS hive3
WITH DBPROPERTIES('creator'='pk');
</code></pre>
<p>/user/hive/warehouse 是 Hive 默认的存储在 HDFS 上的路径</p>
<h3 id="创建表">创建表</h3>
<pre><code class="language-sql">CREATE TABLE emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
</code></pre>
<h3 id="读取本地数据">读取本地数据</h3>
<pre><code class="language-sql">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;
</code></pre>
<p>LOCAL：本地系统，如果没有 local 那么就是指的 HDFS 的路径<br>
OVERWRITE：是否数据覆盖，如果没有那么就是数据追加</p>
<pre><code class="language-sql">LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;

LOAD DATA INPATH 'hdfs://hadoop000:8020/data/emp.txt' INTO TABLE emp;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/hive/'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select empno,ename,sal,deptno from emp;
</code></pre>
<h3 id="基本统计">基本统计</h3>
<pre><code class="language-sql">select * from emp where sal between 800 and 1500 (limit(5));

select * from emp where ename in ('SMITH','KING');

select * from emp where sal is (not) null;
</code></pre>
<h3 id="聚合操作">聚合操作</h3>
<p>max/min/sum/avg</p>
<pre><code class="language-sql">select count(1) from emp where deptno = 10;

select max(sal),min(sal),sum(sal),avg(sal) from emp;
</code></pre>
<h3 id="分组函数">分组函数</h3>
<p>group by</p>
<p>出现在 select 中的字段，如果没有出现在聚合函数里，那么一定要实现在 group by 里</p>
<pre><code class="language-sql">求每个部门的平均工资
select deptno, avg(sal) from emp group by deptno;

求每个部门、工作岗位的平均工资
select deptno,job,avg(sal) from emp group by deptno, job;

求每个部门的平均工资大于2000的部门
select deptno, avg(sal) avg_sal from emp group by deptno where avg_sal &gt; 2000; 错误！！！

group by 不能同时和 where 使用
应该使用having替代where

select deptno, avg(sal) abg_sal from emp group by deptno having avg_sal &gt; 2000;
</code></pre>
<h3 id="多表操作">多表操作</h3>
<p>join</p>
<p>两个表:</p>
<ol>
<li>emp</li>
<li>dept</li>
</ol>
<pre><code class="language-sql">创建表
CREATE TABLE dept(
deptno int,
dname string,
loc string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
加载数据
LOAD DATA LOCAL INPATH '/home/hadoop/data/dept.txt' OVERWRITE INTO TABLE dept;

select
e.empno,e.ename,e.sal,e.deptno,d.name
from emp e join dept d
on e.deptno=d.deptno
</code></pre>
<h4 id="关于-stage-0-stage-3-stage-4">关于 stage-0、stage-3、stage-4</h4>
<p>由于join是一个复杂操作，所以需要分步骤进行查询</p>
<pre><code class="language-sql">explain EXTENDED
select
e.empno,e.ename,e.sal,e.deptno,d.dname
from emp e join dept d
on e.deptno=d.deptno;
</code></pre>
<ul>
<li>stage-4 is root stage</li>
<li>stage-3 depends on stage-4</li>
<li>stage-0 depends on stage-3</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[python基础操作]]></title>
        <id>https://Zu3zz.github.io/post/python-basic</id>
        <link href="https://Zu3zz.github.io/post/python-basic">
        </link>
        <updated>2019-09-04T13:23:26.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  分享Python的一些骚操作</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  分享Python的一些骚操作</p>
<!-- more -->
<h2 id="file">file</h2>
<pre><code class="language-python">file1 = read(&quot;test.txt&quot;)
# 找到当前位置的指针
file1.tell()
# 让文件的指示指针进行偏移 第一个参数代表偏移位置 第二个参数 0代表从文件开头偏移 1代表从当前位置进行偏移 2代表从文件结尾进行偏移
file1.seek(5,0)
</code></pre>
<h2 id="error处理">error处理</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[leetcode-股票买卖问题(121,122,123,188,309,714)]]></title>
        <id>https://Zu3zz.github.io/post/leetcode-stock</id>
        <link href="https://Zu3zz.github.io/post/leetcode-stock">
        </link>
        <updated>2019-08-18T06:28:46.000Z</updated>
        <summary type="html"><![CDATA[<p>如何用一个动态规划方程解决leetcode上所有的股票买卖问题，且听我道来</p>
]]></summary>
        <content type="html"><![CDATA[<p>如何用一个动态规划方程解决leetcode上所有的股票买卖问题，且听我道来</p>
<!-- more -->
<p><strong>直接看题目188</strong></p>
<h2 id="188-买卖股票的最佳时机-iv">188. 买卖股票的最佳时机 IV</h2>
<p>给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。</p>
<p>设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。</p>
<p>注意: 你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p>
<p>示例 1:</p>
<pre><code>输入: [2,4,1], k = 2
输出: 2
解释: 在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2 。
</code></pre>
<p>示例2：</p>
<pre><code>输入: [3,2,6,5,0,3], k = 2
输出: 7
解释: 在第 2 天 (股票价格 = 2) 的时候买入，在第 3 天 (股票价格 = 6) 的时候卖出, 这笔交易所能获得利润 = 6-2 = 4 。
     随后，在第 5 天 (股票价格 = 0) 的时候买入，在第 6 天 (股票价格 = 3) 的时候卖出, 这笔交易所能获得利润 = 3-0 = 3 。
</code></pre>
<p>对于这道题，最麻烦的无异于暴力，复杂度会变成O(2^n)。<br>
使用动态规划(dp)，可以有效的解决</p>
<p>对于一个常见的dp问题来说 通常分两步走</p>
<ol>
<li>定义状态方程</li>
<li>转移公式</li>
</ol>
<p>对于这道题，可以定义一个状态方程max_profit[i]用来记录到第i天的最大利润</p>
<pre><code class="language-c++">mp[i] = mp[i-1] + -a[i](如果买入) + a[i](如果卖出)
</code></pre>
<p>但是股票的买入需要当前手上没有股票买入<br>
同样 股票的卖出需要手上已经持有了一股的股票<br>
单纯的一维数组没有办法记录这些信息 所以需要增加一维度</p>
<pre><code class="language-c++">使用mp[i][j]
j=[0,1] // 0为可以买 1为可以卖
</code></pre>
<p>此时状态方程变为</p>
<pre><code>mp[i][0] = max(mp[i-1][0] //不交易 ,mp[i-1][1] + a[i]// 前一天卖了一股)
mp[i][1] = max(mp[i-1][1] // 不交易, mp[i-1][0] - a[i] //前一天买了一股)
</code></pre>
<p>由于188题有限制条件，即最多只能交易k次，还需要记录交易次数，还需要一维来记录交易了多少次<br>
此时状态方程变成了</p>
<pre><code>mp[i][h][k]
i 表示天数
j 表示是否持有股票
k 表示之前交易了多少次
</code></pre>
<p>此时动态规划的转移方程变成了如下所示:</p>
<pre><code class="language-c++">// 为了方便理解 把k放到了第二维

//前一天 要么不操作 要么卖掉了一股
mp[i][k][0] = max(mp[i-1][k][0], mp[i-1][k-1][1] + a[i])

//  前一天 要么不操作 要么买入了一股
mp[i][k][1] = max(mp[i-1][k][1], mp[i-1][k][0] - a[i)
</code></pre>
<p>想要求出最大的收益 只需要找到<br>
<code>mp[n-1, {0...k},0]</code>的最大值即可</p>
<h2 id="188买卖股票的最佳时机iv">188.买卖股票的最佳时机IV</h2>
<h3 id="python-solution">python solution</h3>
<pre><code class="language-python">class Solution:
    def maxProfit(self, k: int, prices: List[int]) -&gt; int:
        if not prices or not k:
            return 0
        n  = len(prices)
        # 如果k大于数组长度的一半，则可以用贪心解决
        if k &gt; n//2:
            return self.greedy(prices)
        # 动态规划
        dp = [[[0] * 2 for _ in range(k+1)] for _ in range(n)]
        res = []
        # 设置初始状态
        for i in range(k+1):
            dp[0][i][0], dp[0][i][1] = 0, -prices[0]
        # 开始两层循环
        for i in range(1,n):
            for j in range(k+1):
                if not j:
                    dp[i][j][0] = dp[i-1][j][0]
                else:
                    dp[i][j][0] = max(dp[i-1][j][0], dp[i-1][j-1][1] + prices[i])
                dp[i][j][1] = max(dp[i-1][j][1], dp[i-1][j][0] - prices[i])
        # 找到最大值
        for m in range(k+1):
            print(dp[n-1][m][0])
            res.append(dp[n-1][m][0])
        return max(res)
    def greedy(self, prices):
        res = 0
        for i in range(1,len(prices)):
            if prices[i] &gt; prices[i-1]:
                res += prices[i] - prices[i-1]
        return res
</code></pre>
<h2 id="309买卖股票的最佳时机含冻结期">309.买卖股票的最佳时机含冻结期</h2>
<p>对于这道题目 dp状态可以相应减少，因为没有次数的限制了，只需要做一个二维的dp就可以了 一个维度i是用来存储天数，另外一个维度j用来存储当前股票买卖的状态 012分别表示没有买入 买入了 以及处于冻结期</p>
<h3 id="python-solution-2">python solution</h3>
<pre><code class="language-python">class Solution:
    def maxProfit(self, prices: List[int]) -&gt; int:
        if not prices:
            return 0
        n = len(prices)
        dp = [[0] * 3 for _ in range(n)]
        # 0表示今天未持有 1表示今天持有 2表示今天处于冻结状态
        dp[0][0], dp[0][1], dp[0][2] = 0,-prices[0],0
        for i in range(1,n):
            dp[i][0] = max(dp[i-1][0], dp[i-1][2])
            dp[i][1] = max(dp[i-1][1], dp[i-1][0] - prices[i])
            dp[i][2] = dp[i-1][1] + prices[i]
        return max(dp[n-1][0], dp[n-1][2])
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leetcode-weeklyContest-150]]></title>
        <id>https://Zu3zz.github.io/post/leetcode-weeklycontest-150</id>
        <link href="https://Zu3zz.github.io/post/leetcode-weeklycontest-150">
        </link>
        <updated>2019-08-18T05:49:18.000Z</updated>
        <summary type="html"><![CDATA[<p>第150周的题解</p>
]]></summary>
        <content type="html"><![CDATA[<p>第150周的题解</p>
<!-- more -->
<h2 id="第一题">第一题</h2>
<h3 id="5048-拼写单词">5048. 拼写单词</h3>
<p>要求:<br>
给你一份『词汇表』（字符串数组） words 和一张『字母表』（字符串） chars。</p>
<p>假如你可以用 chars 中的『字母』（字符）拼写出 words 中的某个『单词』（字符串），那么我们就认为你掌握了这个单词。</p>
<p>注意：每次拼写时，chars 中的每个字母都只能用一次。</p>
<p>返回词汇表 words 中你掌握的所有单词的 长度之和。<br>
示例1：</p>
<pre><code>输入：words = [&quot;cat&quot;,&quot;bt&quot;,&quot;hat&quot;,&quot;tree&quot;], chars = &quot;atach&quot;
输出：6
解释： 
可以形成字符串 &quot;cat&quot; 和 &quot;hat&quot;，所以答案是 3 + 3 = 6。
</code></pre>
<p>示例2：</p>
<pre><code>输入：words = [&quot;hello&quot;,&quot;world&quot;,&quot;leetcode&quot;], chars = &quot;welldonehoneyr&quot;
输出：10
解释：
可以形成字符串 &quot;hello&quot; 和 &quot;world&quot;，所以答案是 5 + 5 = 10。
</code></pre>
<p><strong>思路：</strong><br>
直接把字符串问题转化成为一个大小为26的数组，对给定的「字母表」进行遍历，将数组的对应位置记录下每个字母出现了多少次，再循环一遍给出的「词汇表」，只要词汇表每个都比给定的字母表的数组元素小，那么就说明可以表示。</p>
<p>solution in cpp:</p>
<pre><code class="language-c++">class Solution {
public:
    int countCharacters(vector&lt;string&gt;&amp; words, string chars) {
        int s[26],t[26],i,result = 0;
        memset(s,0,sizeof(s));
        for(auto c:chars)s[c-'a']++;
        for(auto d:words){
            memset(t,0,sizeof(t));
            for(auto e:d)t[e-'a']++;
            for(i=0;i&lt;26;i++) if(s[i]&lt;t[i]) break;
            if(i==26) result+=d.size();
        }
        return result;
    }
};
</code></pre>
<h2 id="第二题">第二题</h2>
<h3 id="5052最大层内元素和">5052.最大层内元素和</h3>
<p>给你一个二叉树的根节点 root。设根节点位于二叉树的第 1 层，而根节点的子节点位于第 2 层，依此类推。</p>
<p>请你找出层内元素之和 最大 的那几层（可能只有一层）的层号，并返回其中 最小 的那个。<br>
给定一个二叉树 大概长下面这样<br>
实例：<br>
<img src="https://Zu3zz.github.io/post-images/1566107780628.jpeg" alt=""></p>
<pre><code>输入：[1,7,0,7,-8,null,null]
输出：2
解释：
第 1 层各元素之和为 1，
第 2 层各元素之和为 7 + 0 = 7，
第 3 层各元素之和为 7 + -8 = -1，
所以我们返回第 2 层的层号，它的层内元素之和最大。
</code></pre>
<p>思路：<br>
使用一个dict用来存储每一层的元素的值，每一对键值对都是层数和相对应的值的和，使用dfs遍历所有树的节点，同时将节点所在的层数传进去，传进去的同时直接访问对应的dict对应的元素,直接加起来，最后遍历一遍输出最大的值</p>
<ul>
<li>solution in cpp:</li>
</ul>
<pre><code class="language-c++">/**
 * Definition for a binary tree node.
 * struct TreeNode {
 *     int val;
 *     TreeNode *left;
 *     TreeNode *right;
 *     TreeNode(int x) : val(x), left(NULL), right(NULL) {}
 * };
 */
class Solution {
public:
    int s[10005], n;
    void dfs(TreeNode* root, int level){
        if(!root) return;
        n = max(n,level);
        s[level]+=root-&gt;val;
        dfs(root-&gt;left,level+1);
        dfs(root-&gt;right,level+1);
    }
    int maxLevelSum(TreeNode* root) {
        memset(s,0,sizeof(s));
        n = 0;
        dfs(root,1);
        int j = 1;
        for(int i = 1;i&lt;=n;i++){
            if(s[i] &gt; s[j])
                j = i;
        }
        return j;
    }
};
</code></pre>
<ul>
<li>solution in python:</li>
</ul>
<pre><code class="language-python"># Definition for a binary tree node.
# class TreeNode:
#     def __init__(self, x):
#         self.val = x
#         self.left = None
#         self.right = None

class Solution:
    
    def maxLevelSum(self, root: TreeNode) -&gt; int:
        m,res,level = {},0,0
        def dfs(root,level):
            if root == None:
                return
            if level not in m:
                m[level] = 0
            m[level]+=root.val
            dfs(root.left,level+1)
            dfs(root.right,level+1)
        dfs(root, 1)
        min_ = -sys.maxsize
        for key in m.keys():
            if m[key] &gt; min_:
                min_, res = m[key], key
        return res
</code></pre>
<h2 id="第三题">第三题</h2>
<h3 id="5053地图分析">5053.地图分析</h3>
<p>你现在手里有一份大小为 N x N 的『地图』（网格） grid，上面的每个『区域』（单元格）都用 0 和 1 标记好了。其中 0 代表海洋，1 代表陆地，你知道距离陆地区域最远的海洋区域是是哪一个吗？请返回该海洋区域到离它最近的陆地区域的距离。</p>
<p>我们这里说的距离是『曼哈顿距离』（ Manhattan Distance）：(x0, y0) 和 (x1, y1) 这两个区域之间的距离是 |x0 - x1| + |y0 - y1| 。</p>
<p>如果我们的地图上只有陆地或者海洋，请返回 -1。</p>
<p>实例1：<br>
<img src="https://Zu3zz.github.io/post-images/1566108523044.jpeg" alt="实例"></p>
<pre><code>输入：[[1,0,1],[0,0,0],[1,0,1]]
输出：2
解释： 
海洋区域 (1, 1) 和所有陆地区域之间的距离都达到最大，最大距离为 2。
</code></pre>
<p>太菜了 没做出来</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[堆排序(Heap Sort)]]></title>
        <id>https://Zu3zz.github.io/post/heap-sort</id>
        <link href="https://Zu3zz.github.io/post/heap-sort">
        </link>
        <updated>2019-08-17T10:17:56.000Z</updated>
        <summary type="html"><![CDATA[<p>堆排序真的是面试非常容易考到的一个问题了，今天就来聊聊Heap Sort。</p>
]]></summary>
        <content type="html"><![CDATA[<p>堆排序真的是面试非常容易考到的一个问题了，今天就来聊聊Heap Sort。</p>
<!-- more -->
<p>写这篇博客的原因是因为今天面试被问到了如何使用堆排序在一个百万数量级的数组中取出前k大的元素，可能是有点紧张，我大脑当场短路了，直接把二分搜索树和堆排序搞混了，真的是太丢人了，果不其然面试也挂了。</p>
<p>痛定思痛，决定自己认真的再复习和回顾一下堆排序的所有操作。</p>
<h2 id="1基本存储">1.基本存储</h2>
<p>首先从堆这个数据结构说起，堆可以做到入堆和出堆都是O(logn)的复杂度，平均时间上比起O(1)+O(n)这种组合要快出去很多。</p>
<p>首先从堆的基本形态说起，堆本质是就是一个数组，但是在存储的过程中，我们可以把它看做是一个完全二叉树:即索引为0的位置为根节点，索引为1、2的位置为根节点的子节点，下面就是一个二叉堆(Binary Heap)，对应的就是一个二叉树<br>
<img src="https://Zu3zz.github.io/post-images/1566038321792.png" alt=""><br>
可以看到，二叉堆满足性质如下，所有子节点均小于父亲节点，即<br>
<code>41&lt;62 &amp;&amp; 30&lt;62</code></p>
<p><strong>并且 二叉堆是一颗完全二叉树</strong><br>
<img src="https://Zu3zz.github.io/post-images/1566038535528.png" alt=""></p>
<p>所以上图所示的堆可以在cpp中如下所存储</p>
<pre><code class="language-c++">int arr[] = {0,62,41,30,28,16,22,13,19,17,15}
</code></pre>
<p>可以注意到这里index为0的元素没有使用，这是为了方便堆排序进行左右节点的计算规则。<br>
所以这样所有的子节点和父节点就满足如下的公式</p>
<pre><code class="language-c++">parent i = i /2;
left child i = 2 * i;
right child i = 2 * i +1;
</code></pre>
<p>本节对应代码：<br>
<strong><a href="https://github.com/Zu3zz/Learn_some_algorithm_and_data_structure/blob/master/heap-sort/01-MaxHeap/main.cpp">二叉堆的建立与基本操作</a></strong></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[从头梳理服务器渲染原理(SSR)]]></title>
        <id>https://Zu3zz.github.io/post/react-ssr</id>
        <link href="https://Zu3zz.github.io/post/react-ssr">
        </link>
        <updated>2019-08-14T14:54:33.000Z</updated>
        <summary type="html"><![CDATA[<p>这一次，让我们来以React为例，把服务端渲染(Server Side Render，简称“SSR”)学个明明白白。</p>
]]></summary>
        <content type="html"><![CDATA[<p>这一次，让我们来以React为例，把服务端渲染(Server Side Render，简称“SSR”)学个明明白白。</p>
<!-- more -->
<h2 id="part1实现一个基础的react组件ssr">part1：实现一个基础的React组件SSR</h2>
<p>这一部分来简要实现一个React组件的SSR。</p>
<h3 id="一-ssr-vs-csr">一. SSR vs CSR</h3>
<p>什么是服务端渲染？</p>
<p>废话不多说，直接起一个express服务器。</p>
<pre><code class="language-javascript">var express = require('express')
var app = express()

app.get('/', (req, res) =&gt; {
 res.send(
 `
   &lt;html&gt;
     &lt;head&gt;
       &lt;title&gt;hello&lt;/title&gt;
     &lt;/head&gt;
     &lt;body&gt;
       &lt;h1&gt;hello&lt;/h1&gt;
       &lt;p&gt;world&lt;/p&gt;
     &lt;/body&gt;
   &lt;/html&gt;
 `
 )
})

app.listen(3001, () =&gt; {
 console.log('listen:3001')
})
</code></pre>
<p>启动之后打开localhost:3001可以看到页面显示了hello world。而且打开网页源代码如下：<br>
<img src="https://Zu3zz.github.io/post-images/1565794815666.jpg" alt=""><br>
也能够完成显示。<br>
这就是服务端渲染。其实非常好理解，就是服务器返回一堆html字符串，然后让浏览器显示。<br>
与服务端渲染相对的是客户端渲染(Client Side Render)。那什么是客户端渲染？<br>
现在创建一个新的React项目，用脚手架生成项目，然后run起来。<br>
这里你可以看到React脚手架自动生成的首页。</p>
<p><img src="https://Zu3zz.github.io/post-images/1565794890546.jpg" alt=""><br>
然而打开网页源代码,我们会发现网页并没有任何DOM结构<br>
<img src="https://Zu3zz.github.io/post-images/1565794896346.jpg" alt=""><br>
body中除了兼容处理的noscript标签之外，只有一个id为root的标签。那首页的内容是从哪来的呢？很明显，是下面的script中拉取的JS代码控制的。<br>
因此，CSR和SSR最大的区别在于前者的页面渲染是JS负责进行的，而后者是服务器端直接返回HTML让浏览器直接渲染。<br>
为什么要使用服务端渲染呢？<br>
<img src="https://Zu3zz.github.io/post-images/1565795001030.jpeg" alt=""><br>
传统CSR的弊端：</p>
<ol>
<li>由于页面显示过程要进行JS文件拉取和React代码执行，首屏加载时间会比较慢。</li>
<li>对于SEO(Search Engine Optimazition,即搜索引擎优化)，完全无能为力，因为搜索引擎爬虫只认识html结构的内容，而不能识别JS代码内容。</li>
</ol>
<p>SSR的出现，就是为了解决这些传统CSR的弊端。</p>
<h2 id="二-实现react组件的服务端渲染">二、实现React组件的服务端渲染</h2>
<p>刚刚起的express服务返回的只是一个普通的html字符串，但我们讨论的是如何进行React的服务端渲染，那么怎么做呢？ 首先创建containers文件夹，新建一个Home.js的文件，写一个简单的React组件:</p>
<pre><code class="language-javascript">// containers/Home.js
import React from 'react';
const Home = () =&gt; {
  return (
    &lt;div&gt;
      &lt;div&gt;This is zu3zz&lt;/div&gt;
    &lt;/div&gt;
  )
}
export default Home
</code></pre>
<p>现在的任务就是将它转换为html代码返回给浏览器。<br>
总所周知，JSX中的标签其实是基于虚拟DOM的，最终要通过一定的方法将其转换为真实DOM。虚拟DOM也就是JS对象，可以看出整个服务端的渲染流程就是通过虚拟DOM的编译来完成的，因此虚拟DOM巨大的表达力也可见一斑了。<br>
而react-dom这个库中刚好实现了编译虚拟DOM的方法。做法如下:</p>
<pre><code class="language-javascript">// server/index.js
import express from 'express';
import { renderToString } from 'react-dom/server';
import Home from './containers/Home';

const app = express();
const content = renderToString(&lt;Home /&gt;);
app.get('/', function (req, res) {
   res.send(
   `
    &lt;html&gt;
      &lt;head&gt;
        &lt;title&gt;ssr&lt;/title&gt;
      &lt;/head&gt;
      &lt;body&gt;
        &lt;div id=&quot;root&quot;&gt;${content}&lt;/div&gt;
      &lt;/body&gt;
    &lt;/html&gt;
   `
   );
})
app.listen(3001, () =&gt; {
  console.log('listen:3001')
})
</code></pre>
<p>启动express服务，再浏览器上打开对应端口，页面显示出&quot;this is sanyuan&quot;。<br>
到此，就初步实现了一个React组件是服务端渲染。<br>
当然，这只是一个非常简陋的SSR，事实上对于复杂的项目而言是无能为力的，在之后会一步步完善，打造出一个功能完整的React的SSR框架。</p>
]]></content>
    </entry>
</feed>