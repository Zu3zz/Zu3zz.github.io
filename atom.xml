<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Zu3zz.github.io/</id>
    <title>风袖</title>
    <updated>2020-02-16T13:23:10.128Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Zu3zz.github.io/"/>
    <link rel="self" href="https://Zu3zz.github.io/atom.xml"/>
    <subtitle>烟蛾敛略不胜态，风袖低昂如有情</subtitle>
    <logo>https://Zu3zz.github.io/images/avatar.png</logo>
    <icon>https://Zu3zz.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 风袖</rights>
    <entry>
        <title type="html"><![CDATA[主流缓存Redis笔记]]></title>
        <id>https://Zu3zz.github.io/post/interview-redis/</id>
        <link href="https://Zu3zz.github.io/post/interview-redis/">
        </link>
        <updated>2020-02-16T13:15:30.000Z</updated>
        <summary type="html"><![CDATA[<p>🐽详细讲解主流缓存面试会遇到的问题，包含Redis分布式锁、异步队列、持久化、集群等🐽</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐽详细讲解主流缓存面试会遇到的问题，包含Redis分布式锁、异步队列、持久化、集群等🐽</p>
<!-- more -->
<h1 id="主流缓存redis笔记">主流缓存Redis笔记</h1>
<h2 id="1-主流应用架构">1. 主流应用架构</h2>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io//post-images/1581859003669.png" alt="" loading="lazy"></figure>
<h3 id="11-缓存中间件-memcache和redis的区别">1.1 缓存中间件---- Memcache和Redis的区别</h3>
<h4 id="1-memcache代码层次类似hash">1. Memcache：代码层次类似Hash</h4>
<ul>
<li>支持简单数据类型</li>
<li>不支持数据持久化存储</li>
<li>不支持主从</li>
<li>不支持分片</li>
</ul>
<h4 id="2-redis">2. Redis</h4>
<ul>
<li>数据类型丰富</li>
<li>支持数据磁盘持久化存储</li>
<li>支持主从</li>
<li>支持分片</li>
</ul>
<hr>
<h3 id="12-为什么redis能这么快">1.2 为什么Redis能这么快</h3>
<h4 id="1-100000qps每秒内查询次数">1. 100000+QPS（每秒内查询次数）</h4>
<ul>
<li>完全基于内存，绝大部分请求是纯粹的内存操作，执行效率高</li>
<li>数据结构简单，对数据操作也简单</li>
<li>采用单线程，单线程也能处理高并发请求，想多核也可以启动多实例</li>
<li>使用多路I/O复用模型，非阻塞IO</li>
</ul>
<hr>
<h3 id="13-多路io复用模型">1.3 多路I/O复用模型</h3>
<h4 id="1-fd-file-descriptor文件描述符">1. FD: File descriptor，文件描述符</h4>
<ul>
<li>一个打开的文件通过唯一的描述符进行引用，该描述符是打开文件的元数据到文件本身的映射</li>
</ul>
<h4 id="2-传统的阻塞io模型">2. 传统的阻塞I/O模型</h4>
<figure data-type="image" tabindex="2"><img src="https://Zu3zz.github.io//post-images/1581859021180.png" alt="" loading="lazy"></figure>
<h4 id="3-select-系统调用">3. Select 系统调用</h4>
<figure data-type="image" tabindex="3"><img src="https://Zu3zz.github.io//post-images/1581859027686.png" alt="" loading="lazy"></figure>
<h4 id="4-redis采用的io多路复用函数epollkqueueevportselect">4. Redis采用的I/O多路复用函数：epoll/kqueue/evport/select</h4>
<ul>
<li>因地制宜</li>
<li>优先学则时间复杂度为O(1)的I/O多路复用函数作为底层实现</li>
<li>以时间复杂度的O(N)的select作为保底</li>
<li>基于react设计模式监听I/O事件</li>
</ul>
<h2 id="2-redis的数据类型">2. Redis的数据类型</h2>
<h3 id="1-供用户使用的数据类型">1. 供用户使用的数据类型</h3>
<ul>
<li>
<p>String：最基本的数据类型，二进制安全（底层使用sdshdr）</p>
<figure data-type="image" tabindex="4"><img src="https://Zu3zz.github.io//post-images/1581859149419.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>Hash：String元素组成的字典，适合用于存储对象（hmset key1 value1 key2 value2...）</p>
</li>
<li>
<p>List：列表，按照String元素插入顺序排序</p>
</li>
<li>
<p>Set：String元素组成的无序集合，通过哈希表实现，不允许重复(sadd myset 1)</p>
</li>
<li>
<p>Sorted Set：通过分数来为集合中的成员进行从小到达的排序（zadd myzset 3 a)\</p>
</li>
<li>
<p>高级：用来计数的HyperLogLog，用于支持存储地理位置信息的Geo</p>
</li>
</ul>
<h3 id="2-底层数据类型基础">2. 底层数据类型基础</h3>
<ol>
<li>简单动态字符串</li>
<li>链表</li>
<li>字典</li>
<li>跳跃表</li>
<li>整数集合</li>
<li>压缩列表</li>
<li>对象</li>
</ol>
<h2 id="3-从海量key里查询某一固定前缀的key">3. 从海量key里查询某一固定前缀的Key</h2>
<h3 id="1-keys-pattern查找所有符合给定模式pattern的key">1. KEYS pattern：查找所有符合给定模式pattern的key</h3>
<ul>
<li>KEYS指令一次性返回所有匹配的key</li>
<li>键的数量过大会使得服务卡顿</li>
</ul>
<h3 id="2-scan-cursor-match-pattern-count-count">2. SCAN cursor [MATCH pattern] [COUNT count]</h3>
<ul>
<li>基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程</li>
<li>以0作为游标开始一次新的迭代，直到命令返回游标0完成一次遍历</li>
<li>不保证每次执行都返回某个给定数量的元素，支持模糊查询</li>
<li>一次返回的数量不可控，只能是大概率符合count参数</li>
</ul>
<h2 id="4-如何通过redis实现分布式锁">4. 如何通过Redis实现分布式锁</h2>
<h3 id="1-分布式锁需要解决的问题">1. 分布式锁需要解决的问题</h3>
<ul>
<li>互斥性</li>
<li>安全性</li>
<li>死锁</li>
<li>容错</li>
</ul>
<h3 id="2-setnx-key-value如果key不存在则创建并赋值">2. SETNX key value：如果key不存在，则创建并赋值</h3>
<h3 id="3-解决setnx-长期有效的问题">3. 解决SETNX 长期有效的问题</h3>
<ol>
<li>
<p>EXPIRE key seconds</p>
<ul>
<li>设置key的生存时间，当key过期时（生存时间为0），会被自动删除</li>
</ul>
</li>
<li>
<figure data-type="image" tabindex="5"><img src="https://Zu3zz.github.io//post-images/1581859174423.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>SET key value [EX seconds] [PX milliseconds] [NX|XX]</p>
<ul>
<li>EX second：设置键的过期时间为second秒</li>
<li>PX millisecond：设置键的过期时间为millisecond毫秒</li>
<li>NX ：只在键不存在时候，才对键进行操作 （等同于上面 SETNX key value）</li>
<li>XX：只在键不存在时候，才对键进行操作</li>
<li>SET 操作成功完成时， 返回OK, 否则返回nil</li>
</ul>
</li>
</ol>
<h3 id="4-大量key同时过期的注意事项">4. 大量key同时过期的注意事项</h3>
<ol>
<li>集中过期，由于清除大量的key很耗时，会出现短暂的卡顿现象</li>
<li>解决方案：在设置key的过期时间的时候，给每个key加上随机值</li>
</ol>
<h2 id="5-使用redis做异步队列">5. 使用Redis做异步队列</h2>
<h3 id="1-使用list作为队列">1. 使用List作为队列</h3>
<ol>
<li>
<p>RPUSH生产消息 LPOP消费消息</p>
<ul>
<li>缺点：没有等待队列里有值就直接消费</li>
<li>可以通过在应用层引入Sleep机制去调用LPOP重试</li>
</ul>
</li>
<li>
<p><code>BLPOP key [key...] timeout</code>：阻塞直到队列有消息或者超时</p>
</li>
</ol>
<h3 id="2-pubsub主题订阅者模式">2. pub/sub：主题订阅者模式</h3>
<ul>
<li>发送者（pub）发送消息，订阅者（sub）接收消息</li>
<li>先订阅一个频道，会自动获得这个频道里的消息</li>
<li>缺点：消息的发布是无状态的，无法表征可达</li>
</ul>
<h2 id="6-redis如何做持久化">6. Redis如何做持久化</h2>
<h3 id="1-rdb快照持久化保存某个时间点的全量数据快照">1. RDB（快照）持久化：保存某个时间点的全量数据快照</h3>
<ol>
<li>SAVE：阻塞Redis的服务器进程，直到RDB文件被创建完毕</li>
<li><strong>BGSAVE：Fork出一个子进程来创建RDB文件，不阻塞服务器</strong></li>
</ol>
<h3 id="2-自动化触发rbd持久化的方式">2. 自动化触发RBD持久化的方式</h3>
<ol>
<li>根据redis.conf配置里的<code>SAVE m n</code> 定时触发（用的是BGSAVE）</li>
<li>主从复制时，主节点自动触发</li>
<li>指定Debug Reload</li>
<li>执行Shutdown且没有开发AOF持久化</li>
</ol>
<h3 id="3-bgsave的原理">3. BGSAVE的原理</h3>
<figure data-type="image" tabindex="6"><img src="https://Zu3zz.github.io//post-images/1581859203147.png" alt="" loading="lazy"></figure>
<ul>
<li>系统调用fork()：创建进程，实现了Copy-on-Write</li>
<li>缺点
<ul>
<li>内存数据的全量同步，数据量大会由于I/O而影响性能</li>
<li>可能会因为Redis挂掉而损失从当前至最近一次快照期间的数据</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-aofappend-only-file持久化保存写状态">4. AOF（Append-Only-File）持久化：保存写状态</h3>
<ol>
<li>
<p>记录下除了查询以外的所有变更数据库状态的指令</p>
</li>
<li>
<p>以append的形式追加保存到AOF文件中（增量）</p>
</li>
<li>
<p>AOF默认是关闭的 在conf文件中配置</p>
</li>
<li>
<p>日志重写解决AOF文件大小不断增大的问题，原理如下：</p>
<ul>
<li>调用fork（），创建一个子进程</li>
<li>子进程吧新的AOF写到一个临时文件里，不依赖原来的AOF文件</li>
<li>主进程持续将新的变动同时写到内存和原来的AOF里</li>
<li>主进程获取子进程重写AOF的完成信号，往新AOF同步增量变动</li>
<li>使用新的AOF文件替换掉旧的AOF文件</li>
</ul>
</li>
</ol>
<hr>
<h3 id="5-数据恢复过程">5. 数据恢复过程</h3>
<ol>
<li>先看有没有AOF文件，再恢复</li>
<li>在看有没有RDB文件，再恢复</li>
</ol>
<hr>
<h3 id="6-两者优缺点">6. 两者优缺点</h3>
<ol>
<li>RDB优点：全局数据快照，文件小，恢复快</li>
<li>RDB缺点：无法保存最近一次快照之后的数据</li>
<li>AOF优点：可读性高，适合保存增量数据，数据不易丢失</li>
<li>AOF缺点：文件体积大，恢复时间长</li>
</ol>
<hr>
<h3 id="7-使用rdb-aof混合持久化方式">7. 使用RDB-AOF混合持久化方式</h3>
<ul>
<li>BGSAVE做镜像全量持久化，AOF做增量持久化</li>
</ul>
<h2 id="7-pipeline">7. Pipeline</h2>
<h3 id="1-使用pipeline的好处">1. 使用pipeline的好处</h3>
<ol>
<li>pipeline和linux的管道类似</li>
<li>redis基于请求/相应模型，单个请求处理需要一一应答</li>
<li>Pipeline批量执行指令，节省多次IO往返的时间</li>
<li>有顺序依赖的指令建议分批发送</li>
</ol>
<hr>
<h3 id="2-redis的同步机制">2. Redis的同步机制</h3>
<ol>
<li>
<p>主从同步原理（Master/Slave）</p>
<figure data-type="image" tabindex="7"><img src="https://Zu3zz.github.io//post-images/1581859218260.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>全同步过程</p>
<ul>
<li>Slave发送sync命令到master</li>
<li>Master启动一个后台进程，将Redis中的数据快照保存到文件中（BGSAVE）</li>
<li>Master将保存数据快照期间接收到的写命令缓存起来</li>
<li>Master完成写文件操作后，将该文件发送给Slave</li>
<li>使用新的AOF文件替换掉旧的AOF文件</li>
<li>Master将这期间收集到的增量写命令发送给Slave端</li>
</ul>
</li>
<li>
<p>增量同步过程</p>
<ul>
<li>Master接收到用户的操作指令，判断是否需要传播到Slave</li>
<li>将操作记录追加到AOF文件</li>
<li>将操作传播到其他Slave：
<ol>
<li>对齐主从库</li>
<li>往相应缓存写入指令</li>
</ol>
</li>
<li>将缓存中的数据发送给Slave</li>
</ul>
</li>
<li>
<p>Redis Sentinel（Redis哨兵）</p>
<ol>
<li>解决主从同步Master宕机后的主从切换问题：
<ul>
<li>监控：检查主从服务器是否运行正常</li>
<li>提醒：通过API向管理员或者其他应用程序发送故障通知</li>
<li>自动故障迁移：主从切换</li>
</ul>
</li>
</ol>
</li>
<li>
<p>流言协议Gossip</p>
<ol>
<li>在杂乱无章中寻求一致</li>
<li>每个节点都随机地与对方通信，最终所有节点的状态达成一致</li>
<li>种子节点定期随机向其他节点发送节点列表以及需要传播的消息</li>
<li>不保证信息一定会传递给所有节点，但是最终会趋于一致</li>
</ol>
</li>
</ol>
<h2 id="8-redis集群">8. Redis集群</h2>
<h3 id="1-如何从海量数据里快速找到所需">1. 如何从海量数据里快速找到所需</h3>
<ul>
<li>分片：按照某种规则去划分数据，分散存储在多个节点上</li>
</ul>
<hr>
<h3 id="2-redis的集群原理">2. Redis的集群原理</h3>
<ul>
<li>
<p>一致性哈希算法：对 2^32 取模，将哈希值空间组织成虚拟的圆环</p>
</li>
<li>
<p>将数据key使用相同的函数Hash计算出哈希值，找到最近的Hash节点</p>
<figure data-type="image" tabindex="8"><img src="https://Zu3zz.github.io//post-images/1581859268434.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>如果此时<code>Node C</code>宕机,数据都会到<code>Node D</code>中</p>
</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://Zu3zz.github.io//post-images/1581859274355.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>新增一台服务器 <code>Node X</code></p>
<figure data-type="image" tabindex="10"><img src="https://Zu3zz.github.io//post-images/1581859282580.png" alt="" loading="lazy"></figure>
</li>
</ul>
<h3 id="3-数据倾斜问题">3. 数据倾斜问题</h3>
<ol>
<li>
<p>Hash环的数据倾斜问题</p>
<figure data-type="image" tabindex="11"><img src="https://Zu3zz.github.io//post-images/1581859296841.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>引入虚拟节点解决数据倾斜的问题（节点过少时有用）（设置虚拟节点32个）</p>
<figure data-type="image" tabindex="12"><img src="https://Zu3zz.github.io//post-images/1581859303063.png" alt="" loading="lazy"></figure>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据库面试考点]]></title>
        <id>https://Zu3zz.github.io/post/interview-database/</id>
        <link href="https://Zu3zz.github.io/post/interview-database/">
        </link>
        <updated>2020-02-12T10:51:52.000Z</updated>
        <summary type="html"><![CDATA[<p>🐰详细讲解数据库面试常见问题，包含索引、锁、事务等内容🐰</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐰详细讲解数据库面试常见问题，包含索引、锁、事务等内容🐰</p>
<!-- more -->
<h1 id="数据库面试总结">数据库面试总结</h1>
<h2 id="1-架构">1. 架构</h2>
<h3 id="1-rdbms">1. RDBMS</h3>
<ol>
<li>程序实例
<ul>
<li>存储管理</li>
<li>缓存机制</li>
<li>SQL解析</li>
<li>日志管理</li>
<li>权限划分</li>
<li><strong>索引管理</strong></li>
<li><strong>锁管理</strong></li>
</ul>
</li>
<li>存储（文件系统）</li>
</ol>
<h2 id="2-索引">2. 索引</h2>
<ol>
<li>为什么要使用索引：避免全表扫描/快速查询数据</li>
<li>什么样的信息能成为索引：主键、唯一键</li>
<li>索引的数据结构
<ul>
<li>建立二叉查找树进行二分查找</li>
<li>建立B-Tree结构进行查找</li>
<li>建立B+-Tree结构进行查找</li>
<li>建立Hash结构进行查找</li>
</ul>
</li>
</ol>
<hr>
<h3 id="1-优化索引">1. 优化索引</h3>
<ol>
<li>
<p>二叉查找树</p>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io//post-images/1581850416812.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>B-Tree（logn）</p>
<ol>
<li>根节点至少包括两个孩子</li>
<li>树中每个节点最多含有m个孩子（m&gt;=2）</li>
<li>除根节点和叶节点外，其他每个节点至少有ceil（m/2）个孩子</li>
<li>所有叶子节点都位于同一层</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://Zu3zz.github.io//post-images/1581850429749.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>B+树</p>
<ol>
<li>定义基本与B树相同</li>
<li>除了非叶子节点的指针必须与叶子结点数相同</li>
<li>非叶子节点的子树指针P[i]，指向关键字符[K[i]，K[i+1]]的子树</li>
<li>非叶子节点仅用来索引，数据都保存在叶子结点中</li>
<li>所有叶子节点均有一个链指针指向下一个叶子结点</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://Zu3zz.github.io//post-images/1581850440775.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>为什么B+树更适合用来做存储索引</p>
<ul>
<li>B+树的磁盘读写代价更低</li>
<li>B+树的查询效率更加稳定</li>
<li>B+树更有利于对数据库的扫描</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-hash索引">2. Hash索引</h3>
<figure data-type="image" tabindex="4"><img src="https://Zu3zz.github.io//post-images/1581850457121.png" alt="" loading="lazy"></figure>
<h4 id="缺点">缺点：</h4>
<ol>
<li>仅仅能满足“=”，“IN&quot;，不能使用范围查询</li>
<li>无法被用来避免数据的排序操作</li>
<li>不能利用部分索引键查询</li>
<li>不能避免表扫描</li>
<li>遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高</li>
</ol>
<hr>
<h3 id="3-bitmap索引了解">3. BitMap索引（了解）</h3>
<ul>
<li>位图索引</li>
</ul>
<hr>
<h3 id="4-密集索引与稀疏索引的区别">4. 密集索引与稀疏索引的区别</h3>
<ul>
<li>密集索引文件中的每个搜索码值都对应一个索引值</li>
<li>系数索引文件只为索引码的某些值建立索引项</li>
</ul>
<ol>
<li>对于InnoDB
<ol>
<li>若一个主键被定义，则该主键作为密集索引</li>
<li>若没有主键被定义，该表的第一个唯一非空索引则作为密集索引</li>
<li>若没有，innodb内部会生成一个隐藏主键（密集索引）</li>
<li>非主键索引存储相关键位和其对应的主键值，包含两次查找</li>
<li>索引和数据是存在一起的</li>
</ol>
</li>
<li>对于MyISAM
<ol>
<li>都是稀疏索引</li>
<li>索引和数据是分开的</li>
</ol>
</li>
</ol>
<hr>
<h3 id="5-问题回顾与总结">5. 问题回顾与总结</h3>
<ol>
<li>为什么要使用索引</li>
<li>什么样的信息能成为索引</li>
<li>索引的数据结构</li>
<li>密集索引和系数索引的区别</li>
</ol>
<hr>
<h3 id="6-一些衍生问题">6. 一些衍生问题</h3>
<h4 id="61-如何定位并优化慢查询sql">6.1 如何定位并优化慢查询Sql</h4>
<ol>
<li>根据慢日志定位慢查询sql</li>
<li>使用explain等工具分析sql
<ul>
<li>using filesort：文件排序 不能使用索引</li>
<li>using temporary：使用临时表</li>
</ul>
</li>
<li>修改sql或者尽量让sql走索引</li>
</ol>
<h4 id="62-联合索引的最左匹配原则的成因">6.2 联合索引的最左匹配原则的成因</h4>
<ul>
<li>MySQL会一直向右匹配到范围查询（&gt;、&lt;、between、like）就停止匹配</li>
</ul>
<pre><code class="language-sql"> a = 1 and b = 2 and c &gt; 3 and d = 4 
 如果遇到（a,b,c,d)这样的，到c就停止了
</code></pre>
<h4 id="63-索引是建立的越多越好吗">6.3 索引是建立的越多越好吗</h4>
<ul>
<li>否。数据量小的表不需要建立索引，建立或增加额外的索引开销</li>
<li>数据变更需要维护索引，因此更多的索引意味着更多的维护成本</li>
<li>更多的索引意味着也需要更多的空间</li>
</ul>
<h2 id="3-锁">3. 锁</h2>
<h3 id="31-常见问题">3.1 常见问题</h3>
<ol>
<li>MyISAM与InnoDB关于锁方面的区别是什么</li>
<li>数据库事务的四大特性</li>
<li>事务隔离级别以及各2级别下的并发访问问题</li>
<li>InnoDB可重复读隔离级别下如何避免幻读</li>
<li>RC、RR级别下的InnoDB的非阻塞读如何实现</li>
</ol>
<hr>
<h4 id="1-myisam与innodb关于锁方面的区别是什么">1. MyISAM与InnoDB关于锁方面的区别是什么</h4>
<ol>
<li>MyISAM默认用的是表级锁，不支持行级锁</li>
<li>InnoDB默认用的是行级锁，也支持表级锁</li>
<li>写锁 又叫 排它锁</li>
<li>读锁 又叫 共享锁</li>
<li>先读再写不会被锁</li>
<li>先写再读会被锁</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://Zu3zz.github.io//post-images/1581850500524.png" alt="" loading="lazy"></figure>
<h4 id="2-myisam适合的场景">2. MyISAM适合的场景</h4>
<ol>
<li>频繁执行全表count语句</li>
<li>对数据进行增删改的频率不搞，查询非常频繁</li>
<li>没有事务</li>
</ol>
<h4 id="3-innodb适合的场景">3. InnoDB适合的场景</h4>
<ol>
<li>数据增删改查都相当频繁</li>
<li>可靠性要求比较高，要求支持事务</li>
</ol>
<h3 id="32-数据库锁的分类">3.2 数据库锁的分类</h3>
<ul>
<li>按锁的粒度分化，可分为表级锁、行级锁、页级锁</li>
<li>按锁的级别划分，可分为共享锁、排它锁</li>
<li>按枷锁方式划分，课分为自动锁、显式锁</li>
<li>按操作划分，可分为DML锁、DDL锁</li>
<li>按使用方式划分，可分为乐观锁、悲观锁</li>
</ul>
<h4 id="1-乐观锁的实现方式">1. 乐观锁的实现方式</h4>
<ul>
<li>基于时间戳</li>
<li>基于版本号（写SQL语句的时候先检查version）</li>
</ul>
<h3 id="33-数据库事务的四大特性">3.3 数据库事务的四大特性</h3>
<ol>
<li>原子性（Atomic）：发生错误回滚</li>
<li>一致性（Consistency）</li>
<li>隔离性（Isolation）：一个事务的执行不应该影响到其他事务</li>
<li>持久性（Durability）：redo log</li>
</ol>
<h3 id="34-事务隔离级别以及各级别下的并发访问问题">3.4 事务隔离级别以及各级别下的并发访问问题</h3>
<ol>
<li>更新丢失：mysql所有事务隔离级别在数据库层面上均可避免</li>
<li>脏读：一个事务读到另外一个事务未提交的数据：READ——COMMITTED事务隔离级别以上可避免（MyISAM默认）
<ul>
<li>只能读到其他事务提交的级别，就不会造成问题了</li>
</ul>
</li>
<li>不可重复读：REPEATABLE-READ事务隔离级别以上可避免（InnoDB默认）</li>
<li>幻读：另外一个事务对同一个数据库进行了插入或者删除操作：SERIALIZABLE事务隔离级别可避免</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://Zu3zz.github.io//post-images/1581850513584.png" alt="" loading="lazy"></figure>
<h3 id="35-innodb课重复读隔离级别下如何避免幻读">3.5 InnoDB课重复读隔离级别下如何避免幻读</h3>
<ol>
<li>表象：快照读（非阻塞读）——伪MVCC
<ul>
<li>当前读：update、delete、insert、select...lock in share mode</li>
<li>快照读：不加锁的非阻塞读、select</li>
<li>再次调用快照读会出问题</li>
</ul>
</li>
<li>内在：next-key锁（行锁+gap锁）（搞不懂）
<ul>
<li>在Serializable下 是用的这个</li>
</ul>
</li>
</ol>
<h3 id="36-rc-rr级别下的innodb的非阻塞读如何实现">3.6 RC、RR级别下的InnoDB的非阻塞读如何实现</h3>
<ol>
<li>数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID字段</li>
<li>从 undo日志 里恢复</li>
<li>read view</li>
</ol>
<h2 id="4-语法">4. 语法</h2>
<ol>
<li>GROUP BY</li>
<li>HAVING</li>
<li>统计相关：COUNT、SUM、MAX、MIN、AVG</li>
</ol>
<h3 id="1-group-by分组">1. GROUP BY（分组）</h3>
<ul>
<li>满足 “select字句中的列名必须为分组列或者列函数”</li>
<li>列函数对于group by字句定义的每个组各返回一个结果</li>
<li>如果用group by，那么你的Select语句中选出的列要么是你group by里用到的列，要么就是带有之前我们说的如sum min等列函数的列，如果带上其他的，会报错。</li>
</ul>
<h3 id="2-having过滤">2. HAVING（过滤）</h3>
<ul>
<li>通常与GROUP BY字句一起使用</li>
<li>WHERE过滤行，HAVING过滤组</li>
<li>出现在同一行sql的顺序：WHERE&gt;GROUP BY&gt;HAVING</li>
</ul>
<h3 id="3自己多练">3.自己多练</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[计算机网络面试考点总结]]></title>
        <id>https://Zu3zz.github.io/post/interview-network/</id>
        <link href="https://Zu3zz.github.io/post/interview-network/">
        </link>
        <updated>2020-02-06T17:00:31.000Z</updated>
        <summary type="html"><![CDATA[<p>🐱一文总结计算机网络面试常见考点，包括TCP HTTP等🐱</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐱一文总结计算机网络面试常见考点，包括TCP HTTP等🐱</p>
<!-- more -->
<h1 id="计算机网络面试考点总结">计算机网络面试考点总结</h1>
<h2 id="1-tcp的三次握手">1. TCP的三次握手</h2>
<ol>
<li>第一次握手，建立连接时，客户端发送一个SYN包，SYN=j到服务器，并且进入SYN_SEND状态，等到服务器确认</li>
<li>服务器收到SYN包，必须确认客户的SYN值（ack=j+1），同时自己也发送一个SYN包，SYN=K，级SYN + ACK包，此时服务器也进入SYN_SEND状态</li>
<li>客户端收到服务器端的SYN+ACK包，向服务器发送确认包ACK（ack = k + 1），此包发送完毕之后，客户端和服务器都进入ESTABLISHED状态，三次握手完成。</li>
</ol>
<h4 id="11-为什么需要三次握手">1.1 为什么需要三次握手</h4>
<ul>
<li>为了初始化Seqence Number的初始值，用于拼接数据</li>
</ul>
<h4 id="12-syn超时">1.2 SYN超时</h4>
<ul>
<li>不断重试，Linux默认63秒</li>
</ul>
<h4 id="13-针对syn-flood">1.3 针对Syn Flood</h4>
<ul>
<li>
<p>syn队列满了之后，通过tcp_syncookies参数回发 SYN Cookie</p>
</li>
<li>
<p>若正常连接则Clinet会回发SYN Cookie，直接建立连接</p>
</li>
</ul>
<h4 id="14-client出现故障">1.4 Client出现故障</h4>
<ul>
<li>向对方发送保活探测报文</li>
</ul>
<h2 id="2-tcp四次挥手">2. TCP四次挥手</h2>
<ol>
<li>Client发送一个FIN，用来关系Client到Server的数据传送，Client进入FIN_WAIT_1状态</li>
<li>Server收到FIN之后，发送一个ACK给Client，确认需要为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态</li>
<li>Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态</li>
<li>Client收到FIN之后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。</li>
</ol>
<h2 id="3-tcp和udp">3. TCP和UDP</h2>
<ol>
<li>面向非连接，没有快重传等。</li>
<li>不维护连接状态，支持同时向多个客户端传输相同的消息</li>
<li>数据包报头只有8个字节</li>
<li>面向报文</li>
<li>尽最大努力交付</li>
</ol>
<hr>
<h3 id="31-区别">3.1 区别</h3>
<ol>
<li>面向连接 vs 无连接</li>
<li>可靠性</li>
<li>有序性</li>
<li>速度</li>
<li>量级</li>
</ol>
<hr>
<h3 id="32-滑动窗口协议">3.2 滑动窗口协议</h3>
<ol>
<li>RTT: 发送数据包到收到对应的ACK，所花费的时间</li>
<li>RTO: 重传时间间隔</li>
<li>使用滑动窗口做流量控制与乱序重拍
<ul>
<li>保证TCP的可靠性</li>
<li>保证TCP的流量控制特性</li>
</ul>
</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io//post-images/1581008533854.png" alt="" loading="lazy"></figure>
<h2 id="4-http11">4. HTTP(1.1)</h2>
<h3 id="1-主要特点">1. 主要特点</h3>
<ol>
<li>支持客户/服务器模式</li>
<li>简单快速</li>
<li>灵活</li>
<li>无连接</li>
</ol>
<hr>
<h3 id="2-请求相应的步骤">2. 请求/相应的步骤</h3>
<ol>
<li>客户端连接到Web服务器</li>
<li>发送HTTP请求</li>
<li>服务器接收请求并返回HTTP相应</li>
<li>释放连接TCP连接</li>
<li>客户端浏览器解析HTML内容</li>
</ol>
<hr>
<h3 id="3-输入url之后按下回车之后经过的流程">3. 输入URL之后，按下回车之后经过的流程</h3>
<ol>
<li>DNS解析 找对对应的IP地址</li>
<li>TCP连接</li>
<li>发送HTTP请求</li>
<li>服务器处理请求并返回HTTP报文</li>
<li>服务器解析渲染页面</li>
<li>连接结束</li>
</ol>
<hr>
<h3 id="4-http状态码">4. HTTP状态码</h3>
<ul>
<li>五种可能取值
<ul>
<li>1xx：指示信息--表示请求已接收，继续处理</li>
<li>2xx：成功--表示请求已被成功接收、理解、</li>
<li>3xx：重定向--要完成请求必须进行更进一步的操作</li>
<li>4xx：客户端错误--请求有语法错误或者请求无法实现</li>
<li>5xx：服务器端错误--服务器未能实现合法的请求</li>
</ul>
</li>
<li>常见状态码
<ul>
<li>200 OK：正常 返回信息</li>
<li>400 Bad Request：客户端请求有语法错误，不能被服务器所理解</li>
<li>401 Unauthorized：请求未经授权，这个状态码必须和WWW-Authenticate报头域一起使用</li>
<li>403 Forbidden：服务器收到请求，但是拒绝提供服务</li>
<li>404 Not Found：请求资源不存在，eg，输入了错误的URL</li>
<li>500 Internal Server Error：服务器发生不可预期的错误</li>
<li>503 Server Unabailable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-get和post请求的区别">5. GET和POST请求的区别</h3>
<h4 id="1-从三个层面来解答">1. 从三个层面来解答</h4>
<ol>
<li>Http报文层面：GET将请求信息（键值对）放到URL中，POST则放在报文体中</li>
<li>数据库层面：GET符合幂等性和安全性，POST不符合（两个都不符合）</li>
<li>其他层面：GET可以被缓存、被存储，而POST不行</li>
</ol>
<hr>
<h3 id="6-cookie和session的区别">6. Cookie和Session的区别</h3>
<h4 id="cookie简介">Cookie简介</h4>
<ul>
<li>是由服务器发给客户端的特殊信息，以文本的形式存放在客户端</li>
<li>客户端再次请求的时候，会吧Cookie回发</li>
<li>服务器接收到后，会解析Cookie生成与客户端相对应的内容</li>
</ul>
<h4 id="cookie的设置以及发送过程">Cookie的设置以及发送过程</h4>
<figure data-type="image" tabindex="2"><img src="https://Zu3zz.github.io//post-images/1581008546981.png" alt="" loading="lazy"></figure>
<h4 id="session简介">Session简介</h4>
<ul>
<li>服务器端的机制，在服务器上保存的信息（类似hash表）</li>
<li>解析客户端请求并操作session id，按需保存状态信息</li>
</ul>
<h4 id="session的实现方式">Session的实现方式</h4>
<ol>
<li>使用Cookie来实现：服务器给客户端一个JSessionID</li>
<li>使用URL回写来实现</li>
</ol>
<h4 id="cookie和session的区别">Cookie和Session的区别</h4>
<ol>
<li>Cookie数据存放在客户的浏览器上，Session数据放在服务器上</li>
<li>Session相对于Cookie更安全</li>
<li>若考虑减轻服务器负担，应当使用Cookie</li>
</ol>
<h2 id="5-http和https的区别">5 HTTP和HTTPS的区别</h2>
<h3 id="1-https简介">1. HTTPS简介</h3>
<figure data-type="image" tabindex="3"><img src="https://Zu3zz.github.io//post-images/1581008556887.png" alt="" loading="lazy"></figure>
<hr>
<h3 id="2-sslsecurity-sockets-layer-安全套接层">2. SSL（Security Sockets Layer, 安全套接层）</h3>
<ol>
<li>为网络通信提供安全以及数据完整性的一种安全协议</li>
<li>是操作系统对外的API，SSL3.0后更名为TLS</li>
<li>采用身份验证和数据加密保证网络通信的安全和数据的完整性</li>
</ol>
<hr>
<h3 id="3-加密方式">3. 加密方式</h3>
<ol>
<li>对称加密：加密和解密都使用同一个密钥</li>
<li>非对称加密：加密使用的密钥和解密使用的密钥是不相同的</li>
<li>哈希算法：将任意长度的信息转换为固定长度的值，算法不可逆</li>
<li>数字签名：证明某个信息或者文件是某个人发出/认同的</li>
</ol>
<hr>
<h3 id="4-数据传输流程">4. 数据传输流程</h3>
<ol>
<li>浏览器将支持的加密算法信息发送给服务器</li>
<li>服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器</li>
<li>浏览器验证证书合法性，并结合证书公钥加密信息发送给服务器</li>
<li>服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器</li>
<li>浏览器解密响应信息，并对消息进行验证，之后就用同一套秘钥进行加密交换</li>
</ol>
<hr>
<h3 id="5-http和https的区别-2">5. HTTP和HTTPS的区别</h3>
<ol>
<li>HTTPS需要到CA申请证书，HTTP不需要</li>
<li>HTTPS密文传输，HTTP明文传输</li>
<li>连接方式不同，HTTPS默认使用443端口，HTTP使用80端口</li>
<li>HTTPS = HTTP+加密+认证+完整性保护，较HTTP安全</li>
</ol>
<hr>
<h3 id="6-潜在危险">6. 潜在危险</h3>
<ol>
<li>浏览器默认填充http：// ，请求需要进行跳转，又被劫持的风险</li>
<li>可以使用HSTS（HTTP Strict Transport Security）优化</li>
</ol>
<h2 id="6-socket">6. Socket</h2>
<ul>
<li>Socket是对TCP/IP协议的抽象，是操作系统对外开放的接口</li>
</ul>
<h3 id="1-socket通信流程">1. Socket通信流程</h3>
<figure data-type="image" tabindex="4"><img src="https://Zu3zz.github.io//post-images/1581008567663.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkSQL实战笔记（2）---- SparkSQL 概述]]></title>
        <id>https://Zu3zz.github.io/post/sparksql-2/</id>
        <link href="https://Zu3zz.github.io/post/sparksql-2/">
        </link>
        <updated>2020-01-14T11:46:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="sparksql-实战笔记2-sparksql-概述">SparkSQL 实战笔记（2）---- SparkSQL 概述</h1>
<p>##1. 为什么需要 SQL？</p>
<ol>
<li>
<p>事实上的标准</p>
<ul>
<li>
<p>MySQL/Oracle/DB2... RBDMS 关系型数据库 是不是过时呢？</p>
</li>
<li>
<p>数据规模 大数据的处理</p>
<pre><code class="language-shell">MR：Java
Spark：Scala、Java、Python
</code></pre>
</li>
<li>
<p>直接使用 SQL 语句来对数据进行处理分析呢？ 符合市场的需求</p>
<pre><code class="language-shell">Hive SparkSQL Impala...
</code></pre>
</li>
<li>
<p>受众面大、容易上手、易学易用:<code>DDL DML</code></p>
<pre><code class="language-shell"># access.log日志
1,zhangsan,10,beijing
2,lisi,11,shanghai
3,wangwu,12,shenzhen
</code></pre>
</li>
<li>
<p><code>table: Hive/Spark SQL/Impala</code> ：共享元数据</p>
<ul>
<li>name: access</li>
<li>columns: id int,name string,age int,city string</li>
</ul>
<pre><code class="language-mysql">SQL: select xxx from access where ... group by ... having....
</code></pre>
</li>
</ul>
</li>
</ol>
<h2 id="2-sql-on-hadoop">2. SQL on Hadoop</h2>
<ol>
<li>
<p>使用 SQL 语句对大数据进行统计分析，数据是在 Hadoop</p>
</li>
<li>
<p><code>Apache Hive</code></p>
<ul>
<li>SQL 转换成一系列可以在 Hadoop 上运行的 MapReduce/Tez/Spark 作业</li>
<li>SQL 到底底层是运行在哪种分布式引擎之上的，是可以通过一个参数来设置</li>
<li>功能：
<ul>
<li>SQL：命令行、代码</li>
<li>多语言 Apache Thrift 驱动</li>
<li>自定义的 UDF 函数：按照标准接口实现，打包，加载到 Hive 中</li>
<li>元数据</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Cloudera Impala</code></p>
<ul>
<li>使用了自己的执行守护进程集合，一般情况下这些进程是需要与 Hadoop DN 安装在一个节点上</li>
<li>功能：
<ul>
<li>92 SQL 支持</li>
<li>Hive 支持</li>
<li>命令行、代码</li>
<li>与 Hive 能够共享元数据</li>
<li>性能方面是 Hive 要快速一些，基于内存</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Spark SQL</code></p>
<ul>
<li>
<p>Spark 中的一个子模块，是不是仅仅只用 SQL 来处理呢？</p>
<pre><code class="language-shell">$ Hive：SQL ==&gt; MapReduce
</code></pre>
</li>
<li>
<p><code>Spark</code>：能不能直接把 SQL 运行在 Spark 引擎之上呢？</p>
<ol>
<li><code>Shark</code>： <code>SQL==&gt;Spark</code> （不再维护）
<ul>
<li>优点：快 与 Hive 能够兼容</li>
<li>缺点：执行计划优化完全依赖于 Hive 进程 vs 线程</li>
<li>使用：需要独立维护一个打了补丁的 Hive 源码分支</li>
</ul>
</li>
<li><code>Spark SQL</code>: 这是 Spark 项目中的<code>SQL</code>子项目</li>
</ol>
</li>
</ul>
</li>
<li>
<p><code>Hive on Spark</code> ： 这是<code>Hive</code>项目中的，通过切换 Hive 的执行引擎即可，底层添加了 Spark 执行引擎的支持</p>
</li>
<li>
<p><code>Presto</code></p>
<ul>
<li>交互式查询引擎 SQL</li>
<li>功能：
<ul>
<li>共享元数据信息</li>
<li>92 SQL 语法</li>
<li>提供了一系列的连接器，<code>Hive</code> <code>Cassandra</code>...</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Drill</code></p>
<ol>
<li>HDFS、Hive、Spark SQL</li>
<li>支持多种后端存储，然后直接进行各种后端数据的处理</li>
<li>未来的趋势</li>
</ol>
</li>
<li>
<p><code>Phoenix</code></p>
<ol>
<li>HBase 的数据，是要基于 API 进行查询</li>
<li>Phoenix 使用 SQL 来查询 HBase 中的数据</li>
<li>主要点：如果想查询的快的话，还是取决于 ROWKEY 的设计</li>
</ol>
</li>
</ol>
<h2 id="3-spark-sql-是什么">3. Spark SQL 是什么</h2>
<h3 id="31-spark-sql-概念">3.1 Spark SQL 概念</h3>
<ol>
<li>
<p>Spark SQL is Apache Spark's module for working with structured data.</p>
<ul>
<li>误区一：Spark SQL 就是一个 SQL 处理框架</li>
</ul>
<ol>
<li>
<p>集成性：在 Spark 编程中无缝对接多种复杂的 SQL</p>
</li>
<li>
<p>统一的数据访问方式：以类似的方式访问多种不同的数据源，而且可以进行相关操作</p>
<pre><code class="language-scala">spark.read.format(&quot;json&quot;).load(path)
spark.read.format(&quot;text&quot;).load(path)
spark.read.format(&quot;parquet&quot;).load(path)
spark.read.format(&quot;json&quot;).option(&quot;...&quot;,&quot;...&quot;).load(path)
</code></pre>
</li>
<li>
<p>兼容 Hive</p>
<ul>
<li>allowing you to access existing Hive warehouses</li>
<li>如果你想把 Hive 的作业迁移到 Spark SQL，这样的话，迁移成本就会低很多</li>
</ul>
</li>
<li>
<p>标准的数据连接：提供标准的<code>JDBC/ODBC</code>连接方式到 Server 上</p>
</li>
</ol>
</li>
<li>
<p>Spark SQL 应用并不局限于 SQL</p>
<ol>
<li>还支持 Hive、JSON、Parquet 文件的直接读取以及操作</li>
<li>SQL 仅仅是 Spark SQL 中的一个功能而已</li>
</ol>
</li>
<li>
<p>为什么要学习 Spark SQL</p>
<ol>
<li>SQL 带来的便利性</li>
<li>Spark Core： RDD Scala/Java
<ul>
<li>需要熟悉 Java、Scala 语言</li>
</ul>
</li>
<li>Spark SQL
<ul>
<li>Catalyst 为我们自动做了很多的优化工作</li>
<li>SQL(只要了解业务逻辑，然后使用 SQL 来实现)</li>
<li>DF/DS：面向 API 编程的，使用一些 Java/Scala</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="32-spark-sql-架构">3.2 Spark SQL 架构</h3>
<h4 id="321-前端frontend">3.2.1 前端（FrontEnd）</h4>
<ol>
<li>
<p>Hive AST : SQL 语句（字符串）==&gt; 抽象语法树</p>
</li>
<li>
<p>Spark Program : DF/DS API</p>
</li>
<li>
<p>Streaming SQL</p>
</li>
<li>
<p>Catalyst</p>
<ul>
<li>Unresolved LogicPlan</li>
</ul>
<pre><code class="language-mysql">select empno, ename from emp
</code></pre>
</li>
<li>
<p>Schema Catalog 和 MetaStore</p>
</li>
<li>
<p>LogicPlan</p>
</li>
<li>
<p>Optimized LogicPlan</p>
<pre><code class="language-mysql">select * from (select ... from xxx limit 10) limit 5;
将我们的SQL作用上很多内置的Rule，使得我们拿到的逻辑执行计划是比较好的
</code></pre>
<p><code>Physical Plan</code></p>
</li>
</ol>
<h4 id="322-后端backend">3.2.2 后端（Backend）</h4>
<ol>
<li>
<p><code>spark-shell</code></p>
<ul>
<li>每个 Spark 应用程序（spark-shell）在不同目录下启动，其实在该目录下是有 metastore_db</li>
<li>单独的</li>
<li>如果你想 spark-shell 共享我们的元数据的话，肯定要指定元数据信息==&gt; 后续讲 Spark SQL 整合 Hive 的时候讲解</li>
<li><code>spark.sql</code>(sql 语句)</li>
</ul>
</li>
<li>
<p>spark-sql 的使用<br>
spark-shell 你会发现如果要操作 SQL 相关的东西，要使用 spark.sql(sql 语句)</p>
<pre><code class="language-mysql">explain extended
select a.key\*(3+5), b.value from t a join t b on a.key = b.key and a.key &gt; 3;
</code></pre>
<ul>
<li>优化的过程中，可以把一些条件过滤前置</li>
</ul>
</li>
<li>
<p>spark-shell 启动流程分析</p>
<ul>
<li>
<p>REPL: Read-Eval-Print Loop 读取-求值-输出</p>
</li>
<li>
<p>提供给用户即时交互一个命令窗口</p>
<pre><code class="language-mysql">case \$变量名 in
模式 1
command1
;;
模式 2
command2
;;
\*)
default
;;
esac
</code></pre>
</li>
<li>
<p>spark-shell 底层调用的是 spark-submit</p>
</li>
<li>
<p>spark-submit 底层调用的是 spark-class</p>
</li>
</ul>
</li>
<li>
<p><strong>spark-sql 执行流程分析</strong></p>
<ul>
<li>spark-sql 底层调用的也是 spark-submit</li>
<li>因为 spark-sql 它就是一个 Spark 应用程序，和 spark-shell 一样</li>
<li>对于你想启动一个 Spark 应用程序，肯定要借助于 spark-submit 这脚本进行提交</li>
<li>spark-sql 调用的类是<code>org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver</code></li>
<li>spark-shell 调用的类是 <code>org.apache.spark.repl.Main</code></li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkSQL实战笔记（1）---- 部署、项目准备]]></title>
        <id>https://Zu3zz.github.io/post/sparksql-1/</id>
        <link href="https://Zu3zz.github.io/post/sparksql-1/">
        </link>
        <updated>2020-01-13T08:43:13.000Z</updated>
        <content type="html"><![CDATA[<h1 id="sparksql实战笔记1-部署-项目准备">SparkSQL实战笔记（1）---- 部署、项目准备</h1>
<h2 id="1-关于mapreduce的问题">1. 关于MapReduce的问题</h2>
<ul>
<li>
<p>MapReduce的槽点一</p>
<ul>
<li>
<p>需求：统计单词出现的个数（词频统计）</p>
<ul>
<li>
<p>file中每个单词出现的次数</p>
<pre><code class="language-txt">hello,hello,hello
world,world
pk
</code></pre>
</li>
</ul>
</li>
</ul>
<ol>
<li>读取file中每一行的数据</li>
<li>按照分隔符把每一行的内容进行拆分</li>
<li>按照相同的key分发到同一个任务上去进行累加的操作</li>
</ol>
<ul>
<li>
<p>这是一个简单的不能再简单的一个需求，我们需要开发很多的代码</p>
<ol>
<li>自定义Mapper</li>
<li>自定义Reducer</li>
<li>通过Driver把Mapper和Reducer串起来</li>
<li>打包，上传到集群上去</li>
<li>在集群上提交我们的wc程序</li>
</ol>
</li>
<li>
<p>一句话：就是会花费非常多的时间在非业务逻辑改动的工作上</p>
</li>
</ul>
</li>
<li>
<p>MapReduce吐槽点二</p>
<pre><code class="language-shell">Input =&gt; MapReduce ==&gt; Output ==&gt; MapReduce ==&gt; Output
</code></pre>
</li>
<li>
<p>回顾下MapReduce执行流程：</p>
<ul>
<li>MapTask或者ReduceTask都是进程级别</li>
<li>第一个MR的输出要先落地，然后第二个MR把第一个MR的输出当做输入</li>
<li>中间过程的数据是要落地</li>
</ul>
</li>
</ul>
<h2 id="2-spark">2. Spark</h2>
<ol>
<li>
<p>特性</p>
<ol>
<li>
<p>Speed:</p>
<ul>
<li>
<p>both batch and streaming data</p>
</li>
<li>
<p>批流一体 Spark Flink</p>
</li>
</ul>
</li>
<li>
<p>Ease of Use</p>
<ul>
<li>high-level operators</li>
</ul>
</li>
<li>
<p>Generality</p>
<ul>
<li>stack  栈   生态</li>
</ul>
</li>
<li>
<p>Runs Everywhere</p>
<ul>
<li>It can access diverse data sources</li>
<li>YARN/Local/Standalone Spark应用程序的代码需要改动吗？</li>
<li>--master来指定你的Spark应用程序将要运行在什么模式下</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-部署问题">3. 部署问题</h2>
<h3 id="31-jdk部署">3.1 <code>JDK</code>部署</h3>
<ul>
<li>
<p>下载：https://www.oracle.com/index.html</p>
</li>
<li>
<p>服务器端：</p>
<ul>
<li>
<p>下载linux版本的jdk</p>
</li>
<li>
<p>解压：<code>tar -zxvf jdk-8u91-linux-x64.tar.gz -C ~/app</code></p>
</li>
<li>
<p>配置环境变量： <code>~/.bash_profile</code></p>
<pre><code class="language-shell">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91
export PATH=$JAVA_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>使环境变量生效：<code>source ~/.bash_profile</code></p>
</li>
</ul>
</li>
<li>
<p>客户端：Win/Mac/Linux</p>
<ul>
<li>Mac/Linux：就和服务器端安装方法一致</li>
</ul>
</li>
</ul>
<h3 id="32-maven和idea部署">3.2 <code>Maven</code>和<code>IDEA</code>部署</h3>
<ol>
<li>
<p>Maven：IDEA+Maven来管理应用程序</p>
<ul>
<li>为什么你开发的时候不直接拷贝jar包呢？</li>
<li>在maven中的pom.xml中添加我们所需要的dependency就行</li>
</ul>
</li>
<li>
<p>官网：maven.apache.org</p>
<ul>
<li>
<p><code>wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz</code></p>
</li>
<li>
<p>解压：<code>tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ~/app/</code></p>
</li>
<li>
<p>配置环境变量：<code>~/.bash_profile</code></p>
<pre><code class="language-shell">export MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.1
export PATH=$MAVEN_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>使环境变量生效：<code>source ~/.bash_profile</code></p>
</li>
<li>
<p>服务器端：你是需要进行使用maven来编译我们的spark</p>
</li>
<li>
<p>客户端：Win/Mac/Linux</p>
</li>
<li>
<p>我们开发应用程序是在本地/本机，IDEA+Maven，所以本地也是需要安装maven的</p>
</li>
<li>
<p>本地Win/Mac/Linux的maven安装方式和服务器端是一模一样的</p>
</li>
<li>
<p>如果你是win用户，一定要注意: $MAVEN_HOME/conf/setting.xml</p>
<pre><code class="language-xml">&lt;!-- localRepository
	   | The path to the local repository maven will use to store artifacts.
	   |
	   | Default: ${user.home}/.m2/repository
	  &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;
	  --&gt;
</code></pre>
</li>
<li>
<p>Win用户，默认是在C盘，所以建议大家更改Maven本地仓库的路径</p>
</li>
</ul>
</li>
<li>
<p>IDEA官网：http://www.jetbrains.com/</p>
</li>
</ol>
<h3 id="33-hadoop部署">3.3 <code>Hadoop</code>部署</h3>
<h4 id="331-使用cdh-cdh5151">3.3.1 使用<code>CDH</code> <code>cdh5.15.1</code></h4>
<ul>
<li>
<p>下载地址：https://archive.cloudera.com/cdh5/cdh/5/</p>
</li>
<li>
<p>Hadoop：<code>wget https://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</code></p>
</li>
<li>
<p>解压：<code>tar -zxvf hadoop-2.6.0-cdh5.15.1.tar.gz -C ~/app/</code></p>
</li>
<li>
<p>修改<code>hadoop-env.sh</code></p>
<pre><code class="language-shell">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91
</code></pre>
</li>
<li>
<p>修改core-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;fs.default.name&lt;/name&gt;
	&lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改hdfs-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
	&lt;value&gt;/home/hadoop/tmp/dfs/data&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
	&lt;name&gt;dfs.replication&lt;/name&gt;
	&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
	&lt;name&gt;dfs.permissions&lt;/name&gt;
	&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改yarn-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改mapred-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
	&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改slaves（可选）</p>
<ul>
<li>hadoop000</li>
</ul>
</li>
<li>
<p>配置系统环境变量</p>
<pre><code class="language-shell">export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.15.1
export PATH=$HADOOP_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>配置SSH的免密码登录</p>
</li>
<li>
<p>在启动HDFS之前，一定要先对HDFS对格式化</p>
<ul>
<li>切记：格式化只会一次，因为一旦格式化了，那么HDFS上的数据就没了</li>
<li>格式化命令：<code>hdfs namenode -format</code></li>
</ul>
</li>
<li>
<p>启动HDFS</p>
<ol>
<li>
<p>逐个进程启动/停止</p>
<pre><code class="language-shell">$ hadoop-daemon.sh start/stop namenode
$ hadoop-daemon.sh start/stop datanode
</code></pre>
<ul>
<li>jps验证</li>
<li>如果发现有缺失的进程，那么就找缺失进程的名称对应的日志(log而不是out)</li>
</ul>
</li>
<li>
<p>一键式启动HDFS</p>
<pre><code class="language-shell">$ start-dfs.sh
$ stop-dfs.sh
</code></pre>
</li>
</ol>
</li>
</ul>
<h3 id="34-hive部署">3.4 Hive部署</h3>
<ol>
<li>
<p>Hadoop：wget https://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.15.1.tar.gz</p>
</li>
<li>
<p>系统环境变量</p>
<pre><code class="language-shell">export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.15.1
export PATH=$HIVE_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>需要安装<code>MySQL</code> 与<code>yum</code></p>
<ul>
<li>
<p>需要拷贝MySQL的驱动$HIVE_HOME/lib  版本5.x</p>
</li>
<li>
<p>修改<code>$HIVE_HOME/conf/hive-site.xml</code>文件</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://localhost:3306/pk?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre>
</li>
</ul>
</li>
<li>
<p>Hive: HDFS上的数据 + MySQL中元数据信息</p>
</li>
</ol>
<h2 id="4-spark运行模式">4. Spark运行模式</h2>
<ol>
<li>local：本地运行，在开发代码的时候，我们使用该模式进行<strong>测试</strong>是非常方便的</li>
<li>standalone：Hadoop部署多个节点的，同理Spark可以部署多个节点  <strong>用的不多</strong></li>
<li>YARN：将Spark作业提交到Hadoop(YARN)集群中运行，Spark仅仅只是一个客户端而已 <strong>最多的用法</strong></li>
<li>Mesos：不常用</li>
<li>K8S：2.3版本才正式稍微稳定   是未来比较好的一个方向</li>
<li>补充：运行模式和代码没有任何关系，同一份代码可以不做修改运行在不同的运行模式下</li>
</ol>
<h2 id="5-构建应用">5. 构建应用</h2>
<ol>
<li>
<p>使用<code>IDEA</code>+<code>Maven</code>来构建我们的Spark应用</p>
</li>
<li>
<p>在命令行中运行一下<code>MAVEN</code>命令</p>
<pre><code class="language-shell">mvn archetype:generate -DarchetypeGroupId=net.alchim31.maven \
-DarchetypeArtifactId=scala-archetype-simple \
-DremoteRepositories=http://scala-tools.org/repo-releases \
-DarchetypeVersion=1.5 \
-DgroupId=com.imooc.bigdata \
-DartifactId=sparksql-train \
-Dversion=1.0
</code></pre>
</li>
<li>
<p>打开IDEA，把这个项目中的pom.xml打开即可</p>
</li>
<li>
<p>同时，在<code>pom.xml</code>中添加一下相关配置</p>
<pre><code class="language-xml">pom.xml
&lt;properties&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
    &lt;scala.tools.version&gt;2.11&lt;/scala.tools.version&gt;
    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;
    &lt;spark.version&gt;2.4.3&lt;/spark.version&gt;
    &lt;hadoop.version&gt;2.6.0-cdh5.15.1&lt;/hadoop.version&gt;
&lt;/properties&gt;	

添加CDH的仓库
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;

添加Spark SQL和Hadoop Client的依赖
&lt;!--Spark SQL依赖--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;${spark.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- Hadoop相关依赖--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;${hadoop.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
</ol>
<h2 id="6-实战词频统计案例">6. 实战：词频统计案例</h2>
<ol>
<li>
<p>输入：文件</p>
<ul>
<li>需求：统计出文件中每个单词出现的次数
<ol>
<li>读每一行数据</li>
<li>按照分隔符把每一行的数据拆成单词</li>
<li>每个单词赋上次数为1</li>
<li>按照单词进行分发，然后统计单词出现的次数</li>
<li>把结果输出到文件中</li>
</ol>
</li>
</ul>
</li>
<li>
<p>输出：文件</p>
</li>
<li>
<p>使用local模式运行spark-shell</p>
<pre><code class="language-shell">./spark-shell --master local
</code></pre>
<ul>
<li>
<p>打包我们的应用程序，让其运行在local模式下</p>
</li>
<li>
<p>如何运行jar包呢？</p>
</li>
</ul>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master local \
/home/hadoop/lib/sparksql-train-1.0.jar \
file:///home/hadoop/data/wc.data file:///home/hadoop/data/out 
</code></pre>
<ul>
<li>使用local模式的话，你只需要把spark的安装包解压开，什么都不用动，就能使用</li>
</ul>
</li>
<li>
<p>如何提交Spark应用程序到YARN上执行</p>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master yarn \
--name SparkWordCountAppV2 \
/home/hadoop/lib/sparksql-train-1.0.jar \
hdfs://hadoop000:8020/pk/wc.data hdfs://hadoop000:8020/pk/out
</code></pre>
</li>
<li>
<p>要将Spark应用程序运行在YARN上，一定要配置<code>HADOOP_CONF_DIR</code>或者<code>YARN_CONF_DIR</code></p>
<p>指向<code>$HADOOP_HOME/etc/conf</code></p>
</li>
<li>
<p>local和YARN模式：重点掌握</p>
</li>
<li>
<p>Standalone：了解</p>
<ul>
<li>
<p>多个机器，那么你每个机器都需要部署spark</p>
</li>
<li>
<p>相关配置：</p>
<pre><code class="language-shell">$SPARK_HOME/conf/slaves
	hadoop000
SPARK_HOME/conf/spark-env.sh
	SPARK_MASTER_HOST=hadoop000
</code></pre>
</li>
<li>
<p>启动Spark集群</p>
<pre><code class="language-shell">$SPARK_HOME/sbin/start-all.sh
jps： Master  Worker
</code></pre>
</li>
<li>
<p>spark提交作业</p>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master spark://hadoop000:7077 \
--name SparkWordCountAppV2 \
/home/hadoop/lib/sparksql-train-1.0.jar \
hdfs://hadoop000:8020/pk/wc.data hdfs://hadoop000:8020/pk/out2
</code></pre>
</li>
<li>
<p>不管什么运行模式，代码不用改变，只需要在<code>spark-submit</code>脚本提交时</p>
<p>通过<code>--master xxx</code> 来设置你的运行模式即可</p>
</li>
</ul>
</li>
</ol>
<h2 id="7-实战代码">7. 实战代码</h2>
<ul>
<li><code>Scala</code>版本</li>
</ul>
<pre><code class="language-scala">package com.zth.bigdata.examples
import org.apache.spark.{SparkConf, SparkContext}
/**
 * Author: 3zZ.
 * Date: 2020/1/13 3:14 下午
 */
object SparkWordCountApp {
  def main(args: Array[String]): Unit = {
    /**
     * master: 运行模式 local
     */
    val sparkConf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;SparkWordCountApp&quot;)
    val sc = new SparkContext(sparkConf)

    val rdd = sc.textFile(&quot;/Users/3zz/Code/Spark/spark-sql-train/data/input.txt&quot;)
    /**
     * 按照单词个数进行降序排列
     */
    rdd.flatMap(_.split(&quot;,&quot;)).map((_, 1))
      .reduceByKey(_ + _).map(x =&gt; (x._2, x._1))
      .sortByKey(false).map(x =&gt;(x._2,x._1))
      .collect().foreach(println)
    //      .saveAsTextFile(&quot;/Users/3zz/Code/Spark/spark-sql-train/data/out&quot;).
    sc.stop()
  }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HBase学习笔记 ---- 整合篇]]></title>
        <id>https://Zu3zz.github.io/post/hbase-1/</id>
        <link href="https://Zu3zz.github.io/post/hbase-1/">
        </link>
        <updated>2020-01-11T15:54:39.000Z</updated>
        <content type="html"><![CDATA[<h1 id="hbase">Hbase</h1>
<h2 id="1-数据存储">1. 数据存储</h2>
<h3 id="11-rdbms">1.1 RDBMS:</h3>
<ol>
<li>
<p>Data is typed structured before stored</p>
</li>
<li>
<p>传统SQL</p>
<table>
<thead>
<tr>
<th>data</th>
<th>location</th>
</tr>
</thead>
<tbody>
<tr>
<td>entity</td>
<td>table</td>
</tr>
<tr>
<td>record</td>
<td>row</td>
</tr>
<tr>
<td>query</td>
<td>group by 、join</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>大数据时代，如何做实时查询？</p>
</li>
<li>
<p>hadoop/HDFS：能存 没法进行实时查询，随机读写</p>
</li>
<li>
<p>NoSQL（NOT only SQL）：HBase、Redis</p>
</li>
</ol>
<h3 id="12-hbase在hadoop生态圈中的位置">1.2 Hbase在Hadoop生态圈中的位置</h3>
<ol>
<li>HBase是Hadoop生态圈中的一个重要组成部分</li>
<li>HBase是构建在HDFS纸上，也就是说HBase的数据可以存储在HDFS上面</li>
<li>可以通过MapReduce/Spark来处理Hbase中的数据</li>
<li>HBase也提供了shell、API的方式进行数据的访问</li>
</ol>
<h3 id="13-行式vs列式">1.3 行式VS列式</h3>
<ol>
<li>
<p>行式</p>
<ul>
<li>
<p>按行存储</p>
</li>
<li>
<p>没有索引查询的时候需要耗费大量的IO</p>
</li>
<li>
<p>可以通过建立索引或者视图来提速</p>
</li>
<li>
<p>1,3zz,23</p>
</li>
</ul>
</li>
<li>
<p>列式</p>
<ul>
<li>压缩、并行处理</li>
<li>数据就是索引，大大降低IO</li>
</ul>
</li>
</ol>
<h3 id="14-hbase特点">1.4 HBase特点</h3>
<ol>
<li>
<p>大：数据量大</p>
</li>
<li>
<p>面向列：列族（可以存放很多列），列族/列独立索引</p>
</li>
<li>
<p>稀疏：</p>
<table>
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>age</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3zz</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>4xx</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>5yy</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
</li>
<li>
<p>数据类型单一：byte/string</p>
</li>
<li>
<p>无模式：每一行的数据所对应的列不一定相同，每行的列是可以动态添加的</p>
<pre><code class="language-shell">3zz age/birthday/company
4xx company/province/city
</code></pre>
</li>
<li>
<p>数据多版本：比如company可以存放不同的版本的值</p>
<p>默认情况下版本号是自动分配的，是列的值插入时的时间戳</p>
</li>
</ol>
<h3 id="15-hbase-vs-mysql">1.5 HBase VS MySQL</h3>
<ol>
<li>
<p>数据类型不同</p>
</li>
<li>
<p>数据操作：</p>
<ol>
<li>关联查询：MapReduce/Spark/Phoenix</li>
<li>get/put/scan...</li>
</ol>
</li>
<li>
<p>存储模式</p>
<ol>
<li>
<p>MySQL</p>
<table>
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>age</th>
<th>tel</th>
<th>Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>a</td>
<td>11</td>
<td>123</td>
<td>...</td>
</tr>
<tr>
<td>2</td>
<td>b</td>
<td>22</td>
<td>456</td>
<td>...</td>
</tr>
<tr>
<td>3</td>
<td>c</td>
<td>33</td>
<td>789</td>
<td>...</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>在HBase中</p>
<pre><code class="language-shell">basic_info: id	name 
private_info: age	tel		address
</code></pre>
</li>
</ol>
</li>
<li>
<p>transaction事务性：单行</p>
</li>
<li>
<p>数据量：HBase大</p>
</li>
<li>
<p>吞吐量：Million级别</p>
</li>
</ol>
<h3 id="hbase-vs-hdfs">HBase vs HDFS</h3>
<ol>
<li>write pattern（写模式）</li>
<li>read pattern（读模式）</li>
<li>SQL</li>
<li>data size（数据量）</li>
</ol>
<h3 id="hbase优势">HBase优势</h3>
<ol>
<li>成熟</li>
<li>高效</li>
<li>分布式</li>
</ol>
<h3 id="数据模型">数据模型</h3>
<ol>
<li>
<p>rowkey</p>
<ul>
<li>
<p>主键</p>
</li>
<li>
<p>字符串，按字典顺序存储，在HBase内部保存的是字节数组</p>
</li>
</ul>
</li>
<li>
<p>列族：Column Family （CF）</p>
<ul>
<li>
<p>是在创建表的时候就要指定的</p>
</li>
<li>
<p>列族是一系列列的集合</p>
</li>
<li>
<p>一个列族所有列有着相同的前缀</p>
<pre><code class="language-shell">basic_info: id
basic_info: name
private_info: age
private_info: tel
private_info: address
</code></pre>
</li>
</ul>
</li>
<li>
<p>列：Column / Qualifier</p>
<ul>
<li>属于某一个列族</li>
</ul>
</li>
<li>
<p>每条记录被划分到若干个CF中，每条记录对应一个rowkey，每个CF由一个或者多个Column构成</p>
</li>
<li>
<p>存储单元：Cell</p>
<ul>
<li>HBase中ROW和Column确定的一个存储单元</li>
<li>每个Cell都保存这同一份数据的多个版本</li>
<li>在写入数据时，时间戳可以由HBase自动赋值，也可以显示赋值</li>
<li>每个Cell中，不同版本的数据按照时间戳的倒序排列</li>
</ul>
<pre><code class="language-shell">{rowkey, column, version} ==&gt; HBase 中的一个Cell
</code></pre>
</li>
</ol>
<h2 id="2-hbase安装">2. HBase安装</h2>
<h3 id="21-前置需要">2.1 前置需要</h3>
<ol>
<li>JDK（略过）</li>
<li>ZooKeeper安装（brew）</li>
<li>Hadoop安装（使用cdh版本，详细配置参照前面博客）</li>
</ol>
<h3 id="22hbase安装">2.2HBase安装：</h3>
<ol>
<li>
<p>在.bash_profile中添加HBASE_HOME</p>
</li>
<li>
<p>在HBase的<code>conf</code>目录中修改两个文件</p>
<ol>
<li>
<p>在<code>hbase-env.sh</code>文件中</p>
<pre><code class="language-shell"># 1.修改java的路径
# The java implementation to use.  Java 1.7+ required.
# export JAVA_HOME=/usr/java/jdk1.6.0/
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-12.0.1.jdk/Contents/Home
# 注意 这里一定要是用1.8的版本！ 上面使用java12是要出大问题的


# 2.修改zookeeper配置
# Tell HBase whether it should manage it's own instance of Zookeeper or not.
export HBASE_MANAGES_ZK=false
</code></pre>
</li>
<li>
<p>在<code>hbase-site.xml</code>中配置</p>
<pre><code class="language-xml">&lt;configuration&gt;
&lt;!--hbase的数据存放地址（本机）--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.rootdir&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;
&lt;/property&gt;
&lt;!--分布式配置--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!--本机zookeeper地址--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
  &lt;value&gt;localhost:2181&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
</li>
</ol>
</li>
</ol>
<h3 id="23-启动hbase">2.3 启动HBase</h3>
<ol>
<li>先启动hadoop目录下<code>sbin/start-dfs.sh</code></li>
<li>启动zookeeper<code>zKserver start</code></li>
<li>启动HBase目录下的<code>bin/start-hbase.sh</code></li>
<li>打开本机60010端口，有界面 则成功</li>
</ol>
<h3 id="24-hbase-shell使用">2.4 HBase shell使用</h3>
<ol>
<li>在<code>bin</code>目录下输入<code>hbase shell</code>即可进入shell脚本
<ol>
<li>查看版本<code>version</code></li>
<li>查看服务器的状态<code>status</code></li>
</ol>
</li>
</ol>
<h2 id="3-hbase相关操作">3. HBase相关操作</h2>
<h3 id="31-ddl操作">3.1 DDL操作</h3>
<ul>
<li>创建、查询</li>
</ul>
<pre><code class="language-sql"># 创建
create 'member','member_id','address','info'
# 查询详细信息
desc 'member'
</code></pre>
<ul>
<li>返回信息如下：</li>
</ul>
<pre><code class="language-shell">{NAME =&gt; 'address', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCOD
ING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICA
TION_SCOPE =&gt; '0'}                                                                                                              
{NAME =&gt; 'info', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING
 =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATIO
N_SCOPE =&gt; '0'}                                                                                                                 
{NAME =&gt; 'member_id', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENC
ODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLI
CATION_SCOPE =&gt; '0'} 
</code></pre>
<ul>
<li>也可以在图形化界面，即60010端口上查看,点击最上面的<code>Table Details</code></li>
</ul>
<pre><code class="language-shell"># 查看有哪些表
hbase(main):007:0&gt; list
=&gt; [&quot;member&quot;]
# 删除列族中的一列
hbase(main):008:0&gt; alter 'member' ,'delete'=&gt;'member_id'
1/1 regions updated.
Done.
</code></pre>
<ul>
<li>此时在查看<code>member</code>表的结构</li>
</ul>
<pre><code class="language-shell">hbase(main):009:0&gt; desc 'member'

COLUMN FAMILIES DESCRIPTION                                                                                                     
{NAME =&gt; 'address', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCOD
ING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICA
TION_SCOPE =&gt; '0'}                                                                                                              
{NAME =&gt; 'info', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING
 =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATIO
N_SCOPE =&gt; '0'} 
</code></pre>
<ul>
<li>
<p>可以发现<code>member_id</code>已经被我们删掉了</p>
</li>
<li>
<p>删除表</p>
</li>
</ul>
<pre><code class="language-shell"># 先disable
hbase(main):011:0&gt; disable 'member'
# 再drop
hbase(main):012:0&gt; drop 'member'
# 此时再查看所有表
hbase(main):013:0&gt; list
=&gt; []
</code></pre>
<h3 id="32-dml操作">3.2 DML操作</h3>
<ol>
<li>
<p>为了方便，重新创建一张表</p>
<pre><code class="language-shell"># 表名为member，列族为address、info
create 'member','address','info'
</code></pre>
</li>
<li>
<p>创建/修改/删除表</p>
<pre><code class="language-shell"># rowkey为行名（表名） cf为列族 column为列族中的一列
插入数据： put 表名,rowkey,cf:column,key
</code></pre>
</li>
<li>
<p>实际语句如下</p>
<pre><code class="language-shell">put 'member','3z','info:age','23'
put 'member','3z','info:birthday','1996-07-31'
</code></pre>
</li>
<li>
<p>查看member中数据</p>
<pre><code class="language-shell">scan 'member'
# 返回结果
ROW					COLUMN+CELL
3z					column=info:age, timestamp=12341241212, value=23
3z					column=info:birthday, timestamp=12341241213, value=1996-07-31
</code></pre>
</li>
<li>
<p>获取某一行的数据</p>
<pre><code class="language-shell"># 获取所有3z的数据
get 'member','3z'
# 返回结果
COLUMN                            CELL                                                                                          
 info:age                         timestamp=1577955149575, value=23
 info:birthday                    timestamp=1577955128970, value=1996-07-31
</code></pre>
</li>
<li>
<p>修改某一列</p>
<pre><code class="language-shell">put 'member','3z','info:age','18'
# 获取当前年龄
get 'member','3z','info:age'
# 返回结果
COLUMN                            CELL
 info:age                         timestamp=1577955321212, value=18
# age已经被更新
</code></pre>
</li>
<li>
<p>删除某一列</p>
<pre><code class="language-shell">delete 'member','3z','info:birthday'
# 再查看3z的信息
get 'member','3z'
# 返回结果
COLUMN                            CELL
 info:age                         timestamp=1577955321212, value=18
# 删除成功
</code></pre>
</li>
<li>
<p>统计一下几行</p>
<pre><code class="language-shell">count 'member'
# 返回结果
==&gt; 1
</code></pre>
</li>
<li>
<p>删除某个一整行</p>
<pre><code class="language-shell">deleteall 'member','3z'
</code></pre>
</li>
<li>
<p>清空一张表</p>
<pre><code class="language-shell">truncate 'member'
# 会先自己disable 在truncate
Truncating 'member' table (it may take a while):
 - Disabling table...
 - Truncating table...
0 row(s) in 3.4130 seconds
</code></pre>
</li>
</ol>
<h2 id="4-hbase-api开发javascala">4. HBase API开发：Java/Scala</h2>
<ol>
<li>
<p><code>maven:pom.xml</code>：良好的网络支持</p>
<pre><code class="language-java">package com.zth.bigdata.hbase;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;

/**
 * Author: 3zZ.
 * Date: 2020/1/11 6:14 下午
 */
public class HbaseApp {
    Connection connection = null;
    Table table = null;
    Admin admin = null;

    String tableName = &quot;3z_hbase_java_api&quot;;

    @Before
    public void setUp() {
        Configuration configuration = new Configuration();
        configuration.set(&quot;hbase.rootdir&quot;, &quot;hdfs://localhost:8020/hbase&quot;);
        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost:2181&quot;);
        try {
            connection = ConnectionFactory.createConnection(configuration);
            admin = connection.getAdmin();

            Assert.assertNotNull(connection);
            Assert.assertNotNull(admin);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Test
    public void getConnection() {
    }

    @Test
    public void createTable() throws Exception {
        TableName table = TableName.valueOf(tableName);
        if (admin.tableExists(table)) {
            System.out.println(tableName + &quot;已经存在&quot;);
        } else {
            HTableDescriptor descriptor = new HTableDescriptor(table);
            descriptor.addFamily(new HColumnDescriptor(&quot;info&quot;));
            descriptor.addFamily(new HColumnDescriptor(&quot;address&quot;));
            admin.createTable(descriptor);
            System.out.println(tableName + &quot;创建成功&quot;);
        }
    }

    @Test
    public void queryTableInfos() throws Exception {
        HTableDescriptor[] tables = admin.listTables();
        if (tables.length &gt; 0) {
            for (HTableDescriptor table : tables) {
                System.out.println(table.getNameAsString());
                HColumnDescriptor[] columnDescriptors = table.getColumnFamilies();
                for (HColumnDescriptor hColumnDescriptor : columnDescriptors) {
                    System.out.println(&quot;\t&quot; + hColumnDescriptor.getNameAsString());
                }
            }
        }
    }

    @Test
    public void testPut() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(Bytes.toBytes(&quot;3z&quot;));
        // 通过PUT设置要添加数据的CF、qualifier、value
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;24&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;birthday&quot;), Bytes.toBytes(&quot;731&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;company&quot;), Bytes.toBytes(&quot;HIT&quot;));

        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;country&quot;), Bytes.toBytes(&quot;CN&quot;));
        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;province&quot;), Bytes.toBytes(&quot;BJ&quot;));
        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;city&quot;), Bytes.toBytes(&quot;BJ&quot;));
        //  将数据put到hbase中去
        table.put(put);
    }

    @Test
    public void testUpdate() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(Bytes.toBytes(&quot;2z&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;25&quot;));
        table.put(put);
    }

    @Test
    public void testGet01() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Get get = new Get(&quot;3z&quot;.getBytes());
        get.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;));
        Result result = table.get(get);
        printResult(result);
    }
    @Test
    public void testScan01() throws Exception{
        table = connection.getTable(TableName.valueOf(tableName));
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        for (Result result: rs){
            printResult(result);
        }
    }

    public void printResult(Result result) {
        for (Cell cell : result.rawCells()) {
            System.out.println(Bytes.toString(result.getRow()) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneValue(cell)) + &quot;\t&quot; +
                    cell.getTimestamp()
            );
        }
    }

    @After
    public void tearDown() {
        try {
            connection.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>
</li>
<li>
<p>小结</p>
<ul>
<li>
<p><code>Connection</code></p>
</li>
<li>
<p><code>Admin</code></p>
</li>
<li>
<p><code>HTableDescriptor</code></p>
</li>
<li>
<p><code>HcolumnDescriptor</code></p>
</li>
<li>
<p>创建表</p>
</li>
<li>
<p>删除表</p>
</li>
<li>
<p>添加记录：单挑、多条</p>
</li>
<li>
<p>修改记录</p>
</li>
<li>
<p>根据<code>RowKey</code>获取单挑记录</p>
</li>
<li>
<p><code>Scan</code>：空、起始、起始结尾、<code>Get</code></p>
</li>
<li>
<p><code>Filter</code> : <code>RowFilter</code>、<code>PrefixFilter</code>、<code>FilterList</code></p>
</li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scala学习笔记（7）---- Scala隐式转换]]></title>
        <id>https://Zu3zz.github.io/post/scala-7/</id>
        <link href="https://Zu3zz.github.io/post/scala-7/">
        </link>
        <updated>2020-01-10T16:34:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="scala隐式转换">Scala隐式转换</h1>
<h2 id="1-隐式转换概念">1. 隐式转换概念</h2>
<ul>
<li>隐式转换就是将一个类赋予另外一个类中的属性和能力</li>
</ul>
<h2 id="2-例子">2. 例子</h2>
<ul>
<li>将所有隐式转换的方法单独放到一个文件中</li>
</ul>
<pre><code class="language-scala">// ImplicitApp.scala
import java.io.File
/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:15 下午
 */
object ImplicitAspect {
  // 定义隐式转换函数即可
  // 案例1 将只有eat方法的普通人变成有fly方法的超人
  implicit def man2superman(man:Man): Superman = new Superman(man.name)
  // 案例2 为File对象添加直接读的方法
  implicit def file2Richfile(file: File): Richfile = new Richfile(file)
}
</code></pre>
<ul>
<li>在需要使用的文件进行引入</li>
</ul>
<pre><code class="language-scala">import java.io.File
import ImplicitAspect._
/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:00 下午
 */
object ImplicitApp extends App {
  // 定义隐式转换函数即可
  // 案例1
  val man = new Man(&quot;3z&quot;)
  man.fly() // 能够成功飞行
  // 案例2 为File对象添加直接读的方法
  val file = new File(&quot;/Users/3zz/Desktop/test.txt&quot;)
  file.read() // 能够正常读出文件
}
class Man(val name: String) {
  def eat(): Unit = {
    println(s&quot;man $name is eating&quot;)
  }
}
class Superman(val name: String) {
  def fly(): Unit = {
    println(s&quot;superman $name is flying&quot;)
  }
}
class Richfile(val file:File){
  def read() ={
    scala.io.Source.fromFile(file.getPath).mkString
  }
}
</code></pre>
<h2 id="3-隐式参数例子">3. 隐式参数例子</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:00 下午
 */
object ImplicitApp extends App {
  implicit val test = &quot;test&quot;
  def testParam(implicit name:String ): Unit ={
    println(name)
  }
//  testParam 什么都不填会报错 (如果在上面定义了test 就不会报错)
//  testParam(&quot;123&quot;) 正常输出 123
  implicit val name1: String = &quot;implicit_name&quot;
  testParam // 此时有了implicit 就不会报错 正常输出 implicit_name
  testParam(&quot;3z&quot;) // 输出 3z
  implicit val s1 = &quot;s1&quot;
  implicit val s2 = &quot;s3&quot;
  testParam // 此时会报错 因为有两个不确定
}
</code></pre>
<h2 id="4-隐式类例子">4. 隐式类例子</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:32 下午
 */
object ImplicitClassApp extends App {
  implicit class Cal(x:Int){
    def add(a:Int) = a + x
  }
  // 1本身是没有add方法的
  // 上面的隐式类为所有的Int类型添加了add方法
  println(1.add(3)) // 4
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scala学习笔记（6）---- Scala函数高级操作]]></title>
        <id>https://Zu3zz.github.io/post/scala-6/</id>
        <link href="https://Zu3zz.github.io/post/scala-6/">
        </link>
        <updated>2020-01-06T16:31:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="scala-函数高级操作">Scala 函数高级操作</h2>
<h2 id="1-字符串高级操作">1. 字符串高级操作</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 6:59 下午
 */
object StringApp extends App{
  val s = &quot;Hello:&quot;
  val name = &quot;3z&quot;
  println(s&quot;Hello:${name}&quot;) // 插值表达式
  val b =
    &quot;&quot;&quot;
      |这是一个多行字符串
      |hello
      |world
      |&quot;&quot;&quot;.stripMargin
  println(b)
}
// 输出
Hello:3z

这是一个多行字符串
hello
world
</code></pre>
<h2 id="2-匿名函数">2. 匿名函数</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/2 10:30 下午
 * 匿名函数：函数是可以命名的，也可以不命名
 * (参数名：参数类型...) =&gt; 函数体
 */
object FunctionApp extends App {
  val m1 = (x:Int) =&gt; x+1
  println(m1(10)) // 11
  def add = (x:Int, y:Int) =&gt; {x+y}
  println(add(1,2)) // 3
}
</code></pre>
<h2 id="3-curry函数">3. <code>Curry</code>函数</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/2 10:30 下午
 */
object FunctionApp extends App {
  def sum(a:Int, b:Int) = a+b
  println(sum(1,2))
  // 将原来接收两个参数的一个函数，转换成2个
  def sum2(a:Int)(b:Int) = a + b
  println(sum(1,2))
}
</code></pre>
<h2 id="4-高阶函数">4. 高阶函数</h2>
<ol>
<li><code>map</code></li>
<li><code>filter</code></li>
<li><code>flatmap</code></li>
<li><code>foreach</code></li>
<li><code>reduce</code></li>
</ol>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/2 10:30 下午
 */
object FunctionApp extends App {
  val l = List(1,2,3,4,5,6,7,8)
  // map: 逐个去操作集合中的每个元素
  l.map((x:Int) =&gt; x+1)
  l.map(x =&gt; x - 1) // 如果只有一个参数 括号可以省略
  l.map(_*2).filter(_ &gt; 8).foreach(println)// 每一个元素 * 2
  l.take(4) // 取前4个
  l.reduce(_+_) // 1 + 2 = 3 + 3 = 6 + 4 = 10...
  l.reduceLeft(_-_) // ((((1-2)-3)-4)-5)
  l.reduceRight(_-_) // (1-(2-(3-(4-5))))
  l.fold(0)(_-_)
  l.foldLeft(0)(_-_)
  l.foldRight(0)(_-_)
  val f = List(List(1,2),List(3,4),List(5,6))
  f.flatten // List(1, 2, 3, 4, 5, 6)
  f.map(_.map(_*2)) // List(List(2,4),List(6,8),List(10,12))
  f.flatMap(_.map(_*2)) // List(2, 4, 6, 8, 10, 12) == flatten + map
}

</code></pre>
<ul>
<li>
<p>一个简单wordcount的案例</p>
<pre><code class="language-scala">val txt = scala.io.Source.fromFile(&quot;wordcount.txt&quot;).mkString
val txts = List(txt)
txts.flatMap(_.split(&quot;,&quot;)).map(x =&gt; (x,1)).groupBy(_._1).mapValues(_.size)
// scala.collection.immutable.Map[String,Int] = Map(world -&gt; 1, hello -&gt; 2)
</code></pre>
</li>
</ul>
<h2 id="5-偏函数">5. 偏函数</h2>
<ul>
<li>偏函数的定义：被包在花括号类内没有match的一组case语句</li>
</ul>
<pre><code class="language-scala">package com.zth.fun

import scala.util.Random

/**
 * Author: 3zZ.
 * Date: 2020/1/10 10:49 下午
 * 偏函数：被包在花括号类内没有match的一组case语句
 */
object PartitalFunctionApp extends App {
  // 普通match函数
  val names = Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
  val name = names(Random.nextInt(names.length))
  name match {
    case &quot;a&quot; =&gt; println(&quot;1&quot;)
    case &quot;b&quot; =&gt; println(&quot;2&quot;)
    case _ =&gt; println(&quot;others&quot;)
  }
  // 偏函数
  // A: 输入参数类型 B: 输出参数类型
  def sayChinese:PartialFunction[String, String] = {
    case &quot;a&quot; =&gt; &quot;1&quot;
    case &quot;b&quot; =&gt; &quot;2&quot;
    case _ =&gt; &quot;others&quot;
  }
  println(sayChinese(&quot;a&quot;))
}

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scala学习笔记（5）---- Scala集合]]></title>
        <id>https://Zu3zz.github.io/post/scala-5/</id>
        <link href="https://Zu3zz.github.io/post/scala-5/">
        </link>
        <updated>2020-01-04T16:29:40.000Z</updated>
        <content type="html"><![CDATA[<h1 id="scala集合">Scala集合</h1>
<h2 id="1-数组">1. 数组</h2>
<h3 id="11-定长数组array">1.1 定长数组<code>Array</code></h3>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/8 7:56 下午
 */
object ArrayApp extends App {
  val a = new Array[String](5) // Array(null,...null)
  a.length // 5
  a(1) = &quot;hello&quot;
  a(1) // String = &quot;hello&quot;
  val b = Array(&quot;hadoop&quot;, &quot;spark&quot;, &quot;storm&quot;)
  b(1) = &quot;flink&quot;
  b // Array(hadoop, flink, storm)
  val c = Array(2,3,4,5,6,7,8,9)
  c.sum // Int = 44
  c.min // Int = 9
  c.max // Int = 2
  c.mkString // String = 23456789
  c.mkString(&quot;,&quot;) //String = 2,3,4,5,6,7,8,9
  c.mkString(&quot;&lt;&quot;,&quot;,&quot;,&quot;&gt;&quot;) // String = &lt;2,3,4,5,6,7,8,9&gt;
  c.toString // String = [I@44e3760b
}
</code></pre>
<h3 id="12-变长数组arraybuffer">1.2 变长数组<code>ArrayBuffer</code></h3>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/8 7:56 下午
 */
object ArrayApp extends App {
  val c = scala.collection.mutable.ArrayBuffer[Int]()
  c += 1 // ArrayBuffer(1)
  c += (2,3,4,5) // ArrayBuffer(1, 2, 3, 4, 5)
  c ++= Array(6,7,8) // ArrayBuffer(1, 2, 3, 4, 5, 6, 7, 8)
  c.insert(0,0) // ArrayBuffer(0, 1, 2, 3, 4, 5, 6, 7, 8)
  c.remove(1) // ArrayBuffer(0, 2, 3, 4, 5, 6, 7, 8)
  c.remove(0,3) // ArrayBuffer(4, 5, 6, 7, 8)
  c.trimEnd(2) // ArrayBuffer(4, 5, 6)
  c.toArray.mkString // 456(Array类型)
  for(ele &lt;- c){
    println(ele) // 遍历 4 5 6
  }
  for(i &lt;- (0 until c.length).reverse) {
    println(c(i)) // 逆序 6 5 4
  }
}
</code></pre>
<h2 id="2-list">2. List</h2>
<ul>
<li>有序的</li>
<li>可以重复的</li>
</ul>
<h3 id="21-nil是什么">2.1 Nil是什么</h3>
<pre><code class="language-scala">scala&gt; Nil
scala.collection.immutable.Nil.type = List()
</code></pre>
<ul>
<li><code>Nil</code>就是一个空的<code>List</code></li>
</ul>
<h3 id="22-list使用">2.2 List使用</h3>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/9 4:57 下午
 */
object ListApp extends App {
  val l = List(1,2,3,4,5) // l: List[Int] = List(1, 2, 3, 4, 5)
  l.head // Int = 1
  l.tail // List[Int] = List(2, 3, 4, 5)
  val l2 = 1 :: Nil // l2: List[Int] = List(1)
  val l3 = 2 :: l2 //  l3: List[Int] = List(2, 1)
  val l4 = 1 :: 2 :: 3 :: Nil // l4: List[Int] = List(1, 2, 3)
  val l5 = scala.collection.mutable.ListBuffer[Int]()
  l5 +=  2 // ListBuffer(2)
  l5 += (3,4,5) // ListBuffer(2, 3, 4, 5)
  l5 ++= List(6,7,8) // ListBuffer(2, 3, 4, 5, 6, 7, 8)
  l5 -= 2 // ListBuffer(3, 4, 5, 6, 7, 8)
  l5 -= (1,4) // ListBuffer(2, 3, 5, 6, 7, 8)
  l5 --= List(2,3,5,6) // ListBuffer(7, 8)
  l5.toList // List[Int] = List(7, 8)
  l5.toArray // Array[Int] = Array(1, 3, 4, 5)
  l5.head // Int = 1
  l5.isEmpty // Boolean = false
  l5.tail // scala.collection.mutable.ListBuffer[Int] = ListBuffer(3, 4, 5)
  l5.tail.head // Int = 3
  
  def sum(nums:Int*):Int = {
    if(nums.length == 0){
      0
    } else{
      nums.head + sum(nums.tail:_*) // 自动把Seq转换为Int*
    }
  }
  sum(1,2,3,4) // Int = 10
}
</code></pre>
<h2 id="3-set">3. Set</h2>
<ul>
<li>无序的</li>
<li>不可重复的</li>
</ul>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/9 6:24 下午
 */
object SetApp extends App{
  val set = scala.collection.mutable.Set[Int]()
  set += 1 // set.type = Set(1)
  set += (2,1) // set.type = Set(1,2)
}

</code></pre>
<ul>
<li>其他用法基本与<code>List</code>相同</li>
</ul>
<h2 id="4-map">4. Map</h2>
<h2 id="5-option-some-none">5. Option &amp; Some &amp; None</h2>
<h2 id="6-tuple">6. Tuple</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scala学习笔记（4）---- Scala模式匹配]]></title>
        <id>https://Zu3zz.github.io/post/scala-4/</id>
        <link href="https://Zu3zz.github.io/post/scala-4/">
        </link>
        <updated>2020-01-03T16:28:06.000Z</updated>
        <content type="html"><![CDATA[<h1 id="scala-模式匹配">Scala 模式匹配</h1>
<h2 id="1-基本模式匹配">1. 基本模式匹配</h2>
<ol>
<li>
<p><code>Java</code>中： 对一个值进行条件判断，返回针对不同的条件进行不同的处理：<code>switch case</code></p>
</li>
<li>
<p><code>scala</code>中：</p>
<pre><code class="language-scala">变量 match {
  case value1 ==&gt; 代码1
  case value2 ==&gt; 代码2
  ....
  case _ =&gt; 代码N
}
</code></pre>
</li>
<li>
<p>一个栗子</p>
<pre><code class="language-scala">import scala.util.Random
/**
 * Author: 3zZ.
 * Date: 2020/1/10 12:10 上午
 */
object MatchApp extends App {
  val names = Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
  val name = names(Random.nextInt(names.length))
  name match{
    case &quot;a&quot; =&gt; println(1)
    case &quot;b&quot; =&gt; println(2)
    case _ =&gt; println(&quot;不知道&quot;)
  }
  def judgeGrade(grade: String): Unit ={
    grade match{
      case &quot;A&quot; =&gt; println(&quot;Excellent..&quot;)
      case &quot;B&quot; =&gt; println(&quot;Good..&quot;)
      case &quot;C&quot; =&gt; println(&quot;Just so so..&quot;)
      case _ =&gt; println(&quot;u need worker harder&quot;)
    }
  }
  judgeGrade(&quot;A&quot;)
  judgeGrade(&quot;C&quot;)
  judgeGrade(&quot;G&quot;)
}
// 输出结果
1
Excellent..
Just so so..
u need worker harder
</code></pre>
</li>
</ol>
<h2 id="2-加条件">2. 加条件</h2>
<ul>
<li>
<p>双重过滤</p>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 12:10 上午
 */
object MatchApp extends App {
  def judgeGrade(name:String, grade: String): Unit ={
    grade match{
      case &quot;A&quot; =&gt; println(&quot;Excellent..&quot;)
      case &quot;B&quot; =&gt; println(&quot;Good..&quot;)
      case &quot;C&quot; =&gt; println(&quot;Just so so..&quot;)
      case _ if(name == &quot;lisi&quot;) =&gt; println(name + &quot; u need worker harder&quot;)
      case _  =&gt; println(&quot;u need worker harder&quot;)
    }
  }
  judgeGrade(&quot;zhangsan&quot;,&quot;D&quot;)
  judgeGrade(&quot;lisi&quot;,&quot;D&quot;)// 双重过滤
}
// 输出
u need worker harder
lisi u need worker harder
</code></pre>
</li>
</ul>
<h2 id="3-数组array中的过滤">3. 数组(Array)中的过滤</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 12:10 上午
 */
object MatchApp extends App {
  def greeting(array: Array[String]): Unit = {
    array match {
      case Array(&quot;zhangsan&quot;) =&gt; println(&quot;hello zhangsan&quot;)
      case Array(x, y) =&gt; println(&quot;hi: &quot; + x +&quot; and &quot; + y)
      case Array(&quot;zhangsan&quot;, _*) =&gt; println(&quot;hi: zhangsan and other&quot;)
      case _ =&gt; println(&quot;hi everybody&quot;)
    }
  }
  greeting(Array(&quot;zhangsan&quot;))
  greeting(Array(&quot;zhangsan&quot;, &quot;lisi&quot;))
  greeting(Array(&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;))
}
// 返回结果
hello zhangsan
hi: zhangsan and lisi
hi: zhangsan and other
</code></pre>
<h2 id="4-list中的过滤">4. List中的过滤</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 12:10 上午
 */
object MatchApp extends App {
  def greeting(list: List[String]): Unit = {
    list match {
      case &quot;zhangsan&quot; :: Nil =&gt; println(&quot;Hi: zhangsan&quot;)
      case x :: y :: Nil =&gt; println(&quot;Hi:&quot; + x + &quot; , &quot; + y)
      case &quot;zhangsan&quot;::tail =&gt; println(&quot;Hi: zhangsan and others&quot;)
      case _ =&gt; println(&quot;hi: everyone&quot;)
    }
  }
  greeting(List(&quot;zhangsan&quot;))
  greeting(List(&quot;zhangsan&quot;,&quot;lisi&quot;))
  greeting(List(&quot;zhangsan&quot;,&quot;lisi&quot;,&quot;wangwu&quot;))
  greeting(List(&quot;wangwu&quot;,&quot;zhangsan&quot;,&quot;lisi&quot;))
}
// 返回
Hi: zhangsan
Hi: zhangsan , lisi
Hi: zhangsan and others
hi: everyone
</code></pre>
<h2 id="5-类型匹配">5. 类型匹配</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 12:10 上午
 */
object MatchApp extends App {
  def matchType(obj: Any): Unit = {
    obj match {
      case x: Int =&gt; println(&quot;Int&quot;)
      case s:String =&gt; println(&quot;String&quot;)
      case m:Map[_,_] =&gt; m.foreach(println)
      case _ =&gt; println(&quot;other type&quot;)
    }
  }
  matchType(1)
  matchType(&quot;1&quot;)
  matchType(Map(&quot;name&quot; -&gt; &quot;a&quot;))
  matchType(1f)
}
// 返回
Int
String
(name,a)
other type
</code></pre>
<h2 id="6-异常处理">6. 异常处理</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 1:26 上午
 */
object ExceptionApp extends App {
  try {
    val i = 10 / 0
    println(i)
  } catch {
    case e: ArithmeticException =&gt; println(&quot;除数不能为0&quot;)
    case e: Exception =&gt; println(e.getMessage)
  } finally {
    // 释放资源
  }
}
</code></pre>
<ul>
<li>在catch中判断异常</li>
<li>在finally中释放资源</li>
</ul>
<h2 id="7-case-class中的匹配">7. Case Class中的匹配</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 1:26 上午
 */
object ExceptionApp extends App {
  def caseclassMatch(person: Person): Unit ={
    person match{
      case CTO(name, floor) =&gt; println(&quot;CTO name is &quot;+name+&quot; ,floor: &quot;+ floor)
      case Employee(name, floor) =&gt; println(&quot;Employee name is &quot;+name+&quot; ,floor: &quot;+ floor)
      case _ =&gt; println(&quot;other&quot;)
    }
  }
  class Person
  case class CTO(name:String, floor:String) extends Person
  case class Employee(name:String, floor:String) extends Person
  case class Other(name:String) extends Person
  caseclassMatch(CTO(&quot;3zz&quot;,&quot;22&quot;))
  caseclassMatch(Employee(&quot;zhangsan&quot;, &quot;2&quot;))
  caseclassMatch(Other(&quot;lisi&quot;))
}
// 返回
CTO name is 3zz ,floor: 22
Employee name is zhangsan ,floor: 2
other
</code></pre>
]]></content>
    </entry>
</feed>