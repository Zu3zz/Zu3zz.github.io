<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://zu3zz.coding.me</id>
    <title>风袖</title>
    <updated>2020-05-07T02:44:30.403Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://zu3zz.coding.me"/>
    <link rel="self" href="https://zu3zz.coding.me/atom.xml"/>
    <subtitle>烟蛾敛略不胜态，风袖低昂如有情</subtitle>
    <logo>https://zu3zz.coding.me/images/avatar.png</logo>
    <icon>https://zu3zz.coding.me/favicon.ico</icon>
    <rights>All rights reserved 2020, 风袖</rights>
    <entry>
        <title type="html"><![CDATA[Java面试总结(2) -- 线程篇]]></title>
        <id>https://zu3zz.coding.me/post/java-2/</id>
        <link href="https://zu3zz.coding.me/post/java-2/">
        </link>
        <updated>2020-03-09T02:23:49.000Z</updated>
        <summary type="html"><![CDATA[<p>Java面试总结第二篇~ 主要是线程与线程池的相关知识点</p>
]]></summary>
        <content type="html"><![CDATA[<p>Java面试总结第二篇~ 主要是线程与线程池的相关知识点</p>
<!-- more -->
<h2 id="java多线程和并发">Java多线程和并发</h2>
<h2 id="1-进程与线程">1. 进程与线程</h2>
<h4 id="1-进程和线程的区别">1. 进程和线程的区别</h4>
<ol>
<li>
<p>进程和线程的由来</p>
<ul>
<li>串行</li>
<li>批处理</li>
<li>进程：独占内存空间，保存各自运行状态，相互间不干扰且可以互相切换，为并发处理任务提供了可能</li>
<li>线程：共享进程的内存资源，相互间切换更快速，支持更细粒度的任务控制，使进程内的子任务得以并发执行</li>
</ul>
</li>
<li>
<p>进程是资源分配的最小单位，线程是CPU调度的最小单位</p>
<ul>
<li>
<p>所有与进程相关的资源，都被记录在PCB中</p>
<figure data-type="image" tabindex="1"><img src="https://zu3zz.coding.me/post-images/1588818996521.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>进程是抢占处理机的调度单位；线程属于某个进程，共享其资源</p>
</li>
<li>
<p>线程只由堆栈寄存器、程序计数器和TCB组成</p>
<figure data-type="image" tabindex="2"><img src="https://zu3zz.coding.me/post-images/1588819009068.png" alt="" loading="lazy"></figure>
</li>
</ul>
</li>
<li>
<p>总结</p>
<ul>
<li>线程不能看做独立应用，而进程可看做独立应用</li>
<li>进程有独立的地址空间，相互不影响，线程只是进程的不同执行路径</li>
<li>线程没有独立的地址空间，多进程的程序比多线程程序要更健壮</li>
<li>进程的切换比线程的切换开销大</li>
</ul>
</li>
</ol>
<h4 id="2-java进程和线程的关系">2. Java进程和线程的关系</h4>
<ol>
<li>Java对操作系统提供的功能进行封装，包括进程和线程</li>
<li>运行一个程序会产生一个进程，进程包含至少一个线程</li>
<li>每个进程对应一个JVM实例，多个线程共享JVM里的堆</li>
<li>Java采用单线程编程模型，程序会主动创建主线程</li>
<li>主线程可以创建子线程，原则上要后于子线程完成执行</li>
</ol>
<h4 id="3-thread中的start和run方法的区别">3. Thread中的start()和run()方法的区别</h4>
<ol>
<li>
<p>start()方法底层是c++代码：调用了jvm中的JVM_StartThread方法，创建了一个新线程</p>
<figure data-type="image" tabindex="3"><img src="https://zu3zz.coding.me/post-images/1588819024584.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>调用start()方法会创建一个新的子线程并启动</p>
</li>
<li>
<p>run()方法只是Thread的一个普通方法的调用</p>
</li>
</ol>
<h4 id="4-thread和runnable有什么关系">4. Thread和Runnable有什么关系</h4>
<ol>
<li>Thread是实现了Runnable接口的类，使得run支持多线程</li>
<li>因类的单一继承原则，推荐多使用Runnable接口</li>
</ol>
<h4 id="5-如何给run方法传参">5. 如何给run()方法传参</h4>
<ul>
<li>
<p>实现的方式主要有三种</p>
<ul>
<li>构造函数传参</li>
<li>成员变量传参</li>
<li>回调函数传参</li>
</ul>
</li>
<li>
<p>如何处理线程的返回值：三种</p>
<ul>
<li>
<p>主线程等待法（Thread.sleep()）外面循环等待</p>
</li>
<li>
<p>使用Thread类的join()阻塞当前线程以等待子线程处理完毕：用join()代替循环等待</p>
</li>
<li>
<p>通过Callable接口实现：通过FutureTask Or 线程池获取</p>
<pre><code class="language-java">// 首先实现Callable接口 重写call方法
public class MyCallable implements Callable&lt;String&gt; {
    @Override
    public String call() throws Exception{
        String value=&quot;test&quot;;
        System.out.println(&quot;Ready to work&quot;);
        Thread.currentThread().sleep(5000);
        System.out.println(&quot;task done&quot;);
        return value;
    }
}
</code></pre>
<p>在FutureTask中实现该方法</p>
<pre><code class="language-java">// 新建一个FutureTask 用isDone方法获取是否已经执行完毕的状态
public class FutureTaskDemo {
    public static void main(String[] args) throws ExecutionException, InterruptedException {
        FutureTask&lt;String&gt; task = new FutureTask&lt;String&gt;(new MyCallable());
        new Thread(task).start();
        if(!task.isDone()){
            System.out.println(&quot;task has not finished, please wait!&quot;);
        }
        System.out.println(&quot;task return: &quot; + task.get());
    }
}
</code></pre>
<p>通过线程池</p>
<pre><code class="language-java">public class ThreadPoolDemo {
    public static void main(String[] args) {
        ExecutorService newCachedThreadPool = Executors.newCachedThreadPool();
        Future&lt;String&gt; future = newCachedThreadPool.submit(new MyCallable());
        if(!future.isDone()){
            System.out.println(&quot;task has not finished, please wait!&quot;);
        }
        try {
            System.out.println(future.get());
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (ExecutionException e) {
            e.printStackTrace();
        } finally {
            newCachedThreadPool.shutdown();
        }
    }
}
</code></pre>
</li>
</ul>
</li>
</ul>
<h4 id="6-线程的状态">6. 线程的状态</h4>
<ul>
<li>
<p>枚举类型中有六个状态</p>
<ol>
<li>
<p>新建New：创建后尚未启动的线程的状态</p>
</li>
<li>
<p>运行Runnable：包含Running和Ready</p>
</li>
<li>
<p>无限期等待 Waiting：不会被分配CPU执行时间，需要显式被唤醒</p>
<pre><code class="language-shell">没有设置Timeout参数的Object.wait()方法
没有设置TImeout参数的Thread.join()方法
LockSupport.park()方法
</code></pre>
</li>
<li>
<p>限期等待Timed Waiting：在一定时间后会由系统自动唤醒</p>
<pre><code class="language-shell">Thread.sleep()方法
设置了Timeout参数的Object.wait()方法
设置了Tiimeout参数的Thread.join()方法
LockSupport.parkNanos()方法
LockSupport.parkUntil()方法
</code></pre>
</li>
<li>
<p>阻塞 Blocked：等待获取排它锁</p>
</li>
<li>
<p>结束Terminated：已终止线程的状态，线程已经结束执行（结束了在调用会报错）</p>
</li>
</ol>
</li>
</ul>
<h4 id="7-sleep和wait的区别">7. sleep和wait的区别</h4>
<ul>
<li>sleep（）是Thread类中的方法 wait是Object类中定义的方法</li>
<li>sleep（）可以在任何地方使用</li>
<li>wait（）方法只能在synchronized方法或synchronized块中使用</li>
<li>最主要的区别：
<ul>
<li>Thread.sleep只会让出cpu，不会导致锁行为的改变</li>
<li>Object.wait不仅会让出cpu，还会释放已经占有的同步资源锁</li>
</ul>
</li>
</ul>
<h4 id="8-notify与notifyall的区别">8. notify与notifyAll的区别</h4>
<ul>
<li>两个概念
<ul>
<li>锁池 EntryList：其他线程等待锁的释放的地方</li>
<li>等待池 WaitSet：调用了wait（）中，就进入了等待池</li>
</ul>
</li>
<li>区别：
<ul>
<li>notifyAll会让所有处于等待池的线程全部进入锁池去竞争获取锁的机会</li>
<li>notify只会随机选取一个处于等到吃的线程进入锁池去竞争锁的机会</li>
</ul>
</li>
</ul>
<h4 id="9-yield">9. yield</h4>
<ul>
<li>表示当前线程愿意让出CPU的使用权，但是线程调度器可能会忽略这个暗示</li>
<li>取决于调度器</li>
</ul>
<h4 id="10-中断线程">10. 中断线程</h4>
<ul>
<li>通过调用stop（）方法停止线程：已抛弃</li>
<li>suspend（）、 resume（）方法也都被废弃</li>
<li>目前：调用interrupt（），通知线程应该中断了
<ul>
<li>如果处于被阻塞状态，那么退出阻塞状态，抛出<code>InterruptedException</code>异常</li>
<li>如果正常活动状态，那么会将该线程的中断标志设置为true，该线程将会继续执行</li>
<li>检查中断标记</li>
</ul>
</li>
</ul>
<h4 id="11-线程状态">11. 线程状态</h4>
<figure data-type="image" tabindex="4"><img src="https://zu3zz.coding.me/post-images/1588819136266.png" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="5"><img src="https://zu3zz.coding.me/post-images/1588819144367.jpg" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="https://zu3zz.coding.me/post-images/1588819150432.jpg" alt="" loading="lazy"></figure>
<h2 id="2-锁相关">2. 锁相关</h2>
<h3 id="1-synchronized">1. Synchronized</h3>
<ul>
<li>互斥锁：同一时间只有一个线程持有</li>
<li>可见性：修改操作对其他线程是可见的</li>
<li>锁的不是代码，是对象</li>
<li>根据锁的分类：获取对象锁和获取类锁
<ul>
<li>同步代码块（synchronized（this） synchronized（类实例对象））锁的是括号中的实例对象</li>
<li>同步非静态方法（synchronized method）</li>
</ul>
</li>
<li>获取类锁的两种用法
<ul>
<li>同步代码块（synchronized（类.class）），锁是小括号（）中的类对象（Class对象）</li>
<li>同步静态方法（synchronized static method），锁是当前对象的类对象（Class对象）</li>
</ul>
</li>
<li>对象锁和类锁的总结
<ol>
<li>有线程访问对象的同步代码块时，另外的线程可以访问该对象的非同步代码块</li>
<li>若锁住的是同一个对象，一个线程在访问对象的同步代码块时，另一个访问对象的同步代码块的线程会被阻塞</li>
<li>若锁住的是同一个对象，一个线程在访问对象的同步方法时，另一个访问对象同步方法的线程会被阻塞</li>
<li>若锁住的是同一个对象，一个线程在访问对象的同步代码块时，另一个访问对象同步方法的线程会被阻塞，反之亦然</li>
<li>同一个类的不同对象的对象锁互不干扰</li>
<li>类所由于也是一种特殊的对象锁，因此和上述1，2，3，4一直，而由于一个类只有一把对象锁，所以同一个类的不同对象使用类锁将会是同步的</li>
<li>类所和对象锁互不干扰</li>
</ol>
</li>
</ul>
<h3 id="2-synchronized底层原理">2. Synchronized底层原理</h3>
<h4 id="1-对象在内存中的布局">1. 对象在内存中的布局</h4>
<ul>
<li>对象头</li>
<li>实例数据</li>
<li>对齐填充</li>
</ul>
<h4 id="2-对象头的结构">2. 对象头的结构</h4>
<figure data-type="image" tabindex="7"><img src="https://zu3zz.coding.me/post-images/1588819178301.png" alt="" loading="lazy"></figure>
<h4 id="3-mark-word的结构">3. Mark Word的结构</h4>
<figure data-type="image" tabindex="8"><img src="https://zu3zz.coding.me/post-images/1588819188233.png" alt="" loading="lazy"></figure>
<h4 id="4-monitor">4. Monitor</h4>
<ol>
<li>
<p>每个Java对象天生自带了一把看不见的锁</p>
</li>
<li>
<p>Monitor锁的竞争、释放与获取</p>
</li>
</ol>
<figure data-type="image" tabindex="9"><img src="https://zu3zz.coding.me/post-images/1588819197733.png" alt="" loading="lazy"></figure>
<h4 id="5-synchronized发展">5. Synchronized发展</h4>
<ul>
<li>
<p>什么是重入：一个线程再次请求自己持有对象锁的临界资源，这种情况属于重入</p>
<ul>
<li>Synchronized是可重入的</li>
</ul>
</li>
<li>
<p>早期版本中，Synchronized属于重量级锁，依赖于Mutex Lock实现</p>
</li>
<li>
<p>线程之间的切换要从用户态转到核心态，开销较大</p>
</li>
<li>
<p>Java6之后，Synchronized性能得到了很大的提升</p>
<ul>
<li>Adaptive Spinning</li>
<li>Lightweight Locking</li>
<li>Lock Eliminate</li>
<li>Biased Locking</li>
<li>Lock Coarsening</li>
</ul>
</li>
</ul>
<h4 id="6-自旋锁与自适应自旋锁锁优化">6. 自旋锁与自适应自旋锁（锁优化）</h4>
<ol>
<li>自旋锁
<ul>
<li>一些共享数据的锁定状态持续时间较短，切换线程不值得</li>
<li>通过让线程执行忙循环等待锁的释放，不让出CPU</li>
<li>缺点：若锁被其他线程长时间占用，会带来许多性能上的开销（PreBlockSpin）</li>
</ul>
</li>
<li>自适应自旋锁
<ul>
<li>自选的次数不再固定</li>
<li>由前一次在同一个锁上的自选时间及锁的拥有者的状态来决定</li>
</ul>
</li>
<li>锁消除
<ul>
<li>JIT编译时，对运行上下文进行扫描，去除不可能存在竞争的锁</li>
</ul>
</li>
<li>锁粗化
<ul>
<li>将锁的范围加大，避免反复加锁和解锁</li>
</ul>
</li>
</ol>
<h4 id="7-synchronized的四种状态">7. Synchronized的四种状态</h4>
<blockquote>
<p>无锁、偏向锁、轻量级锁、重量级锁</p>
</blockquote>
<ul>
<li>膨胀方向：无锁-&gt;偏向锁-&gt;轻量级锁-&gt;重量级锁</li>
</ul>
<ol>
<li>
<p>偏向锁</p>
<ul>
<li>锁不存在多线程竞争，总是由容易线程多次获得
<ul>
<li>一个线程获得一个锁之后，进入偏向模式，此时Mark Word的结构变成了变相所结构</li>
<li>再次请求锁时，无需再做任何同步操作，获取过程只要检查Mark Word的锁标记位为偏向锁以及当前线程Id等于MarkWord中的ThreadID即可 ，省去了大量有关锁申请的操作</li>
<li>不适用于锁竞争比较激烈的场景</li>
</ul>
</li>
</ul>
</li>
<li>
<p>轻量级锁</p>
<ul>
<li>由偏向锁升级而来，偏向锁运行在一个线程进入同步快的情况下，当第二个线程加入锁征用的时候，偏向锁就会升级成为轻量级锁</li>
<li>适用的场景：线程交替执行同步块</li>
<li>同一时间访问同一锁，就会导致轻量级锁膨胀为重量级锁</li>
</ul>
</li>
<li>
<p>锁的内存语义</p>
<ul>
<li>当线程刷新时，Java内存模型会把该线程对应的本地内存中的共享变量刷新到主内存中</li>
<li>而当线程获取锁时，Java内存模型会吧该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从主内存中读取共享变量</li>
</ul>
</li>
</ol>
<h4 id="8-偏向锁-轻量级锁-重量级锁的汇总">8. 偏向锁、轻量级锁、重量级锁的汇总</h4>
<figure data-type="image" tabindex="10"><img src="https://zu3zz.coding.me/post-images/1588819222315.png" alt="" loading="lazy"></figure>
<h3 id="3-synchronized和reentrantlock的区别">3. Synchronized和ReentrantLock的区别</h3>
<h4 id="1-reentrantlock再入锁">1. ReentrantLock（再入锁）</h4>
<ul>
<li>位于Java.util.concurrent.locks包</li>
<li>和CountDownLatch、FutureTask、Semaphore一样基于AQS AbstractQueuedSynchronizer实现</li>
<li>能够实现比Synchronized更细粒度的控制，比如fairness</li>
<li>调用lock（）之后，必须调用unlock（）释放锁</li>
<li>性能未必比Synchronized高，并且也是可重入的</li>
</ul>
<h4 id="2-公平性的设置">2. 公平性的设置</h4>
<ul>
<li>
<p>公平性</p>
<pre><code class="language-java">ReentrantLock fairLock = new ReentrantLock(true);
</code></pre>
</li>
<li>
<p>参数为true时，倾向于将锁赋予等待时间最久的线程</p>
</li>
<li>
<p>公平锁：获取锁的顺序按照先后调用lock方法的顺序（慎用）</p>
</li>
<li>
<p>非公平锁：抢占的顺序不一定，看运气</p>
</li>
<li>
<p>Synchronized是非公平锁</p>
</li>
<li>
<p>ReentrantLock将锁对象化</p>
<ul>
<li>判断是否有线程，或者某个特定线程，在排队等待获取锁</li>
<li>带超时的获取锁的尝试</li>
<li>感知有没有成功的获取锁</li>
</ul>
</li>
<li>
<p>是否能将wait/notify/notifyAll对象化？</p>
<ul>
<li>JUC包下的locks.Condition</li>
</ul>
</li>
</ul>
<h4 id="3-区别">3. 区别</h4>
<ol>
<li>Synchronized是关键字</li>
<li>ReentrantLock是类，比Synchronized更灵活</li>
<li>ReentrantLock可以对获取锁的等待时间进行设置，避免死锁</li>
<li>ReentrantLock可以获取各种锁的信息</li>
<li>可以灵活的实现多路通知</li>
<li>机制不同：sync操作的是MarkWord，lock调用Unsafe类的park（）方法</li>
</ol>
<h3 id="4-jmm模型">4. JMM模型</h3>
<ol>
<li>
<p>Java内存模型：一组规范</p>
<figure data-type="image" tabindex="11"><img src="https://zu3zz.coding.me/post-images/1588819256199.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>JMM中的主内存</p>
<ul>
<li>存储Java实例对象</li>
<li>包括成员变量、类信息、常量、静态变量等</li>
<li>属于数据共享的区域，多线程并发操作时会引发线程安全问题</li>
</ul>
</li>
<li>
<p>JMM中的工作内存</p>
<ul>
<li>存储当前方法的所有本地变量信息，本地变量对其他线程不可见</li>
<li>字节码行号指示器、Native方法信息</li>
<li>属于线程私有数据区域，不存在线程安全问题</li>
</ul>
</li>
<li>
<p>JMM与Java内存区域</p>
<ol>
<li>概念不同</li>
<li>JMM描述的是一组规则，围绕原子性、有序性、可见性展开</li>
<li>相似点：存在共享区域和私有区域</li>
</ol>
</li>
<li>
<p>主内存和工作内存</p>
<ol>
<li>方法里的基本数据类型，本地变量将直接存储在工作内存的栈帧结构中</li>
<li>引用类型的本地变量：引用存储在工作内存中，实例存储在主内存中</li>
<li>成员变量、static变量、类信息均会被存储在主内存中</li>
<li>主内存共享的方式是线程各拷贝一份数据到工作内存，操作完成之后刷新回主内存</li>
</ol>
<figure data-type="image" tabindex="12"><img src="https://zu3zz.coding.me/post-images/1588819271332.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>指令重排序需要满足的条件</p>
<ul>
<li>在单线程环境下不能改变程序运行的结果</li>
<li>存在数据以来关系的不允许重排序</li>
</ul>
<blockquote>
<p>无法通过happens-before原则推导出来的，才能进行指令的重排序</p>
</blockquote>
<ul>
<li>
<p>A操作的结果需要对B操作课件，则A与B存在happens-before关系</p>
</li>
<li>
<p>happens-before八大原则</p>
<figure data-type="image" tabindex="13"><img src="https://zu3zz.coding.me/post-images/1588819282966.png" alt="" loading="lazy"></figure>
</li>
</ul>
</li>
</ol>
<h3 id="5-volatile">5. Volatile</h3>
<ul>
<li>JVM提供的轻量级同步机制</li>
<li>保证被volatile修饰的共享变量对所有线程总是可见的</li>
<li>禁止指令的重排序优化</li>
</ul>
<ol>
<li>volatile的可见性：原子操作</li>
<li>volatile变量为何立即可见
<ul>
<li>当写一个volatile变量时，JMM会吧该线程对应的工作内存中的共享变量值刷新到主内存中</li>
<li>当读取一个volatile变量时，JMM会吧该线程对应的工作内存置为无效</li>
</ul>
</li>
<li>volatile如何禁止重排优化
<ul>
<li>内存屏障（Memory Barrier）
<ol>
<li>保证特定操作的执行顺序</li>
<li>保证某些变量的内存可见性</li>
</ol>
</li>
<li>通过从插入内存屏障指令禁止在内存屏障前后的指令执行重排序优化</li>
<li>强制刷出CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本</li>
</ul>
</li>
</ol>
<figure data-type="image" tabindex="14"><img src="https://zu3zz.coding.me/post-images/1588819294199.png" alt="" loading="lazy"></figure>
<h3 id="6-volatile和synchronized的区别">6. Volatile和synchronized的区别</h3>
<figure data-type="image" tabindex="15"><img src="https://zu3zz.coding.me/post-images/1588819308732.png" alt="" loading="lazy"></figure>
<h3 id="7-cascompare-and-swap">7. CAS(Compare and Swap)</h3>
<ol>
<li>支持原子更新操作，适用于计数器，序列发生器等场景</li>
<li>属于乐观锁机制</li>
<li>CAS操作失败时由开发者决定是继续尝试，还是执行别的操作</li>
<li>CAS思想
<ol>
<li>三个操作数 内存位置 V 预期原值 A和新值 B</li>
<li>缺点
<ul>
<li>若循环时间长，则开销很大</li>
<li>只能保证一个共享变量的原子操作</li>
<li>ABA问题：AtomicStampedReference：控制版本</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="8-java线程池">8. Java线程池</h3>
<ol>
<li>
<p>利用Executors创建不同的线程池满足不同的场景</p>
<figure data-type="image" tabindex="16"><img src="https://zu3zz.coding.me/post-images/1588819326547.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>Fork/Join框架</p>
<ul>
<li>大任务分割成若干个小任务并执行，汇总小任务结果</li>
<li>Work-Stealing算法：从其他队列里窃取任务来执行：双端队列</li>
</ul>
</li>
<li>
<p>为什么要用</p>
<ul>
<li>降低资源消耗</li>
<li>提高线程的可管理性</li>
</ul>
</li>
<li>
<p>ThreadPoolExecutor</p>
<figure data-type="image" tabindex="17"><img src="https://zu3zz.coding.me/post-images/1588819339498.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>构造函数</p>
<pre><code class="language-shell">coolPoolSize: 核心线程的最大个数
maximumPoolSize：线程不够用时能够创建的最大线程数
workQUeue：任务等待队列（排队机制不同）
keepAliceTime：抢占的顺序不一定，看运气
threadFactory：创建新线程，Executors.defaultThreadFactory()
handler：线程池的饱和策略
	1. AbortPolicy：直接抛出异常，默认
	2. discardPolicy：直接丢弃任务
	3. CallerRunnsPolicy：用调用者所在的线程来执行任务
	4. DiscardOldestPolicy：丢弃最靠前的任务，并执行当前任务
</code></pre>
</li>
</ul>
</li>
<li>
<p>新任务提交execute执行后的判断</p>
<figure data-type="image" tabindex="18"><img src="https://zu3zz.coding.me/post-images/1588819361031.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>线程池的状态</p>
<ol>
<li>Running：能接受新提交的任务，并且也能处理阻塞队列中的任务</li>
<li>ShutDown：不能在接受新提交的任务，但可以处理存量任务</li>
<li>Stop（Shutdown now）：不在接受新提交的任务，也不处理存量任务</li>
<li>Tidying：所有任务都已终止</li>
<li>terminated：terminated（）方法执行完后进入该状态</li>
</ol>
</li>
<li>
<p>线程池的大小如何设定</p>
<ul>
<li>CPU密集型：线程数= CPU核数 + 1</li>
<li>I/O密集型：线程数= CPU核数 * （1 + 平均等待时间/ 平均工作时间）</li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java面试总结(1) -- JVM]]></title>
        <id>https://zu3zz.coding.me/post/java-1/</id>
        <link href="https://zu3zz.coding.me/post/java-1/">
        </link>
        <updated>2020-03-07T02:13:36.000Z</updated>
        <summary type="html"><![CDATA[<p>Java面试第一篇 关于JVM的那些事情</p>
]]></summary>
        <content type="html"><![CDATA[<p>Java面试第一篇 关于JVM的那些事情</p>
<!-- more -->
<h1 id="java面试点">Java面试点</h1>
<h2 id="1-jvm">1. JVM</h2>
<h3 id="11-谈谈java的特性">1.1 谈谈java的特性</h3>
<ol>
<li>平台无关性：使用<code>javap</code>先将<code>java</code>文件编译成<code>class</code>文件</li>
<li>面向对象</li>
<li>GC</li>
<li>语言特性</li>
<li>类库</li>
<li>异常处理</li>
</ol>
<h3 id="12-jvm如何加载class文件">1.2 JVM如何加载.class文件</h3>
<h4 id="java虚拟机">Java虚拟机</h4>
<ol>
<li>Class Loader：依据特定格式，加载class到内存</li>
<li>Execution Engine：对命令进行解析</li>
<li>Native Interface：融合不同语言的原生库为java所用</li>
<li>Runtiime Data Area：JVM内存空间结构模型（重要）</li>
</ol>
<h3 id="13-谈谈反射">1.3 谈谈反射</h3>
<pre><code class="language-java">// 有一个Robot类如下
public class Robot {
    private String name;
    public void sayHi(String helloSentence){
        System.out.println(helloSentence + &quot; &quot; + name);
    }
    private String throwHello(String tag){
        return &quot;Hello &quot; + tag;
    }
    static {
        System.out.println(&quot;Hello Robot&quot;);
    }
}
// 如何使用反射调用私有方法以及属性
public class ReflectSample {
    public static void main(String[] args) throws Exception {
      // 获得当前类对象
      Class rc = Class.forName(&quot;com.interview.javabasic.reflect.Robot&quot;);
      // 新创建一个当前类的实例
      Robot r = (Robot) rc.newInstance();
      // Class name is com.interview.javabasic.reflect.Robot
      System.out.println(&quot;Class name is &quot; + rc.getName());
      // 定义一个方法类
      Method getHello = rc.getDeclaredMethod(&quot;throwHello&quot;, String.class);
      // 设置调用私有方法为true
      getHello.setAccessible(true);
      Object str = getHello.invoke(r, &quot;Bob&quot;);
      // getHello result is Hello Bob
      System.out.println(&quot;getHello result is &quot; + str);
      // 正常调用公共方法
      Method sayHi = rc.getMethod(&quot;sayHi&quot;, String.class);
      sayHi.invoke(r, &quot;Welcome&quot;);
      Field name = rc.getDeclaredField(&quot;name&quot;);
      name.setAccessible(true);
      name.set(r, &quot;Alice&quot;);
      sayHi.invoke(r, &quot;Welcome&quot;);
      System.out.println(System.getProperty(&quot;java.ext.dirs&quot;));
      System.out.println(System.getProperty(&quot;java.class.path&quot;));
    }
}
</code></pre>
<h3 id="14-类从编译到执行的过程">1.4 类从编译到执行的过程</h3>
<ol>
<li>编译器将Robot.java源文件编译为Robot.class字节码文件</li>
<li>ClassLoader将字节码转换为JVM中的<code>Class&lt;Robot&gt;</code>对象</li>
<li>JVM利用<code>Class&lt;Robot&gt;</code>对象实例化为Robot对象</li>
</ol>
<h4 id="1-谈谈classloader">1. 谈谈ClassLoader</h4>
<ul>
<li>ClassLoader主要工作在Class装载的加载阶段</li>
<li>其主要作用是从系统外部获得Class二进制数据流。</li>
<li>他是Java的核心组件，所有的Class都是由ClassLoader进行加载的</li>
<li>ClassLoader负责通过将Class文件里的二进制数据流装载进系统</li>
<li>然后交给Java虚拟机进行连接、初始化等操作</li>
</ul>
<h4 id="2-classloader的种类">2. ClassLoader的种类</h4>
<ol>
<li>BootStrapClassLoader：C++编写，加载核心库java.*</li>
<li>ExtClassLoader：Java编写，加载扩展库 javax.*</li>
<li>AppClassLoader：Java编写，加载程序所在目录</li>
</ol>
<h4 id="3-自定义classloader的实现">3. 自定义ClassLoader的实现</h4>
<ol>
<li>关键函数
<ul>
<li>findClass</li>
<li>defineClass</li>
</ul>
</li>
</ol>
<pre><code class="language-java">public class MyClassLoader extends ClassLoader {
  	// 需要加载的类的路径
    private String path;
  	// class的名字
    private String classLoaderName;
    public MyClassLoader(String path, String classLoaderName) {
        this.path = path;
        this.classLoaderName = classLoaderName;
    }
    //用于寻找类文件
    @Override
    public Class findClass(String name) {
        byte[] b = loadClassData(name);
        return defineClass(name, b, 0, b.length);
    }
    //用于加载类文件
    private byte[] loadClassData(String name) {
        name = path + name + &quot;.class&quot;;
        InputStream in = null;
        ByteArrayOutputStream out = null;
        try {
            in = new FileInputStream(new File(name));
            out = new ByteArrayOutputStream();
            int i = 0;
            while ((i = in.read()) != -1) {
                out.write(i);
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            try {
                out.close();
                in.close();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
        return out.toByteArray();
    }
}
</code></pre>
<p>###1.5 双亲委派机制</p>
<figure data-type="image" tabindex="1"><img src="https://zu3zz.coding.me/post-images/1588817688490.png" alt="" loading="lazy"></figure>
<ul>
<li>避免多份同样字节码的加载</li>
</ul>
<h3 id="16-类的加载方式">1.6 类的加载方式</h3>
<ol>
<li>隐式加载：new</li>
<li>显示加载：loadClass，forName等</li>
</ol>
<h4 id="1-loadclass和forname的区别">1. loadClass和forName的区别</h4>
<ol>
<li>
<p>类的装载过程</p>
<ul>
<li>加载
<ol>
<li>通过ClassLoader加载class文件字节码，生成class对象</li>
</ol>
</li>
<li>链接
<ol>
<li>校验：检查加载的class的正确性和安全性</li>
<li>准备：为类变量分配存储空间并设置类变量初始化</li>
<li>解析：JVM将常量池内的符号引用转换为直接引用</li>
</ol>
</li>
<li>初始化
<ol>
<li>执行类变量赋值和静态代码块</li>
</ol>
</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://zu3zz.coding.me/post-images/1588817704849.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>两者区别</p>
<ul>
<li>Class.forName得到的class是已经初始化完成的</li>
<li>ClassLoader.loadClass得到的class是还没有完成链接的</li>
</ul>
</li>
</ol>
<h3 id="17-java的内存模型">1.7 Java的内存模型</h3>
<h4 id="1-内存简介">1. 内存简介</h4>
<figure data-type="image" tabindex="3"><img src="https://zu3zz.coding.me/post-images/1588817717649.png" alt="" loading="lazy"></figure>
<h4 id="2地址空间的划分">2.地址空间的划分</h4>
<ol>
<li>内核空间</li>
<li>用户空间</li>
</ol>
<figure data-type="image" tabindex="4"><img src="https://zu3zz.coding.me/post-images/1588817731598.png" alt="" loading="lazy"></figure>
<h4 id="3-jvm架构">3. JVM架构</h4>
<figure data-type="image" tabindex="5"><img src="https://zu3zz.coding.me/post-images/1588817740833.png" alt="" loading="lazy"></figure>
<ul>
<li>Class Loader：依据特定格式，加载class到内存</li>
<li>Execution Engine：对命令进行解析</li>
<li>Native Interface：融合不同开发语言的原生库为java所用</li>
<li>Runtime Data Area：JVM内存空间结构模型</li>
</ul>
<h4 id="4-jvm内存模型jdk8">4. JVM内存模型——JDK8</h4>
<figure data-type="image" tabindex="6"><img src="https://zu3zz.coding.me/post-images/1588817751249.png" alt="" loading="lazy"></figure>
<ul>
<li>线程私有：程序计数器、虚拟机栈、本地方法栈</li>
<li>线程共享：MetaSpace、Java堆</li>
<li>程序计数器（Program Counter Register）：
<ul>
<li>当前线程所执行的字节码行号指示器（逻辑计数器）</li>
<li>改变计数器的值来选取下一条需要执行的字节码指令</li>
<li>和线程是一对一的关系即”线程私有“</li>
<li>对Java方法计数，如果是Native方法则计数器值为Undefined</li>
<li>不会发生内存泄漏</li>
</ul>
</li>
<li>Java虚拟机栈（Stack）
<ul>
<li>Java方法执行的内存模型</li>
<li>包含多个栈帧</li>
</ul>
</li>
<li>局部变量表和操作数栈
<ul>
<li>局部变量表：包含方法执行过程中的所有变量</li>
<li>操作数栈：入栈、出栈、复制、交换、产生消费变量</li>
</ul>
</li>
</ul>
<h4 id="5-线程共享">5. 线程共享</h4>
<ol>
<li>
<p>元空间（MetaSpace）和永久代（PermGen）的区别</p>
<ul>
<li>
<p>元空间使用本地内存，而永久代使用的是jvm的内存</p>
<pre><code class="language-java">java.lang.OutOfMemoryError: PermGen Space
</code></pre>
</li>
</ul>
</li>
<li>
<p><code>MetaSpace</code>相比<code>PermGem</code>的优势</p>
<ul>
<li>字符串常量池存在永久代中，容易出现性能问题和内存溢出</li>
<li>类和方法的信息大小难以确定，给永久代的大小指定带来困难</li>
<li>永久代会为GC带来不必要的复杂性</li>
<li>方便<code>HotSpot</code>与其他<code>JVM</code>如<code>Jrockit</code>的继承</li>
</ul>
</li>
<li>
<p>Java堆（Heap）</p>
<ul>
<li>
<p>对象实例的分配区域</p>
<figure data-type="image" tabindex="7"><img src="https://zu3zz.coding.me/post-images/1588817797666.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>GC管理的主要区域</p>
<figure data-type="image" tabindex="8"><img src="https://zu3zz.coding.me/post-images/1588817804182.png" alt="" loading="lazy"></figure>
</li>
</ul>
</li>
</ol>
<h3 id="18-jvm调优参数">1.8 JVM调优参数</h3>
<ul>
<li>-Xms：堆的初始大小</li>
<li>-Xmx：堆能达到的最大值</li>
<li>-Xss：规定了每个线程虚拟机栈（堆栈）的大小</li>
</ul>
<pre><code class="language-java">// 示例代码
java -Xms128m -Xmx128m -Xss256k -jar xxxxx.jar
</code></pre>
<h3 id="19-内存分配策略">1.9 内存分配策略</h3>
<h4 id="1-内存分配策略">1. 内存分配策略</h4>
<ul>
<li>静态存储：编译时确定每个数据目标在运行时的存储空间需求</li>
<li>栈式存储（动态存储）：数据区需求在编译时未知，运行时模块入口前确定</li>
<li>堆式存储：编译时或运行时模块入口都无法确定，动态分配</li>
</ul>
<h4 id="2-java内存中堆和栈的区别">2. Java内存中堆和栈的区别</h4>
<ul>
<li>联系：引用对象、数组时，栈里定义变量保存堆中目标的首地址</li>
<li>管理方式：栈自动释放，堆需要GC</li>
<li>空间大小：栈比堆小</li>
<li>碎片相关：栈产生的碎片远小于堆</li>
<li>分配方式：栈支持静态和动态分配，而堆仅支持动态分配</li>
<li>效率：栈的效率比堆高</li>
<li>详解一个普通的类在加载的时候（元空间、堆、线程独占部分）之间的联系</li>
</ul>
<pre><code class="language-java">// 有如下一个类
public class HelloWorld {
  private String name;
  public void sayHello() {
    System.out.println(&quot;Hello&quot; + name);
  }
  public void setName(String name){
    this.name = name;
  }
  public static void main(String[] args){
    int a = 1;
    HelloWorld hw = new HelloWorld();
    hw.setName(&quot;test&quot;);
    hw.sayHello();
  }
}
</code></pre>
<figure data-type="image" tabindex="9"><img src="https://zu3zz.coding.me/post-images/1588817818449.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Java多线程与并发(1) -- 线程创建]]></title>
        <id>https://zu3zz.coding.me/post/java-thread-1/</id>
        <link href="https://zu3zz.coding.me/post/java-thread-1/">
        </link>
        <updated>2020-02-21T12:33:07.000Z</updated>
        <summary type="html"><![CDATA[<p>Java多进程的第一篇~ 关于线程创建的方法与底层原理😇</p>
]]></summary>
        <content type="html"><![CDATA[<p>Java多进程的第一篇~ 关于线程创建的方法与底层原理😇</p>
<!-- more -->
<h1 id="java多并发-线程创建">Java多并发 -- 线程创建</h1>
<h2 id="实现多线程的方法">实现多线程的方法</h2>
<ul>
<li>
<p>实现多线程的官方正确方法：2种</p>
<ul>
<li>
<p>方法一：实现Runnable接口</p>
<pre><code class="language-java">// 用Runnable方式创建线程
public class RunnableStyle implements Runnable{
  public static void main(String[] args){
    Thread thread = new Thread(new RunnableStyle());
    thread.start();
  }
  @Override
  public void run() {
    System.out.println(&quot;用Runnable方法实现线程&quot;);
  }
}
// 打印输出 用Runnable方法实现线程
</code></pre>
</li>
<li>
<p>方法二： 继承Thread类</p>
<pre><code class="language-java">// 用Thread方式创建线程
public class ThreadStyle extends Thread{
  public static void main(String[] args){
    new ThreadStyle().start();
  }
  @Override
  public void run() {
    System.out.println(&quot;用Thread类实现线程&quot;);
  }
}
// 打印输出 用Thread类实现线程
</code></pre>
</li>
</ul>
</li>
<li>
<p>两种方法的对比</p>
<ul>
<li>方法1（实现Runnable接口）更好</li>
<li>解耦</li>
<li>性能消耗更低</li>
<li>如果已经继承，限制了可扩展性</li>
</ul>
</li>
<li>
<p>两种方法的本质对比（看源码）</p>
<ul>
<li>方法一：最终调用target.run()</li>
<li>方法二：run()整个都被重写</li>
</ul>
</li>
<li>
<p>同时使用两种方法会怎么样？</p>
<pre><code class="language-java">/**
 * 描述：     同时使用Runnable和Thread两种实现线程的方式
 */
public class BothRunnableThread {
    public static void main(String[] args) {
        new Thread(new Runnable() {
            @Override
            public void run() {
                System.out.println(&quot;我来自Runnable&quot;);
            }
        }) {
            @Override
            public void run() {
                System.out.println(&quot;我来自Thread&quot;);
            }
        }.start();
    }
}
// 打印输出 我来自Thread
</code></pre>
</li>
<li>
<p>准确的来说，创建线程只有一种方式就是构造Thread类，而实现线程的单元创建有两种方法：</p>
<ul>
<li>方法一：实现Runnable接口的run方法，并把Runnable实例传给Thread类</li>
<li>方法二：重写Thread类的run方法（继承Thread类）</li>
</ul>
</li>
<li>
<p>典型错误观点</p>
<ul>
<li>线程池创建线程也算是一种新建线程的方式</li>
<li>通过 **Callable ** 和 <strong>FutureTask</strong> 来创建线程</li>
<li><strong>无返回值</strong>是实现runnable接口，**有返回值 **是实现Callable接口，所以callable是新的实现线程的方式</li>
<li>定时器（底层也是自动创建）</li>
<li>匿名内部类（语法层面）</li>
<li>Lambda表达式（语法层面）</li>
</ul>
</li>
<li>
<p>本质万变不离其宗</p>
</li>
<li>
<p>常见面试问题</p>
<ul>
<li>有多少种实现线程的方法？5点思路
<ul>
<li>从不同的角度看，会有不同的答案</li>
<li>典型答案是两种</li>
<li>我们看原理，两种本质都是一样的</li>
<li>具体展开说其他方式</li>
<li>结论</li>
</ul>
</li>
<li>实现Runnable接口和继承Thread类那种方式更好
<ul>
<li>从代码架构角度（解耦合）</li>
<li>新建线程的损耗（新建类消耗大）</li>
<li>Java不支持双继承（继承会麻烦）</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[主流缓存Redis笔记]]></title>
        <id>https://zu3zz.coding.me/post/interview-redis/</id>
        <link href="https://zu3zz.coding.me/post/interview-redis/">
        </link>
        <updated>2020-02-16T13:15:30.000Z</updated>
        <summary type="html"><![CDATA[<p>🐽详细讲解主流缓存面试会遇到的问题，包含Redis分布式锁、异步队列、持久化、集群等🐽</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐽详细讲解主流缓存面试会遇到的问题，包含Redis分布式锁、异步队列、持久化、集群等🐽</p>
<!-- more -->
<h1 id="主流缓存redis笔记">主流缓存Redis笔记</h1>
<h2 id="1-主流应用架构">1. 主流应用架构</h2>
<figure data-type="image" tabindex="1"><img src="https://zu3zz.coding.me/post-images/1581859003669.png" alt="" loading="lazy"></figure>
<h3 id="11-缓存中间件-memcache和redis的区别">1.1 缓存中间件---- Memcache和Redis的区别</h3>
<h4 id="1-memcache代码层次类似hash">1. Memcache：代码层次类似Hash</h4>
<ul>
<li>支持简单数据类型</li>
<li>不支持数据持久化存储</li>
<li>不支持主从</li>
<li>不支持分片</li>
</ul>
<h4 id="2-redis">2. Redis</h4>
<ul>
<li>数据类型丰富</li>
<li>支持数据磁盘持久化存储</li>
<li>支持主从</li>
<li>支持分片</li>
</ul>
<hr>
<h3 id="12-为什么redis能这么快">1.2 为什么Redis能这么快</h3>
<h4 id="1-100000qps每秒内查询次数">1. 100000+QPS（每秒内查询次数）</h4>
<ul>
<li>完全基于内存，绝大部分请求是纯粹的内存操作，执行效率高</li>
<li>数据结构简单，对数据操作也简单</li>
<li>采用单线程，单线程也能处理高并发请求，想多核也可以启动多实例</li>
<li>使用多路I/O复用模型，非阻塞IO</li>
</ul>
<hr>
<h3 id="13-多路io复用模型">1.3 多路I/O复用模型</h3>
<h4 id="1-fd-file-descriptor文件描述符">1. FD: File descriptor，文件描述符</h4>
<ul>
<li>一个打开的文件通过唯一的描述符进行引用，该描述符是打开文件的元数据到文件本身的映射</li>
</ul>
<h4 id="2-传统的阻塞io模型">2. 传统的阻塞I/O模型</h4>
<figure data-type="image" tabindex="2"><img src="https://zu3zz.coding.me/post-images/1581859021180.png" alt="" loading="lazy"></figure>
<h4 id="3-select-系统调用">3. Select 系统调用</h4>
<figure data-type="image" tabindex="3"><img src="https://zu3zz.coding.me/post-images/1581859027686.png" alt="" loading="lazy"></figure>
<h4 id="4-redis采用的io多路复用函数epollkqueueevportselect">4. Redis采用的I/O多路复用函数：epoll/kqueue/evport/select</h4>
<ul>
<li>因地制宜</li>
<li>优先学则时间复杂度为O(1)的I/O多路复用函数作为底层实现</li>
<li>以时间复杂度的O(N)的select作为保底</li>
<li>基于react设计模式监听I/O事件</li>
</ul>
<h2 id="2-redis的数据类型">2. Redis的数据类型</h2>
<h3 id="1-供用户使用的数据类型">1. 供用户使用的数据类型</h3>
<ul>
<li>
<p>String：最基本的数据类型，二进制安全（底层使用sdshdr）</p>
<figure data-type="image" tabindex="4"><img src="https://zu3zz.coding.me/post-images/1581859149419.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>Hash：String元素组成的字典，适合用于存储对象（hmset key1 value1 key2 value2...）</p>
</li>
<li>
<p>List：列表，按照String元素插入顺序排序</p>
</li>
<li>
<p>Set：String元素组成的无序集合，通过哈希表实现，不允许重复(sadd myset 1)</p>
</li>
<li>
<p>Sorted Set：通过分数来为集合中的成员进行从小到达的排序（zadd myzset 3 a)\</p>
</li>
<li>
<p>高级：用来计数的HyperLogLog，用于支持存储地理位置信息的Geo</p>
</li>
</ul>
<h3 id="2-底层数据类型基础">2. 底层数据类型基础</h3>
<ol>
<li>简单动态字符串</li>
<li>链表</li>
<li>字典</li>
<li>跳跃表</li>
<li>整数集合</li>
<li>压缩列表</li>
<li>对象</li>
</ol>
<h2 id="3-从海量key里查询某一固定前缀的key">3. 从海量key里查询某一固定前缀的Key</h2>
<h3 id="1-keys-pattern查找所有符合给定模式pattern的key">1. KEYS pattern：查找所有符合给定模式pattern的key</h3>
<ul>
<li>KEYS指令一次性返回所有匹配的key</li>
<li>键的数量过大会使得服务卡顿</li>
</ul>
<h3 id="2-scan-cursor-match-pattern-count-count">2. SCAN cursor [MATCH pattern] [COUNT count]</h3>
<ul>
<li>基于游标的迭代器，需要基于上一次的游标延续之前的迭代过程</li>
<li>以0作为游标开始一次新的迭代，直到命令返回游标0完成一次遍历</li>
<li>不保证每次执行都返回某个给定数量的元素，支持模糊查询</li>
<li>一次返回的数量不可控，只能是大概率符合count参数</li>
</ul>
<h2 id="4-如何通过redis实现分布式锁">4. 如何通过Redis实现分布式锁</h2>
<h3 id="1-分布式锁需要解决的问题">1. 分布式锁需要解决的问题</h3>
<ul>
<li>互斥性</li>
<li>安全性</li>
<li>死锁</li>
<li>容错</li>
</ul>
<h3 id="2-setnx-key-value如果key不存在则创建并赋值">2. SETNX key value：如果key不存在，则创建并赋值</h3>
<h3 id="3-解决setnx-长期有效的问题">3. 解决SETNX 长期有效的问题</h3>
<ol>
<li>
<p>EXPIRE key seconds</p>
<ul>
<li>设置key的生存时间，当key过期时（生存时间为0），会被自动删除</li>
</ul>
</li>
<li>
<figure data-type="image" tabindex="5"><img src="https://zu3zz.coding.me/post-images/1581859174423.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>SET key value [EX seconds] [PX milliseconds] [NX|XX]</p>
<ul>
<li>EX second：设置键的过期时间为second秒</li>
<li>PX millisecond：设置键的过期时间为millisecond毫秒</li>
<li>NX ：只在键不存在时候，才对键进行操作 （等同于上面 SETNX key value）</li>
<li>XX：只在键不存在时候，才对键进行操作</li>
<li>SET 操作成功完成时， 返回OK, 否则返回nil</li>
</ul>
</li>
</ol>
<h3 id="4-大量key同时过期的注意事项">4. 大量key同时过期的注意事项</h3>
<ol>
<li>集中过期，由于清除大量的key很耗时，会出现短暂的卡顿现象</li>
<li>解决方案：在设置key的过期时间的时候，给每个key加上随机值</li>
</ol>
<h2 id="5-使用redis做异步队列">5. 使用Redis做异步队列</h2>
<h3 id="1-使用list作为队列">1. 使用List作为队列</h3>
<ol>
<li>
<p>RPUSH生产消息 LPOP消费消息</p>
<ul>
<li>缺点：没有等待队列里有值就直接消费</li>
<li>可以通过在应用层引入Sleep机制去调用LPOP重试</li>
</ul>
</li>
<li>
<p><code>BLPOP key [key...] timeout</code>：阻塞直到队列有消息或者超时</p>
</li>
</ol>
<h3 id="2-pubsub主题订阅者模式">2. pub/sub：主题订阅者模式</h3>
<ul>
<li>发送者（pub）发送消息，订阅者（sub）接收消息</li>
<li>先订阅一个频道，会自动获得这个频道里的消息</li>
<li>缺点：消息的发布是无状态的，无法表征可达</li>
</ul>
<h2 id="6-redis如何做持久化">6. Redis如何做持久化</h2>
<h3 id="1-rdb快照持久化保存某个时间点的全量数据快照">1. RDB（快照）持久化：保存某个时间点的全量数据快照</h3>
<ol>
<li>SAVE：阻塞Redis的服务器进程，直到RDB文件被创建完毕</li>
<li><strong>BGSAVE：Fork出一个子进程来创建RDB文件，不阻塞服务器</strong></li>
</ol>
<h3 id="2-自动化触发rbd持久化的方式">2. 自动化触发RBD持久化的方式</h3>
<ol>
<li>根据redis.conf配置里的<code>SAVE m n</code> 定时触发（用的是BGSAVE）</li>
<li>主从复制时，主节点自动触发</li>
<li>指定Debug Reload</li>
<li>执行Shutdown且没有开发AOF持久化</li>
</ol>
<h3 id="3-bgsave的原理">3. BGSAVE的原理</h3>
<figure data-type="image" tabindex="6"><img src="https://zu3zz.coding.me/post-images/1581859203147.png" alt="" loading="lazy"></figure>
<ul>
<li>系统调用fork()：创建进程，实现了Copy-on-Write</li>
<li>缺点
<ul>
<li>内存数据的全量同步，数据量大会由于I/O而影响性能</li>
<li>可能会因为Redis挂掉而损失从当前至最近一次快照期间的数据</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-aofappend-only-file持久化保存写状态">4. AOF（Append-Only-File）持久化：保存写状态</h3>
<ol>
<li>
<p>记录下除了查询以外的所有变更数据库状态的指令</p>
</li>
<li>
<p>以append的形式追加保存到AOF文件中（增量）</p>
</li>
<li>
<p>AOF默认是关闭的 在conf文件中配置</p>
</li>
<li>
<p>日志重写解决AOF文件大小不断增大的问题，原理如下：</p>
<ul>
<li>调用fork（），创建一个子进程</li>
<li>子进程吧新的AOF写到一个临时文件里，不依赖原来的AOF文件</li>
<li>主进程持续将新的变动同时写到内存和原来的AOF里</li>
<li>主进程获取子进程重写AOF的完成信号，往新AOF同步增量变动</li>
<li>使用新的AOF文件替换掉旧的AOF文件</li>
</ul>
</li>
</ol>
<hr>
<h3 id="5-数据恢复过程">5. 数据恢复过程</h3>
<ol>
<li>先看有没有AOF文件，再恢复</li>
<li>在看有没有RDB文件，再恢复</li>
</ol>
<hr>
<h3 id="6-两者优缺点">6. 两者优缺点</h3>
<ol>
<li>RDB优点：全局数据快照，文件小，恢复快</li>
<li>RDB缺点：无法保存最近一次快照之后的数据</li>
<li>AOF优点：可读性高，适合保存增量数据，数据不易丢失</li>
<li>AOF缺点：文件体积大，恢复时间长</li>
</ol>
<hr>
<h3 id="7-使用rdb-aof混合持久化方式">7. 使用RDB-AOF混合持久化方式</h3>
<ul>
<li>BGSAVE做镜像全量持久化，AOF做增量持久化</li>
</ul>
<h2 id="7-pipeline">7. Pipeline</h2>
<h3 id="1-使用pipeline的好处">1. 使用pipeline的好处</h3>
<ol>
<li>pipeline和linux的管道类似</li>
<li>redis基于请求/相应模型，单个请求处理需要一一应答</li>
<li>Pipeline批量执行指令，节省多次IO往返的时间</li>
<li>有顺序依赖的指令建议分批发送</li>
</ol>
<hr>
<h3 id="2-redis的同步机制">2. Redis的同步机制</h3>
<ol>
<li>
<p>主从同步原理（Master/Slave）</p>
<figure data-type="image" tabindex="7"><img src="https://zu3zz.coding.me/post-images/1581859218260.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>全同步过程</p>
<ul>
<li>Slave发送sync命令到master</li>
<li>Master启动一个后台进程，将Redis中的数据快照保存到文件中（BGSAVE）</li>
<li>Master将保存数据快照期间接收到的写命令缓存起来</li>
<li>Master完成写文件操作后，将该文件发送给Slave</li>
<li>使用新的AOF文件替换掉旧的AOF文件</li>
<li>Master将这期间收集到的增量写命令发送给Slave端</li>
</ul>
</li>
<li>
<p>增量同步过程</p>
<ul>
<li>Master接收到用户的操作指令，判断是否需要传播到Slave</li>
<li>将操作记录追加到AOF文件</li>
<li>将操作传播到其他Slave：
<ol>
<li>对齐主从库</li>
<li>往相应缓存写入指令</li>
</ol>
</li>
<li>将缓存中的数据发送给Slave</li>
</ul>
</li>
<li>
<p>Redis Sentinel（Redis哨兵）</p>
<ol>
<li>解决主从同步Master宕机后的主从切换问题：
<ul>
<li>监控：检查主从服务器是否运行正常</li>
<li>提醒：通过API向管理员或者其他应用程序发送故障通知</li>
<li>自动故障迁移：主从切换</li>
</ul>
</li>
</ol>
</li>
<li>
<p>流言协议Gossip</p>
<ol>
<li>在杂乱无章中寻求一致</li>
<li>每个节点都随机地与对方通信，最终所有节点的状态达成一致</li>
<li>种子节点定期随机向其他节点发送节点列表以及需要传播的消息</li>
<li>不保证信息一定会传递给所有节点，但是最终会趋于一致</li>
</ol>
</li>
</ol>
<h2 id="8-redis集群">8. Redis集群</h2>
<h3 id="1-如何从海量数据里快速找到所需">1. 如何从海量数据里快速找到所需</h3>
<ul>
<li>分片：按照某种规则去划分数据，分散存储在多个节点上</li>
</ul>
<hr>
<h3 id="2-redis的集群原理">2. Redis的集群原理</h3>
<ul>
<li>
<p>一致性哈希算法：对 2^32 取模，将哈希值空间组织成虚拟的圆环</p>
</li>
<li>
<p>将数据key使用相同的函数Hash计算出哈希值，找到最近的Hash节点</p>
<figure data-type="image" tabindex="8"><img src="https://zu3zz.coding.me/post-images/1581859268434.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>如果此时<code>Node C</code>宕机,数据都会到<code>Node D</code>中</p>
</li>
</ul>
<figure data-type="image" tabindex="9"><img src="https://zu3zz.coding.me/post-images/1581859274355.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>新增一台服务器 <code>Node X</code></p>
<figure data-type="image" tabindex="10"><img src="https://zu3zz.coding.me/post-images/1581859282580.png" alt="" loading="lazy"></figure>
</li>
</ul>
<h3 id="3-数据倾斜问题">3. 数据倾斜问题</h3>
<ol>
<li>
<p>Hash环的数据倾斜问题</p>
<figure data-type="image" tabindex="11"><img src="https://zu3zz.coding.me/post-images/1581859296841.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>引入虚拟节点解决数据倾斜的问题（节点过少时有用）（设置虚拟节点32个）</p>
<figure data-type="image" tabindex="12"><img src="https://zu3zz.coding.me/post-images/1581859303063.png" alt="" loading="lazy"></figure>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[数据库面试考点]]></title>
        <id>https://zu3zz.coding.me/post/interview-database/</id>
        <link href="https://zu3zz.coding.me/post/interview-database/">
        </link>
        <updated>2020-02-12T10:51:52.000Z</updated>
        <summary type="html"><![CDATA[<p>🐰详细讲解数据库面试常见问题，包含索引、锁、事务等内容🐰</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐰详细讲解数据库面试常见问题，包含索引、锁、事务等内容🐰</p>
<!-- more -->
<h1 id="数据库面试总结">数据库面试总结</h1>
<h2 id="1-架构">1. 架构</h2>
<h3 id="1-rdbms">1. RDBMS</h3>
<ol>
<li>程序实例
<ul>
<li>存储管理</li>
<li>缓存机制</li>
<li>SQL解析</li>
<li>日志管理</li>
<li>权限划分</li>
<li><strong>索引管理</strong></li>
<li><strong>锁管理</strong></li>
</ul>
</li>
<li>存储（文件系统）</li>
</ol>
<h2 id="2-索引">2. 索引</h2>
<ol>
<li>为什么要使用索引：避免全表扫描/快速查询数据</li>
<li>什么样的信息能成为索引：主键、唯一键</li>
<li>索引的数据结构
<ul>
<li>建立二叉查找树进行二分查找</li>
<li>建立B-Tree结构进行查找</li>
<li>建立B+-Tree结构进行查找</li>
<li>建立Hash结构进行查找</li>
</ul>
</li>
</ol>
<hr>
<h3 id="1-优化索引">1. 优化索引</h3>
<ol>
<li>
<p>二叉查找树</p>
<figure data-type="image" tabindex="1"><img src="https://zu3zz.coding.me/post-images/1581850416812.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>B-Tree（logn）</p>
<ol>
<li>根节点至少包括两个孩子</li>
<li>树中每个节点最多含有m个孩子（m&gt;=2）</li>
<li>除根节点和叶节点外，其他每个节点至少有ceil（m/2）个孩子</li>
<li>所有叶子节点都位于同一层</li>
</ol>
<figure data-type="image" tabindex="2"><img src="https://zu3zz.coding.me/post-images/1581850429749.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>B+树</p>
<ol>
<li>定义基本与B树相同</li>
<li>除了非叶子节点的指针必须与叶子结点数相同</li>
<li>非叶子节点的子树指针P[i]，指向关键字符[K[i]，K[i+1]]的子树</li>
<li>非叶子节点仅用来索引，数据都保存在叶子结点中</li>
<li>所有叶子节点均有一个链指针指向下一个叶子结点</li>
</ol>
<figure data-type="image" tabindex="3"><img src="https://zu3zz.coding.me/post-images/1581850440775.png" alt="" loading="lazy"></figure>
</li>
<li>
<p>为什么B+树更适合用来做存储索引</p>
<ul>
<li>B+树的磁盘读写代价更低</li>
<li>B+树的查询效率更加稳定</li>
<li>B+树更有利于对数据库的扫描</li>
</ul>
</li>
</ol>
<hr>
<h3 id="2-hash索引">2. Hash索引</h3>
<figure data-type="image" tabindex="4"><img src="https://zu3zz.coding.me/post-images/1581850457121.png" alt="" loading="lazy"></figure>
<h4 id="缺点">缺点：</h4>
<ol>
<li>仅仅能满足“=”，“IN&quot;，不能使用范围查询</li>
<li>无法被用来避免数据的排序操作</li>
<li>不能利用部分索引键查询</li>
<li>不能避免表扫描</li>
<li>遇到大量Hash值相等的情况后性能并不一定就会比B-Tree索引高</li>
</ol>
<hr>
<h3 id="3-bitmap索引了解">3. BitMap索引（了解）</h3>
<ul>
<li>位图索引</li>
</ul>
<hr>
<h3 id="4-密集索引与稀疏索引的区别">4. 密集索引与稀疏索引的区别</h3>
<ul>
<li>密集索引文件中的每个搜索码值都对应一个索引值</li>
<li>系数索引文件只为索引码的某些值建立索引项</li>
</ul>
<ol>
<li>对于InnoDB
<ol>
<li>若一个主键被定义，则该主键作为密集索引</li>
<li>若没有主键被定义，该表的第一个唯一非空索引则作为密集索引</li>
<li>若没有，innodb内部会生成一个隐藏主键（密集索引）</li>
<li>非主键索引存储相关键位和其对应的主键值，包含两次查找</li>
<li>索引和数据是存在一起的</li>
</ol>
</li>
<li>对于MyISAM
<ol>
<li>都是稀疏索引</li>
<li>索引和数据是分开的</li>
</ol>
</li>
</ol>
<hr>
<h3 id="5-问题回顾与总结">5. 问题回顾与总结</h3>
<ol>
<li>为什么要使用索引</li>
<li>什么样的信息能成为索引</li>
<li>索引的数据结构</li>
<li>密集索引和系数索引的区别</li>
</ol>
<hr>
<h3 id="6-一些衍生问题">6. 一些衍生问题</h3>
<h4 id="61-如何定位并优化慢查询sql">6.1 如何定位并优化慢查询Sql</h4>
<ol>
<li>根据慢日志定位慢查询sql</li>
<li>使用explain等工具分析sql
<ul>
<li>using filesort：文件排序 不能使用索引</li>
<li>using temporary：使用临时表</li>
</ul>
</li>
<li>修改sql或者尽量让sql走索引</li>
</ol>
<h4 id="62-联合索引的最左匹配原则的成因">6.2 联合索引的最左匹配原则的成因</h4>
<ul>
<li>MySQL会一直向右匹配到范围查询（&gt;、&lt;、between、like）就停止匹配</li>
</ul>
<pre><code class="language-sql"> a = 1 and b = 2 and c &gt; 3 and d = 4 
 如果遇到（a,b,c,d)这样的，到c就停止了
</code></pre>
<h4 id="63-索引是建立的越多越好吗">6.3 索引是建立的越多越好吗</h4>
<ul>
<li>否。数据量小的表不需要建立索引，建立或增加额外的索引开销</li>
<li>数据变更需要维护索引，因此更多的索引意味着更多的维护成本</li>
<li>更多的索引意味着也需要更多的空间</li>
</ul>
<h2 id="3-锁">3. 锁</h2>
<h3 id="31-常见问题">3.1 常见问题</h3>
<ol>
<li>MyISAM与InnoDB关于锁方面的区别是什么</li>
<li>数据库事务的四大特性</li>
<li>事务隔离级别以及各2级别下的并发访问问题</li>
<li>InnoDB可重复读隔离级别下如何避免幻读</li>
<li>RC、RR级别下的InnoDB的非阻塞读如何实现</li>
</ol>
<hr>
<h4 id="1-myisam与innodb关于锁方面的区别是什么">1. MyISAM与InnoDB关于锁方面的区别是什么</h4>
<ol>
<li>MyISAM默认用的是表级锁，不支持行级锁</li>
<li>InnoDB默认用的是行级锁，也支持表级锁</li>
<li>写锁 又叫 排它锁</li>
<li>读锁 又叫 共享锁</li>
<li>先读再写不会被锁</li>
<li>先写再读会被锁</li>
</ol>
<figure data-type="image" tabindex="5"><img src="https://zu3zz.coding.me/post-images/1581850500524.png" alt="" loading="lazy"></figure>
<h4 id="2-myisam适合的场景">2. MyISAM适合的场景</h4>
<ol>
<li>频繁执行全表count语句</li>
<li>对数据进行增删改的频率不搞，查询非常频繁</li>
<li>没有事务</li>
</ol>
<h4 id="3-innodb适合的场景">3. InnoDB适合的场景</h4>
<ol>
<li>数据增删改查都相当频繁</li>
<li>可靠性要求比较高，要求支持事务</li>
</ol>
<h3 id="32-数据库锁的分类">3.2 数据库锁的分类</h3>
<ul>
<li>按锁的粒度分化，可分为表级锁、行级锁、页级锁</li>
<li>按锁的级别划分，可分为共享锁、排它锁</li>
<li>按枷锁方式划分，课分为自动锁、显式锁</li>
<li>按操作划分，可分为DML锁、DDL锁</li>
<li>按使用方式划分，可分为乐观锁、悲观锁</li>
</ul>
<h4 id="1-乐观锁的实现方式">1. 乐观锁的实现方式</h4>
<ul>
<li>基于时间戳</li>
<li>基于版本号（写SQL语句的时候先检查version）</li>
</ul>
<h3 id="33-数据库事务的四大特性">3.3 数据库事务的四大特性</h3>
<ol>
<li>原子性（Atomic）：发生错误回滚</li>
<li>一致性（Consistency）</li>
<li>隔离性（Isolation）：一个事务的执行不应该影响到其他事务</li>
<li>持久性（Durability）：redo log</li>
</ol>
<h3 id="34-事务隔离级别以及各级别下的并发访问问题">3.4 事务隔离级别以及各级别下的并发访问问题</h3>
<ol>
<li>更新丢失：mysql所有事务隔离级别在数据库层面上均可避免</li>
<li>脏读：一个事务读到另外一个事务未提交的数据：READ——COMMITTED事务隔离级别以上可避免（MyISAM默认）
<ul>
<li>只能读到其他事务提交的级别，就不会造成问题了</li>
</ul>
</li>
<li>不可重复读：REPEATABLE-READ事务隔离级别以上可避免（InnoDB默认）</li>
<li>幻读：另外一个事务对同一个数据库进行了插入或者删除操作：SERIALIZABLE事务隔离级别可避免</li>
</ol>
<figure data-type="image" tabindex="6"><img src="https://zu3zz.coding.me/post-images/1581850513584.png" alt="" loading="lazy"></figure>
<h3 id="35-innodb课重复读隔离级别下如何避免幻读">3.5 InnoDB课重复读隔离级别下如何避免幻读</h3>
<ol>
<li>表象：快照读（非阻塞读）——伪MVCC
<ul>
<li>当前读：update、delete、insert、select...lock in share mode</li>
<li>快照读：不加锁的非阻塞读、select</li>
<li>再次调用快照读会出问题</li>
</ul>
</li>
<li>内在：next-key锁（行锁+gap锁）（搞不懂）
<ul>
<li>在Serializable下 是用的这个</li>
</ul>
</li>
</ol>
<h3 id="36-rc-rr级别下的innodb的非阻塞读如何实现">3.6 RC、RR级别下的InnoDB的非阻塞读如何实现</h3>
<ol>
<li>数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID字段</li>
<li>从 undo日志 里恢复</li>
<li>read view</li>
</ol>
<h2 id="4-语法">4. 语法</h2>
<ol>
<li>GROUP BY</li>
<li>HAVING</li>
<li>统计相关：COUNT、SUM、MAX、MIN、AVG</li>
</ol>
<h3 id="1-group-by分组">1. GROUP BY（分组）</h3>
<ul>
<li>满足 “select字句中的列名必须为分组列或者列函数”</li>
<li>列函数对于group by字句定义的每个组各返回一个结果</li>
<li>如果用group by，那么你的Select语句中选出的列要么是你group by里用到的列，要么就是带有之前我们说的如sum min等列函数的列，如果带上其他的，会报错。</li>
</ul>
<h3 id="2-having过滤">2. HAVING（过滤）</h3>
<ul>
<li>通常与GROUP BY字句一起使用</li>
<li>WHERE过滤行，HAVING过滤组</li>
<li>出现在同一行sql的顺序：WHERE&gt;GROUP BY&gt;HAVING</li>
</ul>
<h3 id="3自己多练">3.自己多练</h3>
<h2 id="5-myisam与innodb区别">5 MyISAM与InnoDB区别</h2>
<ol>
<li>InnoDB支持事务，MyISAM不支持，对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务；</li>
<li>InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败；</li>
<li>InnoDB是聚集索引，使用B+Tree作为索引结构，数据文件是和（主键）索引绑在一起的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。</li>
<li>InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件）</li>
<li>Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快高；PS：5.7以后的InnoDB支持全文索引了</li>
<li>MyISAM表格可以被压缩后进行查询操作</li>
<li>InnoDB支持表、行(默认)级锁，而MyISAM支持表级锁</li>
<li>InnoDB表必须有主键（用户没有指定的话会自己找或生产一个主键），而Myisam可以没有</li>
<li>Innodb存储文件有frm、ibd，而Myisam是frm、MYD、MYI
<ul>
<li>Innodb：frm是表定义文件，ibd是数据文件</li>
<li>Myisam：frm是表定义文件，myd是数据文件，myi是索引文件</li>
</ul>
</li>
</ol>
<h2 id="6-sql语句的优化">6. SQL语句的优化</h2>
<h4 id="1-定位慢sql语句">1. 定位慢SQL语句</h4>
<ul>
<li>
<p>好慢询可以帮我们找到执行慢的 SQL，在使用前，我们需要先看下慢查询是否已经开启，使用下面这条命令即可：</p>
<pre><code class="language-sql">mysql &gt; show variables like '%slow_query_log';
</code></pre>
</li>
<li>
<p>我们能看到 slow_query_log=OFF，也就是说慢查询日志此时是关上的。我们可以把慢查询日志打开，注意设置变量值的时候需要使用 global，否则会报错</p>
<pre><code class="language-sql">mysql &gt; set global slow_query_log='ON';
mysql &gt; set global long_query_time = 3;
</code></pre>
</li>
</ul>
<h4 id="2-使用explain">2. 使用EXPLAIN</h4>
<p>EXPLAIN 可以帮助我们了解数据表的读取顺序、SELECT 子句的类型、数据表的访问类型、可使用的索引、实际使用的索引、使用的索引长度、上一个表的连接匹配条件、被优化器查询的行的数量以及额外的信息（比如是否使用了外部排序，是否使用了临时表等）等。</p>
<pre><code class="language-sql">EXPLAIN SELECT comment_id, comment_text, user_id FROM product_comment WHERE comment_id = 500000;
</code></pre>
<h4 id="3-使用profile">3. 使用PROFILE</h4>
<ul>
<li>SHOW PROFILE 相比 EXPLAIN 能看到更进一步的执行解析，包括 SQL 都做了什么、所花费的时间等。默认情况下，profiling 是关闭的，我们可以在会话级别开启这个功能。</li>
</ul>
<pre><code class="language-sql">mysql &gt; set profiling = 'ON';
mysql &gt; show profiles;
</code></pre>
<ul>
<li>不过 SHOW PROFILE 命令将被弃用，我们可以从 information_schema 中的 profiling 数据表进行查看。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[计算机网络面试考点总结]]></title>
        <id>https://zu3zz.coding.me/post/interview-network/</id>
        <link href="https://zu3zz.coding.me/post/interview-network/">
        </link>
        <updated>2020-02-06T17:00:31.000Z</updated>
        <summary type="html"><![CDATA[<p>🐱一文总结计算机网络面试常见考点，包括TCP HTTP等🐱</p>
]]></summary>
        <content type="html"><![CDATA[<p>🐱一文总结计算机网络面试常见考点，包括TCP HTTP等🐱</p>
<!-- more -->
<h1 id="计算机网络面试考点总结">计算机网络面试考点总结</h1>
<h2 id="1-tcp的三次握手">1. TCP的三次握手</h2>
<ol>
<li>第一次握手，建立连接时，客户端发送一个SYN包，SYN=j到服务器，并且进入SYN_SEND状态，等到服务器确认</li>
<li>服务器收到SYN包，必须确认客户的SYN值（ack=j+1），同时自己也发送一个SYN包，SYN=K，级SYN + ACK包，此时服务器也进入SYN_SEND状态</li>
<li>客户端收到服务器端的SYN+ACK包，向服务器发送确认包ACK（ack = k + 1），此包发送完毕之后，客户端和服务器都进入ESTABLISHED状态，三次握手完成。</li>
</ol>
<h4 id="11-为什么需要三次握手">1.1 为什么需要三次握手</h4>
<ul>
<li>为了初始化Seqence Number的初始值，用于拼接数据</li>
</ul>
<h4 id="12-syn超时">1.2 SYN超时</h4>
<ul>
<li>不断重试，Linux默认63秒</li>
</ul>
<h4 id="13-针对syn-flood">1.3 针对Syn Flood</h4>
<ul>
<li>
<p>syn队列满了之后，通过tcp_syncookies参数回发 SYN Cookie</p>
</li>
<li>
<p>若正常连接则Clinet会回发SYN Cookie，直接建立连接</p>
</li>
</ul>
<h4 id="14-client出现故障">1.4 Client出现故障</h4>
<ul>
<li>向对方发送保活探测报文</li>
</ul>
<h2 id="2-tcp四次挥手">2. TCP四次挥手</h2>
<ol>
<li>Client发送一个FIN，用来关系Client到Server的数据传送，Client进入FIN_WAIT_1状态</li>
<li>Server收到FIN之后，发送一个ACK给Client，确认需要为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态</li>
<li>Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态</li>
<li>Client收到FIN之后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。</li>
</ol>
<h2 id="3-tcp和udp">3. TCP和UDP</h2>
<ol>
<li>面向非连接，没有快重传等。</li>
<li>不维护连接状态，支持同时向多个客户端传输相同的消息</li>
<li>数据包报头只有8个字节</li>
<li>面向报文</li>
<li>尽最大努力交付</li>
</ol>
<hr>
<h3 id="31-区别">3.1 区别</h3>
<ol>
<li>面向连接 vs 无连接</li>
<li>可靠性</li>
<li>有序性</li>
<li>速度</li>
<li>量级</li>
</ol>
<hr>
<h3 id="32-滑动窗口协议">3.2 滑动窗口协议</h3>
<ol>
<li>RTT: 发送数据包到收到对应的ACK，所花费的时间</li>
<li>RTO: 重传时间间隔</li>
<li>使用滑动窗口做流量控制与乱序重拍
<ul>
<li>保证TCP的可靠性</li>
<li>保证TCP的流量控制特性</li>
</ul>
</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://zu3zz.coding.me/post-images/1581008533854.png" alt="" loading="lazy"></figure>
<h2 id="4-http11">4. HTTP(1.1)</h2>
<h3 id="1-主要特点">1. 主要特点</h3>
<ol>
<li>支持客户/服务器模式</li>
<li>简单快速</li>
<li>灵活</li>
<li>无连接</li>
</ol>
<hr>
<h3 id="2-请求相应的步骤">2. 请求/相应的步骤</h3>
<ol>
<li>客户端连接到Web服务器</li>
<li>发送HTTP请求</li>
<li>服务器接收请求并返回HTTP相应</li>
<li>释放连接TCP连接</li>
<li>客户端浏览器解析HTML内容</li>
</ol>
<hr>
<h3 id="3-输入url之后按下回车之后经过的流程">3. 输入URL之后，按下回车之后经过的流程</h3>
<ol>
<li>DNS解析 找对对应的IP地址</li>
<li>TCP连接</li>
<li>发送HTTP请求</li>
<li>服务器处理请求并返回HTTP报文</li>
<li>服务器解析渲染页面</li>
<li>连接结束</li>
</ol>
<hr>
<h3 id="4-http状态码">4. HTTP状态码</h3>
<ul>
<li>五种可能取值
<ul>
<li>1xx：指示信息--表示请求已接收，继续处理</li>
<li>2xx：成功--表示请求已被成功接收、理解、</li>
<li>3xx：重定向--要完成请求必须进行更进一步的操作</li>
<li>4xx：客户端错误--请求有语法错误或者请求无法实现</li>
<li>5xx：服务器端错误--服务器未能实现合法的请求</li>
</ul>
</li>
<li>常见状态码
<ul>
<li>200 OK：正常 返回信息</li>
<li>400 Bad Request：客户端请求有语法错误，不能被服务器所理解</li>
<li>401 Unauthorized：请求未经授权，这个状态码必须和WWW-Authenticate报头域一起使用</li>
<li>403 Forbidden：服务器收到请求，但是拒绝提供服务</li>
<li>404 Not Found：请求资源不存在，eg，输入了错误的URL</li>
<li>500 Internal Server Error：服务器发生不可预期的错误</li>
<li>503 Server Unabailable：服务器当前不能处理客户端的请求，一段时间后可能恢复正常</li>
</ul>
</li>
</ul>
<hr>
<h3 id="5-get和post请求的区别">5. GET和POST请求的区别</h3>
<h4 id="1-从三个层面来解答">1. 从三个层面来解答</h4>
<ol>
<li>Http报文层面：GET将请求信息（键值对）放到URL中，POST则放在报文体中</li>
<li>数据库层面：GET符合幂等性和安全性，POST不符合（两个都不符合）</li>
<li>其他层面：GET可以被缓存、被存储，而POST不行</li>
</ol>
<hr>
<h3 id="6-cookie和session的区别">6. Cookie和Session的区别</h3>
<h4 id="cookie简介">Cookie简介</h4>
<ul>
<li>是由服务器发给客户端的特殊信息，以文本的形式存放在客户端</li>
<li>客户端再次请求的时候，会吧Cookie回发</li>
<li>服务器接收到后，会解析Cookie生成与客户端相对应的内容</li>
</ul>
<h4 id="cookie的设置以及发送过程">Cookie的设置以及发送过程</h4>
<figure data-type="image" tabindex="2"><img src="https://zu3zz.coding.me/post-images/1581008546981.png" alt="" loading="lazy"></figure>
<h4 id="session简介">Session简介</h4>
<ul>
<li>服务器端的机制，在服务器上保存的信息（类似hash表）</li>
<li>解析客户端请求并操作session id，按需保存状态信息</li>
</ul>
<h4 id="session的实现方式">Session的实现方式</h4>
<ol>
<li>使用Cookie来实现：服务器给客户端一个JSessionID</li>
<li>使用URL回写来实现</li>
</ol>
<h4 id="cookie和session的区别">Cookie和Session的区别</h4>
<ol>
<li>Cookie数据存放在客户的浏览器上，Session数据放在服务器上</li>
<li>Session相对于Cookie更安全</li>
<li>若考虑减轻服务器负担，应当使用Cookie</li>
</ol>
<h2 id="5-http和https的区别">5 HTTP和HTTPS的区别</h2>
<h3 id="1-https简介">1. HTTPS简介</h3>
<figure data-type="image" tabindex="3"><img src="https://zu3zz.coding.me/post-images/1581008556887.png" alt="" loading="lazy"></figure>
<hr>
<h3 id="2-sslsecurity-sockets-layer-安全套接层">2. SSL（Security Sockets Layer, 安全套接层）</h3>
<ol>
<li>为网络通信提供安全以及数据完整性的一种安全协议</li>
<li>是操作系统对外的API，SSL3.0后更名为TLS</li>
<li>采用身份验证和数据加密保证网络通信的安全和数据的完整性</li>
</ol>
<hr>
<h3 id="3-加密方式">3. 加密方式</h3>
<ol>
<li>对称加密：加密和解密都使用同一个密钥</li>
<li>非对称加密：加密使用的密钥和解密使用的密钥是不相同的</li>
<li>哈希算法：将任意长度的信息转换为固定长度的值，算法不可逆</li>
<li>数字签名：证明某个信息或者文件是某个人发出/认同的</li>
</ol>
<hr>
<h3 id="4-数据传输流程">4. 数据传输流程</h3>
<ol>
<li>浏览器将支持的加密算法信息发送给服务器</li>
<li>服务器选择一套浏览器支持的加密算法，以证书的形式回发浏览器</li>
<li>浏览器验证证书合法性，并结合证书公钥加密信息发送给服务器</li>
<li>服务器使用私钥解密信息，验证哈希，加密响应消息回发浏览器</li>
<li>浏览器解密响应信息，并对消息进行验证，之后就用同一套秘钥进行加密交换</li>
</ol>
<hr>
<h3 id="5-http和https的区别-2">5. HTTP和HTTPS的区别</h3>
<ol>
<li>HTTPS需要到CA申请证书，HTTP不需要</li>
<li>HTTPS密文传输，HTTP明文传输</li>
<li>连接方式不同，HTTPS默认使用443端口，HTTP使用80端口</li>
<li>HTTPS = HTTP+加密+认证+完整性保护，较HTTP安全</li>
</ol>
<hr>
<h3 id="6-潜在危险">6. 潜在危险</h3>
<ol>
<li>浏览器默认填充http：// ，请求需要进行跳转，又被劫持的风险</li>
<li>可以使用HSTS（HTTP Strict Transport Security）优化</li>
</ol>
<h2 id="6-socket">6. Socket</h2>
<ul>
<li>Socket是对TCP/IP协议的抽象，是操作系统对外开放的接口</li>
</ul>
<h3 id="1-socket通信流程">1. Socket通信流程</h3>
<figure data-type="image" tabindex="4"><img src="https://zu3zz.coding.me/post-images/1581008567663.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkSQL实战笔记（2）---- SparkSQL 概述]]></title>
        <id>https://zu3zz.coding.me/post/sparksql-2/</id>
        <link href="https://zu3zz.coding.me/post/sparksql-2/">
        </link>
        <updated>2020-01-14T11:46:21.000Z</updated>
        <content type="html"><![CDATA[<h1 id="sparksql-实战笔记2-sparksql-概述">SparkSQL 实战笔记（2）---- SparkSQL 概述</h1>
<p>##1. 为什么需要 SQL？</p>
<ol>
<li>
<p>事实上的标准</p>
<ul>
<li>
<p>MySQL/Oracle/DB2... RBDMS 关系型数据库 是不是过时呢？</p>
</li>
<li>
<p>数据规模 大数据的处理</p>
<pre><code class="language-shell">MR：Java
Spark：Scala、Java、Python
</code></pre>
</li>
<li>
<p>直接使用 SQL 语句来对数据进行处理分析呢？ 符合市场的需求</p>
<pre><code class="language-shell">Hive SparkSQL Impala...
</code></pre>
</li>
<li>
<p>受众面大、容易上手、易学易用:<code>DDL DML</code></p>
<pre><code class="language-shell"># access.log日志
1,zhangsan,10,beijing
2,lisi,11,shanghai
3,wangwu,12,shenzhen
</code></pre>
</li>
<li>
<p><code>table: Hive/Spark SQL/Impala</code> ：共享元数据</p>
<ul>
<li>name: access</li>
<li>columns: id int,name string,age int,city string</li>
</ul>
<pre><code class="language-mysql">SQL: select xxx from access where ... group by ... having....
</code></pre>
</li>
</ul>
</li>
</ol>
<h2 id="2-sql-on-hadoop">2. SQL on Hadoop</h2>
<ol>
<li>
<p>使用 SQL 语句对大数据进行统计分析，数据是在 Hadoop</p>
</li>
<li>
<p><code>Apache Hive</code></p>
<ul>
<li>SQL 转换成一系列可以在 Hadoop 上运行的 MapReduce/Tez/Spark 作业</li>
<li>SQL 到底底层是运行在哪种分布式引擎之上的，是可以通过一个参数来设置</li>
<li>功能：
<ul>
<li>SQL：命令行、代码</li>
<li>多语言 Apache Thrift 驱动</li>
<li>自定义的 UDF 函数：按照标准接口实现，打包，加载到 Hive 中</li>
<li>元数据</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Cloudera Impala</code></p>
<ul>
<li>使用了自己的执行守护进程集合，一般情况下这些进程是需要与 Hadoop DN 安装在一个节点上</li>
<li>功能：
<ul>
<li>92 SQL 支持</li>
<li>Hive 支持</li>
<li>命令行、代码</li>
<li>与 Hive 能够共享元数据</li>
<li>性能方面是 Hive 要快速一些，基于内存</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Spark SQL</code></p>
<ul>
<li>
<p>Spark 中的一个子模块，是不是仅仅只用 SQL 来处理呢？</p>
<pre><code class="language-shell">$ Hive：SQL ==&gt; MapReduce
</code></pre>
</li>
<li>
<p><code>Spark</code>：能不能直接把 SQL 运行在 Spark 引擎之上呢？</p>
<ol>
<li><code>Shark</code>： <code>SQL==&gt;Spark</code> （不再维护）
<ul>
<li>优点：快 与 Hive 能够兼容</li>
<li>缺点：执行计划优化完全依赖于 Hive 进程 vs 线程</li>
<li>使用：需要独立维护一个打了补丁的 Hive 源码分支</li>
</ul>
</li>
<li><code>Spark SQL</code>: 这是 Spark 项目中的<code>SQL</code>子项目</li>
</ol>
</li>
</ul>
</li>
<li>
<p><code>Hive on Spark</code> ： 这是<code>Hive</code>项目中的，通过切换 Hive 的执行引擎即可，底层添加了 Spark 执行引擎的支持</p>
</li>
<li>
<p><code>Presto</code></p>
<ul>
<li>交互式查询引擎 SQL</li>
<li>功能：
<ul>
<li>共享元数据信息</li>
<li>92 SQL 语法</li>
<li>提供了一系列的连接器，<code>Hive</code> <code>Cassandra</code>...</li>
</ul>
</li>
</ul>
</li>
<li>
<p><code>Drill</code></p>
<ol>
<li>HDFS、Hive、Spark SQL</li>
<li>支持多种后端存储，然后直接进行各种后端数据的处理</li>
<li>未来的趋势</li>
</ol>
</li>
<li>
<p><code>Phoenix</code></p>
<ol>
<li>HBase 的数据，是要基于 API 进行查询</li>
<li>Phoenix 使用 SQL 来查询 HBase 中的数据</li>
<li>主要点：如果想查询的快的话，还是取决于 ROWKEY 的设计</li>
</ol>
</li>
</ol>
<h2 id="3-spark-sql-是什么">3. Spark SQL 是什么</h2>
<h3 id="31-spark-sql-概念">3.1 Spark SQL 概念</h3>
<ol>
<li>
<p>Spark SQL is Apache Spark's module for working with structured data.</p>
<ul>
<li>误区一：Spark SQL 就是一个 SQL 处理框架</li>
</ul>
<ol>
<li>
<p>集成性：在 Spark 编程中无缝对接多种复杂的 SQL</p>
</li>
<li>
<p>统一的数据访问方式：以类似的方式访问多种不同的数据源，而且可以进行相关操作</p>
<pre><code class="language-scala">spark.read.format(&quot;json&quot;).load(path)
spark.read.format(&quot;text&quot;).load(path)
spark.read.format(&quot;parquet&quot;).load(path)
spark.read.format(&quot;json&quot;).option(&quot;...&quot;,&quot;...&quot;).load(path)
</code></pre>
</li>
<li>
<p>兼容 Hive</p>
<ul>
<li>allowing you to access existing Hive warehouses</li>
<li>如果你想把 Hive 的作业迁移到 Spark SQL，这样的话，迁移成本就会低很多</li>
</ul>
</li>
<li>
<p>标准的数据连接：提供标准的<code>JDBC/ODBC</code>连接方式到 Server 上</p>
</li>
</ol>
</li>
<li>
<p>Spark SQL 应用并不局限于 SQL</p>
<ol>
<li>还支持 Hive、JSON、Parquet 文件的直接读取以及操作</li>
<li>SQL 仅仅是 Spark SQL 中的一个功能而已</li>
</ol>
</li>
<li>
<p>为什么要学习 Spark SQL</p>
<ol>
<li>SQL 带来的便利性</li>
<li>Spark Core： RDD Scala/Java
<ul>
<li>需要熟悉 Java、Scala 语言</li>
</ul>
</li>
<li>Spark SQL
<ul>
<li>Catalyst 为我们自动做了很多的优化工作</li>
<li>SQL(只要了解业务逻辑，然后使用 SQL 来实现)</li>
<li>DF/DS：面向 API 编程的，使用一些 Java/Scala</li>
</ul>
</li>
</ol>
</li>
</ol>
<h3 id="32-spark-sql-架构">3.2 Spark SQL 架构</h3>
<h4 id="321-前端frontend">3.2.1 前端（FrontEnd）</h4>
<ol>
<li>
<p>Hive AST : SQL 语句（字符串）==&gt; 抽象语法树</p>
</li>
<li>
<p>Spark Program : DF/DS API</p>
</li>
<li>
<p>Streaming SQL</p>
</li>
<li>
<p>Catalyst</p>
<ul>
<li>Unresolved LogicPlan</li>
</ul>
<pre><code class="language-mysql">select empno, ename from emp
</code></pre>
</li>
<li>
<p>Schema Catalog 和 MetaStore</p>
</li>
<li>
<p>LogicPlan</p>
</li>
<li>
<p>Optimized LogicPlan</p>
<pre><code class="language-mysql">select * from (select ... from xxx limit 10) limit 5;
将我们的SQL作用上很多内置的Rule，使得我们拿到的逻辑执行计划是比较好的
</code></pre>
<p><code>Physical Plan</code></p>
</li>
</ol>
<h4 id="322-后端backend">3.2.2 后端（Backend）</h4>
<ol>
<li>
<p><code>spark-shell</code></p>
<ul>
<li>每个 Spark 应用程序（spark-shell）在不同目录下启动，其实在该目录下是有 metastore_db</li>
<li>单独的</li>
<li>如果你想 spark-shell 共享我们的元数据的话，肯定要指定元数据信息==&gt; 后续讲 Spark SQL 整合 Hive 的时候讲解</li>
<li><code>spark.sql</code>(sql 语句)</li>
</ul>
</li>
<li>
<p>spark-sql 的使用<br>
spark-shell 你会发现如果要操作 SQL 相关的东西，要使用 spark.sql(sql 语句)</p>
<pre><code class="language-mysql">explain extended
select a.key\*(3+5), b.value from t a join t b on a.key = b.key and a.key &gt; 3;
</code></pre>
<ul>
<li>优化的过程中，可以把一些条件过滤前置</li>
</ul>
</li>
<li>
<p>spark-shell 启动流程分析</p>
<ul>
<li>
<p>REPL: Read-Eval-Print Loop 读取-求值-输出</p>
</li>
<li>
<p>提供给用户即时交互一个命令窗口</p>
<pre><code class="language-mysql">case \$变量名 in
模式 1
command1
;;
模式 2
command2
;;
\*)
default
;;
esac
</code></pre>
</li>
<li>
<p>spark-shell 底层调用的是 spark-submit</p>
</li>
<li>
<p>spark-submit 底层调用的是 spark-class</p>
</li>
</ul>
</li>
<li>
<p><strong>spark-sql 执行流程分析</strong></p>
<ul>
<li>spark-sql 底层调用的也是 spark-submit</li>
<li>因为 spark-sql 它就是一个 Spark 应用程序，和 spark-shell 一样</li>
<li>对于你想启动一个 Spark 应用程序，肯定要借助于 spark-submit 这脚本进行提交</li>
<li>spark-sql 调用的类是<code>org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver</code></li>
<li>spark-shell 调用的类是 <code>org.apache.spark.repl.Main</code></li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SparkSQL实战笔记（1）---- 部署、项目准备]]></title>
        <id>https://zu3zz.coding.me/post/sparksql-1/</id>
        <link href="https://zu3zz.coding.me/post/sparksql-1/">
        </link>
        <updated>2020-01-13T08:43:13.000Z</updated>
        <content type="html"><![CDATA[<h1 id="sparksql实战笔记1-部署-项目准备">SparkSQL实战笔记（1）---- 部署、项目准备</h1>
<h2 id="1-关于mapreduce的问题">1. 关于MapReduce的问题</h2>
<ul>
<li>
<p>MapReduce的槽点一</p>
<ul>
<li>
<p>需求：统计单词出现的个数（词频统计）</p>
<ul>
<li>
<p>file中每个单词出现的次数</p>
<pre><code class="language-txt">hello,hello,hello
world,world
pk
</code></pre>
</li>
</ul>
</li>
</ul>
<ol>
<li>读取file中每一行的数据</li>
<li>按照分隔符把每一行的内容进行拆分</li>
<li>按照相同的key分发到同一个任务上去进行累加的操作</li>
</ol>
<ul>
<li>
<p>这是一个简单的不能再简单的一个需求，我们需要开发很多的代码</p>
<ol>
<li>自定义Mapper</li>
<li>自定义Reducer</li>
<li>通过Driver把Mapper和Reducer串起来</li>
<li>打包，上传到集群上去</li>
<li>在集群上提交我们的wc程序</li>
</ol>
</li>
<li>
<p>一句话：就是会花费非常多的时间在非业务逻辑改动的工作上</p>
</li>
</ul>
</li>
<li>
<p>MapReduce吐槽点二</p>
<pre><code class="language-shell">Input =&gt; MapReduce ==&gt; Output ==&gt; MapReduce ==&gt; Output
</code></pre>
</li>
<li>
<p>回顾下MapReduce执行流程：</p>
<ul>
<li>MapTask或者ReduceTask都是进程级别</li>
<li>第一个MR的输出要先落地，然后第二个MR把第一个MR的输出当做输入</li>
<li>中间过程的数据是要落地</li>
</ul>
</li>
</ul>
<h2 id="2-spark">2. Spark</h2>
<ol>
<li>
<p>特性</p>
<ol>
<li>
<p>Speed:</p>
<ul>
<li>
<p>both batch and streaming data</p>
</li>
<li>
<p>批流一体 Spark Flink</p>
</li>
</ul>
</li>
<li>
<p>Ease of Use</p>
<ul>
<li>high-level operators</li>
</ul>
</li>
<li>
<p>Generality</p>
<ul>
<li>stack  栈   生态</li>
</ul>
</li>
<li>
<p>Runs Everywhere</p>
<ul>
<li>It can access diverse data sources</li>
<li>YARN/Local/Standalone Spark应用程序的代码需要改动吗？</li>
<li>--master来指定你的Spark应用程序将要运行在什么模式下</li>
</ul>
</li>
</ol>
</li>
</ol>
<h2 id="3-部署问题">3. 部署问题</h2>
<h3 id="31-jdk部署">3.1 <code>JDK</code>部署</h3>
<ul>
<li>
<p>下载：https://www.oracle.com/index.html</p>
</li>
<li>
<p>服务器端：</p>
<ul>
<li>
<p>下载linux版本的jdk</p>
</li>
<li>
<p>解压：<code>tar -zxvf jdk-8u91-linux-x64.tar.gz -C ~/app</code></p>
</li>
<li>
<p>配置环境变量： <code>~/.bash_profile</code></p>
<pre><code class="language-shell">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91
export PATH=$JAVA_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>使环境变量生效：<code>source ~/.bash_profile</code></p>
</li>
</ul>
</li>
<li>
<p>客户端：Win/Mac/Linux</p>
<ul>
<li>Mac/Linux：就和服务器端安装方法一致</li>
</ul>
</li>
</ul>
<h3 id="32-maven和idea部署">3.2 <code>Maven</code>和<code>IDEA</code>部署</h3>
<ol>
<li>
<p>Maven：IDEA+Maven来管理应用程序</p>
<ul>
<li>为什么你开发的时候不直接拷贝jar包呢？</li>
<li>在maven中的pom.xml中添加我们所需要的dependency就行</li>
</ul>
</li>
<li>
<p>官网：maven.apache.org</p>
<ul>
<li>
<p><code>wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz</code></p>
</li>
<li>
<p>解压：<code>tar -zxvf apache-maven-3.6.1-bin.tar.gz -C ~/app/</code></p>
</li>
<li>
<p>配置环境变量：<code>~/.bash_profile</code></p>
<pre><code class="language-shell">export MAVEN_HOME=/home/hadoop/app/apache-maven-3.6.1
export PATH=$MAVEN_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>使环境变量生效：<code>source ~/.bash_profile</code></p>
</li>
<li>
<p>服务器端：你是需要进行使用maven来编译我们的spark</p>
</li>
<li>
<p>客户端：Win/Mac/Linux</p>
</li>
<li>
<p>我们开发应用程序是在本地/本机，IDEA+Maven，所以本地也是需要安装maven的</p>
</li>
<li>
<p>本地Win/Mac/Linux的maven安装方式和服务器端是一模一样的</p>
</li>
<li>
<p>如果你是win用户，一定要注意: $MAVEN_HOME/conf/setting.xml</p>
<pre><code class="language-xml">&lt;!-- localRepository
	   | The path to the local repository maven will use to store artifacts.
	   |
	   | Default: ${user.home}/.m2/repository
	  &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt;
	  --&gt;
</code></pre>
</li>
<li>
<p>Win用户，默认是在C盘，所以建议大家更改Maven本地仓库的路径</p>
</li>
</ul>
</li>
<li>
<p>IDEA官网：http://www.jetbrains.com/</p>
</li>
</ol>
<h3 id="33-hadoop部署">3.3 <code>Hadoop</code>部署</h3>
<h4 id="331-使用cdh-cdh5151">3.3.1 使用<code>CDH</code> <code>cdh5.15.1</code></h4>
<ul>
<li>
<p>下载地址：https://archive.cloudera.com/cdh5/cdh/5/</p>
</li>
<li>
<p>Hadoop：<code>wget https://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</code></p>
</li>
<li>
<p>解压：<code>tar -zxvf hadoop-2.6.0-cdh5.15.1.tar.gz -C ~/app/</code></p>
</li>
<li>
<p>修改<code>hadoop-env.sh</code></p>
<pre><code class="language-shell">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91
</code></pre>
</li>
<li>
<p>修改core-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;fs.default.name&lt;/name&gt;
	&lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改hdfs-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
	&lt;value&gt;/home/hadoop/tmp/dfs/data&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
	&lt;name&gt;dfs.replication&lt;/name&gt;
	&lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
	&lt;name&gt;dfs.permissions&lt;/name&gt;
	&lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改yarn-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改mapred-site.xml</p>
<pre><code class="language-xml">&lt;property&gt;
	&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
	&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
</li>
<li>
<p>修改slaves（可选）</p>
<ul>
<li>hadoop000</li>
</ul>
</li>
<li>
<p>配置系统环境变量</p>
<pre><code class="language-shell">export HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.15.1
export PATH=$HADOOP_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>配置SSH的免密码登录</p>
</li>
<li>
<p>在启动HDFS之前，一定要先对HDFS对格式化</p>
<ul>
<li>切记：格式化只会一次，因为一旦格式化了，那么HDFS上的数据就没了</li>
<li>格式化命令：<code>hdfs namenode -format</code></li>
</ul>
</li>
<li>
<p>启动HDFS</p>
<ol>
<li>
<p>逐个进程启动/停止</p>
<pre><code class="language-shell">$ hadoop-daemon.sh start/stop namenode
$ hadoop-daemon.sh start/stop datanode
</code></pre>
<ul>
<li>jps验证</li>
<li>如果发现有缺失的进程，那么就找缺失进程的名称对应的日志(log而不是out)</li>
</ul>
</li>
<li>
<p>一键式启动HDFS</p>
<pre><code class="language-shell">$ start-dfs.sh
$ stop-dfs.sh
</code></pre>
</li>
</ol>
</li>
</ul>
<h3 id="34-hive部署">3.4 Hive部署</h3>
<ol>
<li>
<p>Hadoop：wget https://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.15.1.tar.gz</p>
</li>
<li>
<p>系统环境变量</p>
<pre><code class="language-shell">export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.15.1
export PATH=$HIVE_HOME/bin:$PATH
</code></pre>
</li>
<li>
<p>需要安装<code>MySQL</code> 与<code>yum</code></p>
<ul>
<li>
<p>需要拷贝MySQL的驱动$HIVE_HOME/lib  版本5.x</p>
</li>
<li>
<p>修改<code>$HIVE_HOME/conf/hive-site.xml</code>文件</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://localhost:3306/pk?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;

&lt;/configuration&gt;
</code></pre>
</li>
</ul>
</li>
<li>
<p>Hive: HDFS上的数据 + MySQL中元数据信息</p>
</li>
</ol>
<h2 id="4-spark运行模式">4. Spark运行模式</h2>
<ol>
<li>local：本地运行，在开发代码的时候，我们使用该模式进行<strong>测试</strong>是非常方便的</li>
<li>standalone：Hadoop部署多个节点的，同理Spark可以部署多个节点  <strong>用的不多</strong></li>
<li>YARN：将Spark作业提交到Hadoop(YARN)集群中运行，Spark仅仅只是一个客户端而已 <strong>最多的用法</strong></li>
<li>Mesos：不常用</li>
<li>K8S：2.3版本才正式稍微稳定   是未来比较好的一个方向</li>
<li>补充：运行模式和代码没有任何关系，同一份代码可以不做修改运行在不同的运行模式下</li>
</ol>
<h2 id="5-构建应用">5. 构建应用</h2>
<ol>
<li>
<p>使用<code>IDEA</code>+<code>Maven</code>来构建我们的Spark应用</p>
</li>
<li>
<p>在命令行中运行一下<code>MAVEN</code>命令</p>
<pre><code class="language-shell">mvn archetype:generate -DarchetypeGroupId=net.alchim31.maven \
-DarchetypeArtifactId=scala-archetype-simple \
-DremoteRepositories=http://scala-tools.org/repo-releases \
-DarchetypeVersion=1.5 \
-DgroupId=com.imooc.bigdata \
-DartifactId=sparksql-train \
-Dversion=1.0
</code></pre>
</li>
<li>
<p>打开IDEA，把这个项目中的pom.xml打开即可</p>
</li>
<li>
<p>同时，在<code>pom.xml</code>中添加一下相关配置</p>
<pre><code class="language-xml">pom.xml
&lt;properties&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
    &lt;scala.tools.version&gt;2.11&lt;/scala.tools.version&gt;
    &lt;scala.version&gt;2.11.8&lt;/scala.version&gt;
    &lt;spark.version&gt;2.4.3&lt;/spark.version&gt;
    &lt;hadoop.version&gt;2.6.0-cdh5.15.1&lt;/hadoop.version&gt;
&lt;/properties&gt;	

添加CDH的仓库
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;cloudera&lt;/id&gt;
        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;

添加Spark SQL和Hadoop Client的依赖
&lt;!--Spark SQL依赖--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;${spark.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- Hadoop相关依赖--&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;
    &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;
    &lt;version&gt;${hadoop.version}&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
</li>
</ol>
<h2 id="6-实战词频统计案例">6. 实战：词频统计案例</h2>
<ol>
<li>
<p>输入：文件</p>
<ul>
<li>需求：统计出文件中每个单词出现的次数
<ol>
<li>读每一行数据</li>
<li>按照分隔符把每一行的数据拆成单词</li>
<li>每个单词赋上次数为1</li>
<li>按照单词进行分发，然后统计单词出现的次数</li>
<li>把结果输出到文件中</li>
</ol>
</li>
</ul>
</li>
<li>
<p>输出：文件</p>
</li>
<li>
<p>使用local模式运行spark-shell</p>
<pre><code class="language-shell">./spark-shell --master local
</code></pre>
<ul>
<li>
<p>打包我们的应用程序，让其运行在local模式下</p>
</li>
<li>
<p>如何运行jar包呢？</p>
</li>
</ul>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master local \
/home/hadoop/lib/sparksql-train-1.0.jar \
file:///home/hadoop/data/wc.data file:///home/hadoop/data/out 
</code></pre>
<ul>
<li>使用local模式的话，你只需要把spark的安装包解压开，什么都不用动，就能使用</li>
</ul>
</li>
<li>
<p>如何提交Spark应用程序到YARN上执行</p>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master yarn \
--name SparkWordCountAppV2 \
/home/hadoop/lib/sparksql-train-1.0.jar \
hdfs://hadoop000:8020/pk/wc.data hdfs://hadoop000:8020/pk/out
</code></pre>
</li>
<li>
<p>要将Spark应用程序运行在YARN上，一定要配置<code>HADOOP_CONF_DIR</code>或者<code>YARN_CONF_DIR</code></p>
<p>指向<code>$HADOOP_HOME/etc/conf</code></p>
</li>
<li>
<p>local和YARN模式：重点掌握</p>
</li>
<li>
<p>Standalone：了解</p>
<ul>
<li>
<p>多个机器，那么你每个机器都需要部署spark</p>
</li>
<li>
<p>相关配置：</p>
<pre><code class="language-shell">$SPARK_HOME/conf/slaves
	hadoop000
SPARK_HOME/conf/spark-env.sh
	SPARK_MASTER_HOST=hadoop000
</code></pre>
</li>
<li>
<p>启动Spark集群</p>
<pre><code class="language-shell">$SPARK_HOME/sbin/start-all.sh
jps： Master  Worker
</code></pre>
</li>
<li>
<p>spark提交作业</p>
<pre><code class="language-shell">./spark-submit \
--class  com.imooc.bigdata.chapter02.SparkWordCountAppV2 \
--master spark://hadoop000:7077 \
--name SparkWordCountAppV2 \
/home/hadoop/lib/sparksql-train-1.0.jar \
hdfs://hadoop000:8020/pk/wc.data hdfs://hadoop000:8020/pk/out2
</code></pre>
</li>
<li>
<p>不管什么运行模式，代码不用改变，只需要在<code>spark-submit</code>脚本提交时</p>
<p>通过<code>--master xxx</code> 来设置你的运行模式即可</p>
</li>
</ul>
</li>
</ol>
<h2 id="7-实战代码">7. 实战代码</h2>
<ul>
<li><code>Scala</code>版本</li>
</ul>
<pre><code class="language-scala">package com.zth.bigdata.examples
import org.apache.spark.{SparkConf, SparkContext}
/**
 * Author: 3zZ.
 * Date: 2020/1/13 3:14 下午
 */
object SparkWordCountApp {
  def main(args: Array[String]): Unit = {
    /**
     * master: 运行模式 local
     */
    val sparkConf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;SparkWordCountApp&quot;)
    val sc = new SparkContext(sparkConf)

    val rdd = sc.textFile(&quot;/Users/3zz/Code/Spark/spark-sql-train/data/input.txt&quot;)
    /**
     * 按照单词个数进行降序排列
     */
    rdd.flatMap(_.split(&quot;,&quot;)).map((_, 1))
      .reduceByKey(_ + _).map(x =&gt; (x._2, x._1))
      .sortByKey(false).map(x =&gt;(x._2,x._1))
      .collect().foreach(println)
    //      .saveAsTextFile(&quot;/Users/3zz/Code/Spark/spark-sql-train/data/out&quot;).
    sc.stop()
  }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HBase学习笔记 ---- 整合篇]]></title>
        <id>https://zu3zz.coding.me/post/hbase-1/</id>
        <link href="https://zu3zz.coding.me/post/hbase-1/">
        </link>
        <updated>2020-01-11T15:54:39.000Z</updated>
        <content type="html"><![CDATA[<h1 id="hbase">Hbase</h1>
<h2 id="1-数据存储">1. 数据存储</h2>
<h3 id="11-rdbms">1.1 RDBMS:</h3>
<ol>
<li>
<p>Data is typed structured before stored</p>
</li>
<li>
<p>传统SQL</p>
<table>
<thead>
<tr>
<th>data</th>
<th>location</th>
</tr>
</thead>
<tbody>
<tr>
<td>entity</td>
<td>table</td>
</tr>
<tr>
<td>record</td>
<td>row</td>
</tr>
<tr>
<td>query</td>
<td>group by 、join</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>大数据时代，如何做实时查询？</p>
</li>
<li>
<p>hadoop/HDFS：能存 没法进行实时查询，随机读写</p>
</li>
<li>
<p>NoSQL（NOT only SQL）：HBase、Redis</p>
</li>
</ol>
<h3 id="12-hbase在hadoop生态圈中的位置">1.2 Hbase在Hadoop生态圈中的位置</h3>
<ol>
<li>HBase是Hadoop生态圈中的一个重要组成部分</li>
<li>HBase是构建在HDFS纸上，也就是说HBase的数据可以存储在HDFS上面</li>
<li>可以通过MapReduce/Spark来处理Hbase中的数据</li>
<li>HBase也提供了shell、API的方式进行数据的访问</li>
</ol>
<h3 id="13-行式vs列式">1.3 行式VS列式</h3>
<ol>
<li>
<p>行式</p>
<ul>
<li>
<p>按行存储</p>
</li>
<li>
<p>没有索引查询的时候需要耗费大量的IO</p>
</li>
<li>
<p>可以通过建立索引或者视图来提速</p>
</li>
<li>
<p>1,3zz,23</p>
</li>
</ul>
</li>
<li>
<p>列式</p>
<ul>
<li>压缩、并行处理</li>
<li>数据就是索引，大大降低IO</li>
</ul>
</li>
</ol>
<h3 id="14-hbase特点">1.4 HBase特点</h3>
<ol>
<li>
<p>大：数据量大</p>
</li>
<li>
<p>面向列：列族（可以存放很多列），列族/列独立索引</p>
</li>
<li>
<p>稀疏：</p>
<table>
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>age</th>
<th>...</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3zz</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>4xx</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>5yy</td>
<td>0</td>
<td></td>
</tr>
</tbody>
</table>
</li>
<li>
<p>数据类型单一：byte/string</p>
</li>
<li>
<p>无模式：每一行的数据所对应的列不一定相同，每行的列是可以动态添加的</p>
<pre><code class="language-shell">3zz age/birthday/company
4xx company/province/city
</code></pre>
</li>
<li>
<p>数据多版本：比如company可以存放不同的版本的值</p>
<p>默认情况下版本号是自动分配的，是列的值插入时的时间戳</p>
</li>
</ol>
<h3 id="15-hbase-vs-mysql">1.5 HBase VS MySQL</h3>
<ol>
<li>
<p>数据类型不同</p>
</li>
<li>
<p>数据操作：</p>
<ol>
<li>关联查询：MapReduce/Spark/Phoenix</li>
<li>get/put/scan...</li>
</ol>
</li>
<li>
<p>存储模式</p>
<ol>
<li>
<p>MySQL</p>
<table>
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>age</th>
<th>tel</th>
<th>Address</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>a</td>
<td>11</td>
<td>123</td>
<td>...</td>
</tr>
<tr>
<td>2</td>
<td>b</td>
<td>22</td>
<td>456</td>
<td>...</td>
</tr>
<tr>
<td>3</td>
<td>c</td>
<td>33</td>
<td>789</td>
<td>...</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>在HBase中</p>
<pre><code class="language-shell">basic_info: id	name 
private_info: age	tel		address
</code></pre>
</li>
</ol>
</li>
<li>
<p>transaction事务性：单行</p>
</li>
<li>
<p>数据量：HBase大</p>
</li>
<li>
<p>吞吐量：Million级别</p>
</li>
</ol>
<h3 id="hbase-vs-hdfs">HBase vs HDFS</h3>
<ol>
<li>write pattern（写模式）</li>
<li>read pattern（读模式）</li>
<li>SQL</li>
<li>data size（数据量）</li>
</ol>
<h3 id="hbase优势">HBase优势</h3>
<ol>
<li>成熟</li>
<li>高效</li>
<li>分布式</li>
</ol>
<h3 id="数据模型">数据模型</h3>
<ol>
<li>
<p>rowkey</p>
<ul>
<li>
<p>主键</p>
</li>
<li>
<p>字符串，按字典顺序存储，在HBase内部保存的是字节数组</p>
</li>
</ul>
</li>
<li>
<p>列族：Column Family （CF）</p>
<ul>
<li>
<p>是在创建表的时候就要指定的</p>
</li>
<li>
<p>列族是一系列列的集合</p>
</li>
<li>
<p>一个列族所有列有着相同的前缀</p>
<pre><code class="language-shell">basic_info: id
basic_info: name
private_info: age
private_info: tel
private_info: address
</code></pre>
</li>
</ul>
</li>
<li>
<p>列：Column / Qualifier</p>
<ul>
<li>属于某一个列族</li>
</ul>
</li>
<li>
<p>每条记录被划分到若干个CF中，每条记录对应一个rowkey，每个CF由一个或者多个Column构成</p>
</li>
<li>
<p>存储单元：Cell</p>
<ul>
<li>HBase中ROW和Column确定的一个存储单元</li>
<li>每个Cell都保存这同一份数据的多个版本</li>
<li>在写入数据时，时间戳可以由HBase自动赋值，也可以显示赋值</li>
<li>每个Cell中，不同版本的数据按照时间戳的倒序排列</li>
</ul>
<pre><code class="language-shell">{rowkey, column, version} ==&gt; HBase 中的一个Cell
</code></pre>
</li>
</ol>
<h2 id="2-hbase安装">2. HBase安装</h2>
<h3 id="21-前置需要">2.1 前置需要</h3>
<ol>
<li>JDK（略过）</li>
<li>ZooKeeper安装（brew）</li>
<li>Hadoop安装（使用cdh版本，详细配置参照前面博客）</li>
</ol>
<h3 id="22hbase安装">2.2HBase安装：</h3>
<ol>
<li>
<p>在.bash_profile中添加HBASE_HOME</p>
</li>
<li>
<p>在HBase的<code>conf</code>目录中修改两个文件</p>
<ol>
<li>
<p>在<code>hbase-env.sh</code>文件中</p>
<pre><code class="language-shell"># 1.修改java的路径
# The java implementation to use.  Java 1.7+ required.
# export JAVA_HOME=/usr/java/jdk1.6.0/
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-12.0.1.jdk/Contents/Home
# 注意 这里一定要是用1.8的版本！ 上面使用java12是要出大问题的


# 2.修改zookeeper配置
# Tell HBase whether it should manage it's own instance of Zookeeper or not.
export HBASE_MANAGES_ZK=false
</code></pre>
</li>
<li>
<p>在<code>hbase-site.xml</code>中配置</p>
<pre><code class="language-xml">&lt;configuration&gt;
&lt;!--hbase的数据存放地址（本机）--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.rootdir&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;
&lt;/property&gt;
&lt;!--分布式配置--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;!--本机zookeeper地址--&gt;
&lt;property&gt;
  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
  &lt;value&gt;localhost:2181&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
</li>
</ol>
</li>
</ol>
<h3 id="23-启动hbase">2.3 启动HBase</h3>
<ol>
<li>先启动hadoop目录下<code>sbin/start-dfs.sh</code></li>
<li>启动zookeeper<code>zKserver start</code></li>
<li>启动HBase目录下的<code>bin/start-hbase.sh</code></li>
<li>打开本机60010端口，有界面 则成功</li>
</ol>
<h3 id="24-hbase-shell使用">2.4 HBase shell使用</h3>
<ol>
<li>在<code>bin</code>目录下输入<code>hbase shell</code>即可进入shell脚本
<ol>
<li>查看版本<code>version</code></li>
<li>查看服务器的状态<code>status</code></li>
</ol>
</li>
</ol>
<h2 id="3-hbase相关操作">3. HBase相关操作</h2>
<h3 id="31-ddl操作">3.1 DDL操作</h3>
<ul>
<li>创建、查询</li>
</ul>
<pre><code class="language-sql"># 创建
create 'member','member_id','address','info'
# 查询详细信息
desc 'member'
</code></pre>
<ul>
<li>返回信息如下：</li>
</ul>
<pre><code class="language-shell">{NAME =&gt; 'address', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCOD
ING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICA
TION_SCOPE =&gt; '0'}                                                                                                              
{NAME =&gt; 'info', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING
 =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATIO
N_SCOPE =&gt; '0'}                                                                                                                 
{NAME =&gt; 'member_id', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENC
ODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLI
CATION_SCOPE =&gt; '0'} 
</code></pre>
<ul>
<li>也可以在图形化界面，即60010端口上查看,点击最上面的<code>Table Details</code></li>
</ul>
<pre><code class="language-shell"># 查看有哪些表
hbase(main):007:0&gt; list
=&gt; [&quot;member&quot;]
# 删除列族中的一列
hbase(main):008:0&gt; alter 'member' ,'delete'=&gt;'member_id'
1/1 regions updated.
Done.
</code></pre>
<ul>
<li>此时在查看<code>member</code>表的结构</li>
</ul>
<pre><code class="language-shell">hbase(main):009:0&gt; desc 'member'

COLUMN FAMILIES DESCRIPTION                                                                                                     
{NAME =&gt; 'address', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCOD
ING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICA
TION_SCOPE =&gt; '0'}                                                                                                              
{NAME =&gt; 'info', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING
 =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATIO
N_SCOPE =&gt; '0'} 
</code></pre>
<ul>
<li>
<p>可以发现<code>member_id</code>已经被我们删掉了</p>
</li>
<li>
<p>删除表</p>
</li>
</ul>
<pre><code class="language-shell"># 先disable
hbase(main):011:0&gt; disable 'member'
# 再drop
hbase(main):012:0&gt; drop 'member'
# 此时再查看所有表
hbase(main):013:0&gt; list
=&gt; []
</code></pre>
<h3 id="32-dml操作">3.2 DML操作</h3>
<ol>
<li>
<p>为了方便，重新创建一张表</p>
<pre><code class="language-shell"># 表名为member，列族为address、info
create 'member','address','info'
</code></pre>
</li>
<li>
<p>创建/修改/删除表</p>
<pre><code class="language-shell"># rowkey为行名（表名） cf为列族 column为列族中的一列
插入数据： put 表名,rowkey,cf:column,key
</code></pre>
</li>
<li>
<p>实际语句如下</p>
<pre><code class="language-shell">put 'member','3z','info:age','23'
put 'member','3z','info:birthday','1996-07-31'
</code></pre>
</li>
<li>
<p>查看member中数据</p>
<pre><code class="language-shell">scan 'member'
# 返回结果
ROW					COLUMN+CELL
3z					column=info:age, timestamp=12341241212, value=23
3z					column=info:birthday, timestamp=12341241213, value=1996-07-31
</code></pre>
</li>
<li>
<p>获取某一行的数据</p>
<pre><code class="language-shell"># 获取所有3z的数据
get 'member','3z'
# 返回结果
COLUMN                            CELL                                                                                          
 info:age                         timestamp=1577955149575, value=23
 info:birthday                    timestamp=1577955128970, value=1996-07-31
</code></pre>
</li>
<li>
<p>修改某一列</p>
<pre><code class="language-shell">put 'member','3z','info:age','18'
# 获取当前年龄
get 'member','3z','info:age'
# 返回结果
COLUMN                            CELL
 info:age                         timestamp=1577955321212, value=18
# age已经被更新
</code></pre>
</li>
<li>
<p>删除某一列</p>
<pre><code class="language-shell">delete 'member','3z','info:birthday'
# 再查看3z的信息
get 'member','3z'
# 返回结果
COLUMN                            CELL
 info:age                         timestamp=1577955321212, value=18
# 删除成功
</code></pre>
</li>
<li>
<p>统计一下几行</p>
<pre><code class="language-shell">count 'member'
# 返回结果
==&gt; 1
</code></pre>
</li>
<li>
<p>删除某个一整行</p>
<pre><code class="language-shell">deleteall 'member','3z'
</code></pre>
</li>
<li>
<p>清空一张表</p>
<pre><code class="language-shell">truncate 'member'
# 会先自己disable 在truncate
Truncating 'member' table (it may take a while):
 - Disabling table...
 - Truncating table...
0 row(s) in 3.4130 seconds
</code></pre>
</li>
</ol>
<h2 id="4-hbase-api开发javascala">4. HBase API开发：Java/Scala</h2>
<ol>
<li>
<p><code>maven:pom.xml</code>：良好的网络支持</p>
<pre><code class="language-java">package com.zth.bigdata.hbase;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;

/**
 * Author: 3zZ.
 * Date: 2020/1/11 6:14 下午
 */
public class HbaseApp {
    Connection connection = null;
    Table table = null;
    Admin admin = null;

    String tableName = &quot;3z_hbase_java_api&quot;;

    @Before
    public void setUp() {
        Configuration configuration = new Configuration();
        configuration.set(&quot;hbase.rootdir&quot;, &quot;hdfs://localhost:8020/hbase&quot;);
        configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;localhost:2181&quot;);
        try {
            connection = ConnectionFactory.createConnection(configuration);
            admin = connection.getAdmin();

            Assert.assertNotNull(connection);
            Assert.assertNotNull(admin);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Test
    public void getConnection() {
    }

    @Test
    public void createTable() throws Exception {
        TableName table = TableName.valueOf(tableName);
        if (admin.tableExists(table)) {
            System.out.println(tableName + &quot;已经存在&quot;);
        } else {
            HTableDescriptor descriptor = new HTableDescriptor(table);
            descriptor.addFamily(new HColumnDescriptor(&quot;info&quot;));
            descriptor.addFamily(new HColumnDescriptor(&quot;address&quot;));
            admin.createTable(descriptor);
            System.out.println(tableName + &quot;创建成功&quot;);
        }
    }

    @Test
    public void queryTableInfos() throws Exception {
        HTableDescriptor[] tables = admin.listTables();
        if (tables.length &gt; 0) {
            for (HTableDescriptor table : tables) {
                System.out.println(table.getNameAsString());
                HColumnDescriptor[] columnDescriptors = table.getColumnFamilies();
                for (HColumnDescriptor hColumnDescriptor : columnDescriptors) {
                    System.out.println(&quot;\t&quot; + hColumnDescriptor.getNameAsString());
                }
            }
        }
    }

    @Test
    public void testPut() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(Bytes.toBytes(&quot;3z&quot;));
        // 通过PUT设置要添加数据的CF、qualifier、value
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;24&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;birthday&quot;), Bytes.toBytes(&quot;731&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;company&quot;), Bytes.toBytes(&quot;HIT&quot;));

        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;country&quot;), Bytes.toBytes(&quot;CN&quot;));
        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;province&quot;), Bytes.toBytes(&quot;BJ&quot;));
        put.addColumn(Bytes.toBytes(&quot;address&quot;), Bytes.toBytes(&quot;city&quot;), Bytes.toBytes(&quot;BJ&quot;));
        //  将数据put到hbase中去
        table.put(put);
    }

    @Test
    public void testUpdate() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(Bytes.toBytes(&quot;2z&quot;));
        put.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(&quot;25&quot;));
        table.put(put);
    }

    @Test
    public void testGet01() throws Exception {
        table = connection.getTable(TableName.valueOf(tableName));
        Get get = new Get(&quot;3z&quot;.getBytes());
        get.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;));
        Result result = table.get(get);
        printResult(result);
    }
    @Test
    public void testScan01() throws Exception{
        table = connection.getTable(TableName.valueOf(tableName));
        Scan scan = new Scan();
        ResultScanner rs = table.getScanner(scan);
        for (Result result: rs){
            printResult(result);
        }
    }

    public void printResult(Result result) {
        for (Cell cell : result.rawCells()) {
            System.out.println(Bytes.toString(result.getRow()) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneFamily(cell)) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneQualifier(cell)) + &quot;\t&quot; +
                    Bytes.toString(CellUtil.cloneValue(cell)) + &quot;\t&quot; +
                    cell.getTimestamp()
            );
        }
    }

    @After
    public void tearDown() {
        try {
            connection.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
</code></pre>
</li>
<li>
<p>小结</p>
<ul>
<li>
<p><code>Connection</code></p>
</li>
<li>
<p><code>Admin</code></p>
</li>
<li>
<p><code>HTableDescriptor</code></p>
</li>
<li>
<p><code>HcolumnDescriptor</code></p>
</li>
<li>
<p>创建表</p>
</li>
<li>
<p>删除表</p>
</li>
<li>
<p>添加记录：单挑、多条</p>
</li>
<li>
<p>修改记录</p>
</li>
<li>
<p>根据<code>RowKey</code>获取单挑记录</p>
</li>
<li>
<p><code>Scan</code>：空、起始、起始结尾、<code>Get</code></p>
</li>
<li>
<p><code>Filter</code> : <code>RowFilter</code>、<code>PrefixFilter</code>、<code>FilterList</code></p>
</li>
</ul>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scala学习笔记（7）---- Scala隐式转换]]></title>
        <id>https://zu3zz.coding.me/post/scala-7/</id>
        <link href="https://zu3zz.coding.me/post/scala-7/">
        </link>
        <updated>2020-01-10T16:34:35.000Z</updated>
        <content type="html"><![CDATA[<h1 id="scala隐式转换">Scala隐式转换</h1>
<h2 id="1-隐式转换概念">1. 隐式转换概念</h2>
<ul>
<li>隐式转换就是将一个类赋予另外一个类中的属性和能力</li>
</ul>
<h2 id="2-例子">2. 例子</h2>
<ul>
<li>将所有隐式转换的方法单独放到一个文件中</li>
</ul>
<pre><code class="language-scala">// ImplicitApp.scala
import java.io.File
/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:15 下午
 */
object ImplicitAspect {
  // 定义隐式转换函数即可
  // 案例1 将只有eat方法的普通人变成有fly方法的超人
  implicit def man2superman(man:Man): Superman = new Superman(man.name)
  // 案例2 为File对象添加直接读的方法
  implicit def file2Richfile(file: File): Richfile = new Richfile(file)
}
</code></pre>
<ul>
<li>在需要使用的文件进行引入</li>
</ul>
<pre><code class="language-scala">import java.io.File
import ImplicitAspect._
/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:00 下午
 */
object ImplicitApp extends App {
  // 定义隐式转换函数即可
  // 案例1
  val man = new Man(&quot;3z&quot;)
  man.fly() // 能够成功飞行
  // 案例2 为File对象添加直接读的方法
  val file = new File(&quot;/Users/3zz/Desktop/test.txt&quot;)
  file.read() // 能够正常读出文件
}
class Man(val name: String) {
  def eat(): Unit = {
    println(s&quot;man $name is eating&quot;)
  }
}
class Superman(val name: String) {
  def fly(): Unit = {
    println(s&quot;superman $name is flying&quot;)
  }
}
class Richfile(val file:File){
  def read() ={
    scala.io.Source.fromFile(file.getPath).mkString
  }
}
</code></pre>
<h2 id="3-隐式参数例子">3. 隐式参数例子</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:00 下午
 */
object ImplicitApp extends App {
  implicit val test = &quot;test&quot;
  def testParam(implicit name:String ): Unit ={
    println(name)
  }
//  testParam 什么都不填会报错 (如果在上面定义了test 就不会报错)
//  testParam(&quot;123&quot;) 正常输出 123
  implicit val name1: String = &quot;implicit_name&quot;
  testParam // 此时有了implicit 就不会报错 正常输出 implicit_name
  testParam(&quot;3z&quot;) // 输出 3z
  implicit val s1 = &quot;s1&quot;
  implicit val s2 = &quot;s3&quot;
  testParam // 此时会报错 因为有两个不确定
}
</code></pre>
<h2 id="4-隐式类例子">4. 隐式类例子</h2>
<pre><code class="language-scala">/**
 * Author: 3zZ.
 * Date: 2020/1/10 11:32 下午
 */
object ImplicitClassApp extends App {
  implicit class Cal(x:Int){
    def add(a:Int) = a + x
  }
  // 1本身是没有add方法的
  // 上面的隐式类为所有的Int类型添加了add方法
  println(1.add(3)) // 4
}
</code></pre>
]]></content>
    </entry>
</feed>