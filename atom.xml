<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://Zu3zz.github.io</id>
    <title>第三新东京</title>
    <updated>2019-10-18T11:00:27.847Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://Zu3zz.github.io"/>
    <link rel="self" href="https://Zu3zz.github.io/atom.xml"/>
    <subtitle>Everyone Can (Not) Comprehend.</subtitle>
    <logo>https://Zu3zz.github.io/images/avatar.png</logo>
    <icon>https://Zu3zz.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, 第三新东京</rights>
    <entry>
        <title type="html"><![CDATA[Python与常用消息中间件(1)----概念篇]]></title>
        <id>https://Zu3zz.github.io/post/message-queue-python</id>
        <link href="https://Zu3zz.github.io/post/message-queue-python">
        </link>
        <updated>2019-10-18T10:51:28.000Z</updated>
        <summary type="html"><![CDATA[<p>🌈最近在学习消息中间件，由于大数据端天生靠近数据，所以在很多场景下，会需要对日志等文件进行分析<br>
🎊那么这一系列，我们使用Python语言对常用的消息中间件进行一次整合<br>
🎉看看如何将nginx、kafka、zookeeper、redis、logstash一起，使用python语言搭建一个日志报警系统吧</p>
]]></summary>
        <content type="html"><![CDATA[<p>🌈最近在学习消息中间件，由于大数据端天生靠近数据，所以在很多场景下，会需要对日志等文件进行分析<br>
🎊那么这一系列，我们使用Python语言对常用的消息中间件进行一次整合<br>
🎉看看如何将nginx、kafka、zookeeper、redis、logstash一起，使用python语言搭建一个日志报警系统吧</p>
<!-- more -->
<h2 id="1-常用消息中间件比较">1. 常用消息中间件比较</h2>
<h3 id="11-常用消息中间件介绍">1.1 常用消息中间件介绍</h3>
<ul>
<li>Redis</li>
<li>RabbitMQ</li>
<li>RocketMQ</li>
<li>ZeroMQ</li>
<li>Kafka</li>
</ul>
<h3 id="12-常用消息中间件对比">1.2 常用消息中间件对比</h3>
<ul>
<li>实现语言</li>
<li>对外接口</li>
<li>持久化策略</li>
<li>消息处理模式</li>
<li>时序保证</li>
</ul>
<table>
<thead>
<tr>
<th>消息中间件</th>
<th>实现语言</th>
<th>持久化</th>
<th>消息处理模式</th>
<th>时序保证</th>
</tr>
</thead>
<tbody>
<tr>
<td>Redis</td>
<td>C</td>
<td>支持磁盘</td>
<td>Push-Pull Pub-Sub</td>
<td>有序</td>
</tr>
<tr>
<td>RabbitMQ</td>
<td>Erlang</td>
<td>磁盘</td>
<td>Push</td>
<td>单消费者有序</td>
</tr>
<tr>
<td>RocketMQ</td>
<td>Java</td>
<td>磁盘</td>
<td>Puch-Pull</td>
<td>同队列有序</td>
</tr>
<tr>
<td>Kafka</td>
<td>Scala</td>
<td>磁盘</td>
<td>Pull</td>
<td>单Paritition有序</td>
</tr>
</tbody>
</table>
<h2 id="2-消息中间件常见概念">2. 消息中间件常见概念</h2>
<h3 id="21-消息中间件中的各种概念和角色">2.1 消息中间件中的各种概念和角色</h3>
<ul>
<li>Producer/borker/cunsumer</li>
<li>Queue/channel/topic</li>
<li>Partition</li>
<li>Publish/subscribe</li>
<li>Acknowledge</li>
</ul>
<h3 id="22-生产者">2.2 生产者</h3>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io/post-images/1571396146907.png" alt="producer"></figure>
<h3 id="23-partition">2.3 Partition</h3>
<ul>
<li>一个topic可以分为多个partition</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://Zu3zz.github.io/post-images/1571396164758.png" alt=""></figure>
<h3 id="24-订阅">2.4 订阅</h3>
<figure data-type="image" tabindex="3"><img src="https://Zu3zz.github.io/post-images/1571396178821.png" alt=""></figure>
<h3 id="25-消息确认机制">2.5 消息确认机制</h3>
<ul>
<li>只有经过用户确认的消息才会从queue中去除</li>
</ul>
<figure data-type="image" tabindex="4"><img src="https://Zu3zz.github.io/post-images/1571396204405.png" alt=""></figure>
<h2 id="3-redis">3. Redis</h2>
<h3 id="31-redis简介">3.1 Redis简介</h3>
<p><strong>Redis关键词</strong></p>
<ul>
<li>Key-value</li>
<li>高性能</li>
<li>缓存</li>
<li>C开发</li>
<li>五大数据结构</li>
<li>lua扩展</li>
</ul>
<h3 id="32-redis常见应用场景">3.2 Redis常见应用场景</h3>
<ol>
<li>
<p>String</p>
<ul>
<li>缓存二进制对象，比如图片、序列化对象等</li>
<li>计数器，比如文章访问量统计</li>
<li>位运算，节约内存</li>
</ul>
</li>
<li>
<p>List</p>
<ul>
<li>获取最新的N条数据</li>
<li>消息队列</li>
<li>实时分析系统，比如服务器监控程序</li>
</ul>
</li>
<li>
<p>Hash</p>
<ul>
<li>
<p>类似Python中的Dict</p>
</li>
<li>
<p>存储具有多个属性的对象</p>
</li>
<li>
<p>比如用户的年龄、姓名、性别、积分</p>
</li>
</ul>
</li>
<li>
<p>Set</p>
<ul>
<li>集合操作，比如通过交集实现共同关注，共同好友</li>
<li>存储无序不重复数据，比如存储文章标签</li>
</ul>
</li>
<li>
<p>Sorted Set</p>
<ul>
<li>TopN排序，比如排行榜</li>
<li>范围查找，比如判断ip地址所在地</li>
<li>优先级队列</li>
<li>过期项目处理</li>
</ul>
</li>
<li>
<p>Pub Sub</p>
<ul>
<li>实时消息系统</li>
<li>比如即时聊天，群聊</li>
</ul>
</li>
</ol>
<h3 id="33-redis安装与默认配置">3.3 Redis安装与默认配置</h3>
<h4 id="331redis服务器默认配置">3.3.1Redis服务器默认配置</h4>
<ul>
<li>端口号：port=6379</li>
<li>IP地址 bind=0.0.0.0</li>
<li>数据库存放位置 dir=./</li>
<li>数据库名字 dbfilename=dump.rdb</li>
<li>守护进程模式 daemonize=no</li>
</ul>
<h4 id="332-redis服务器配置文件">3.3.2 Redis服务器配置文件</h4>
<ul>
<li>
<p>Redis服务器启用配置文件：运行</p>
<pre><code class="language-shell">redis-server name.conf
</code></pre>
</li>
<li>
<p>查看redis服务器所有选项配置：运行</p>
<pre><code class="language-shell">redis-cli config get '*'
</code></pre>
</li>
<li>
<p>查看redis服务器某个配置选项：运行</p>
<pre><code class="language-shell">redis-cli config get xxx(e.g: bind)
</code></pre>
</li>
</ul>
<h4 id="333-常见参数">3.3.3 常见参数</h4>
<ol>
<li>--daemonize
<ul>
<li>含义：是否以守护进程的形式启动(后台启动)</li>
<li>用法：daemonize yes|no</li>
<li>默认值：no</li>
<li>实例：daemonize yes</li>
</ul>
</li>
<li>--bind
<ul>
<li>含义：redis监听的ip地址</li>
<li>用法：bind ip地址</li>
<li>默认值：127.0.0.1</li>
<li>实例：bind 0.0.0.0(监听所有)</li>
</ul>
</li>
<li>--port(一般不改)
<ul>
<li>含义：redis监听的端口号</li>
<li>用法：port 端口号</li>
<li>默认值：6379</li>
<li>实例：port 6380</li>
</ul>
</li>
<li>--dir
<ul>
<li>含义：redis持久化文件存放目录</li>
<li>用法：dir 文件路径</li>
<li>默认值：./</li>
<li>实例：dir /mnt/redis/data/</li>
</ul>
</li>
<li>--dbfilename
<ul>
<li>含义：redis持久化文件文件名</li>
<li>用法：dbfilename 文件名</li>
<li>默认值：dump.rdb</li>
<li>实例：dbfilename xxxx.rdb(e.g. user.rdb)</li>
</ul>
</li>
<li>--unixsocket(效率高于socket套接字)
<ul>
<li>含义：redis监听的unix套接字地址</li>
<li>用法：unixsocket文件地址</li>
<li>默认值：空</li>
<li>实例：unixsocket /tmp/redis.sock</li>
</ul>
</li>
</ol>
<h3 id="34-redis常见操作和命令">3.4 Redis常见操作和命令</h3>
<p><strong>首先使用redis-server启动redis服务，然后使用redis-cli进入命令行界面</strong></p>
<h4 id="341-redis常用命令讲解">3.4.1 Redis常用命令讲解</h4>
<ul>
<li>测试客户端与服务器连接是否正常：PING</li>
<li>获得符合规则的键名列表：KEYS pattern</li>
<li>判断一个键是否存在：EXISTS key</li>
<li>删除一个键：DEL key</li>
<li>获取键的类型：TYPE key</li>
<li>清空当前数据库所有数据：FLUSHDB</li>
<li>设置一个键的生育生存时间：EXPIRE key seconds</li>
<li>返回一个键的生育生存时间：TTL key</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; ping
PONG
127.0.0.1:6379&gt; keys *
 1) &quot;1558695101775_0.7258740928094547&quot;
 2) &quot;1558693978601_0.21519364815403463&quot;
 3) &quot;1558695158141_0.8666190807031027&quot;
127.0.0.1:6379&gt; flushdb
OK
127.0.0.1:6379&gt; keys *
(empty list or set)
127.0.0.1:6379&gt; set foo bar
OK
127.0.0.1:6379&gt; keys foo
1) &quot;foo&quot;
127.0.0.1:6379&gt; type foo
string
127.0.0.1:6379&gt; exists foo
(integer) 1
127.0.0.1:6379&gt; del foo
(integer) 1
127.0.0.1:6379&gt; keys *
(empty list or set)
127.0.0.1:6379&gt; set foo bar
OK
127.0.0.1:6379&gt; expire foo 60
(integer) 1
127.0.0.1:6379&gt; ttl foo
(integer) 58
127.0.0.1:6379&gt; ttl foo
(integer) -2
</code></pre>
<h4 id="342-redis常用命令之string">3.4.2 Redis常用命令之String</h4>
<ul>
<li>SET</li>
<li>GET</li>
<li>INCR(递增一个值为整数的string)</li>
<li>MSET(批量设置)</li>
<li>MGET(批量获取)</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; set foo barstring
OK
127.0.0.1:6379&gt; get foo
&quot;barstring&quot;
127.0.0.1:6379&gt; get ffo
(nil)
127.0.0.1:6379&gt; mset foo1 bar1 foo2 bar2 foo3 bar3
OK
127.0.0.1:6379&gt; mget foo1 foo2 foo3
1) &quot;bar1&quot;
2) &quot;bar2&quot;
3) &quot;bar3&quot;
127.0.0.1:6379&gt; set num 1
OK
127.0.0.1:6379&gt; incr num
(integer) 2
127.0.0.1:6379&gt; get num
&quot;2&quot;
</code></pre>
<h4 id="343-redis常用命令讲解之hash">3.4.3 Redis常用命令讲解之Hash</h4>
<ul>
<li>HSET</li>
<li>HGET</li>
<li>HMSET</li>
<li>HMGET</li>
<li>HGETALL</li>
<li>HDEL</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; hset user name 3zz
(integer) 1
127.0.0.1:6379&gt; hset user sex man
(integer) 1
127.0.0.1:6379&gt; hget user name
&quot;3zz&quot;
127.0.0.1:6379&gt; hgetall user
1) &quot;name&quot;
2) &quot;3zz&quot;
3) &quot;sex&quot;
4) &quot;man&quot;
127.0.0.1:6379&gt; hdel user name
(integer) 1
127.0.0.1:6379&gt; hgetall user
1) &quot;sex&quot;
2) &quot;man&quot;
127.0.0.1:6379&gt; del user
(integer) 1
127.0.0.1:6379&gt; exists user
(integer) 0
</code></pre>
<h4 id="344-redis常用命令讲解之list">3.4.4 Redis常用命令讲解之List</h4>
<ul>
<li>LPUSH(从左侧放入)</li>
<li>RPUSH(从右侧放入)</li>
<li>LPOP(从左侧拿出)</li>
<li>RPOP(从右侧拿出)</li>
<li>BRPOP(有数据则返回，没有就一直等待直到有数据)</li>
<li>LLEN(返回长度)</li>
<li>LRANGE(返回从索引start到end两端的元素，左右都闭)</li>
<li>RPOPLPUSH(先右拿出 左放入)</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; lpush list0 a
(integer) 1
127.0.0.1:6379&gt; lpush list0 b c 
(integer) 4
127.0.0.1:6379&gt; lrange list0 0 -1
1) &quot;c&quot;
2) &quot;b&quot;
3) &quot;a&quot;
127.0.0.1:6379&gt; rpush list0 d e
(integer) 5
127.0.0.1:6379&gt; lrange list0 0 -1
1) &quot;c&quot;
2) &quot;b&quot;
3) &quot;a&quot;
4) &quot;d&quot;
5) &quot;e&quot;
127.0.0.1:6379&gt; llen list0
(integer) 5
127.0.0.1:6379&gt; rpoplpush list0 list1
&quot;e&quot;
127.0.0.1:6379&gt; lrange list1 0 -1
1) &quot;e&quot;
127.0.0.1:6379&gt; lrange list0 0 -1
1) &quot;c&quot;
2) &quot;b&quot;
3) &quot;a&quot;
4) &quot;d&quot;
127.0.0.1:6379&gt; rpoplpush list0 list0 
&quot;d&quot;
127.0.0.1:6379&gt; lrange list0 0 -1
1) &quot;d&quot;
2) &quot;c&quot;
3) &quot;b&quot;
4) &quot;a&quot;
127.0.0.1:6379&gt; llen list0
(integer) 4
</code></pre>
<h4 id="345-redis常用命令讲解之set">3.4.5 Redis常用命令讲解之Set</h4>
<ul>
<li>SADD</li>
<li>SREM</li>
<li>SMEMBERS</li>
<li>SISMEMBER</li>
<li>SDIFF(差集)</li>
<li>SINTER(并集)</li>
<li>SUNION(交集)</li>
<li>SCARD(元素个数)</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; sadd letters a b
(integer) 2
127.0.0.1:6379&gt; SMEMBERS letters
1) &quot;b&quot;
2) &quot;a&quot;
127.0.0.1:6379&gt; SISMEMBER letters a
(integer) 1
127.0.0.1:6379&gt; sadd setA 1 2 3
(integer) 3
127.0.0.1:6379&gt; sadd setB 2 3 4
(integer) 3
127.0.0.1:6379&gt; sdiff setA setB
1) &quot;1&quot;
127.0.0.1:6379&gt; SINTER seta setb
(empty list or set)
127.0.0.1:6379&gt; SINTER setA setB
1) &quot;2&quot;
2) &quot;3&quot;
127.0.0.1:6379&gt; SUNION setA setB
1) &quot;1&quot;
2) &quot;2&quot;
3) &quot;3&quot;
4) &quot;4&quot;
127.0.0.1:6379&gt; scard setA
(integer) 3
</code></pre>
<h4 id="346-redis常用命令讲解之sorted-set">3.4.6 Redis常用命令讲解之Sorted Set</h4>
<ul>
<li>ZADD(添加元素)</li>
<li>ZSCORE(获得某个元素)</li>
<li>ZRANGE(按照次序给出存在的元素)</li>
<li>ZRANGEBYSCORE(给出给定range里的所有元素，左右都闭)</li>
<li>ZINCRBY(增加一个元素分数)</li>
</ul>
<pre><code class="language-shell">127.0.0.1:6379&gt; ZADD scoreboard 10 x 20 y 30 z 15 a
(integer) 4
127.0.0.1:6379&gt; zscore scoreboard y
&quot;20&quot;
127.0.0.1:6379&gt; zrange scoreboard 2 3
1) &quot;y&quot;
2) &quot;z&quot;
127.0.0.1:6379&gt; ZRANGEBYSCORE scoreboard 15 25
1) &quot;a&quot;
2) &quot;y&quot;
127.0.0.1:6379&gt; ZINCRBY scoreboard 30 x
&quot;40&quot;
</code></pre>
<h2 id="4-python与redis">4. Python与Redis</h2>
<h3 id="41-redis-python客户端介绍">4.1 Redis Python客户端介绍</h3>
<blockquote>
<p><strong>推荐使用redis-py</strong></p>
</blockquote>
<ul>
<li>
<p>Getting Started</p>
<pre><code class="language-shell">&gt;&gt;&gt; import redis
&gt;&gt;&gt; r = redis.StrictRedis(host=&quot;localhost&quot;, port=6379,db=0)
&gt;&gt;&gt; r.set('foo','bar')
True
&gt;&gt;&gt; r.get('foo')
'bar'
</code></pre>
</li>
</ul>
<h3 id="42-消息队列的插入与取出">4.2 消息队列的插入与取出</h3>
<h4 id="421-基于-redis-py-生产者开发">4.2.1 基于 redis-py 生产者开发</h4>
<ul>
<li>
<p>每2秒随机生成一个用户名插入到队列中</p>
</li>
<li>
<p>LPUSH</p>
<pre><code class="language-python"># producer1.py
import redis 
import names
import time

r = redis.StrictRedis(host='localhost', port=6379, db=0)

while True:
  time.sleep(2)
  name = names.get_full_name()
  x = r.lpush('names',name)
  print(x,name)
</code></pre>
</li>
</ul>
<h4 id="422-基于-redis-py-消费者开发">4.2.2 基于 redis-py 消费者开发</h4>
<ul>
<li>
<p>将队列中的用户名按照插入顺序取出并打印到屏幕上</p>
</li>
<li>
<p>BRPOP</p>
<pre><code class="language-python"># consumer1.py
import redis

r = redis.StrictRedis(host='localhost', port=6379, db=0)

def consume(key):
  while(True):
    value = r.brpop(key)
    yield value

for v in consume('names'):
  print(v)
</code></pre>
</li>
</ul>
<h3 id="43-基于频道的发布与订阅">4.3 基于频道的发布与订阅</h3>
<h4 id="431-基于-redis-py-生产者开发">4.3.1 基于 redis-py 生产者开发</h4>
<ul>
<li>
<p>每2秒随机生成一个用户名发布到频道中</p>
</li>
<li>
<p>PUBLISH</p>
<pre><code class="language-python"># producer2.py
import redis 
import names
import time

r = redis.StrictRedis(host='localhost', port=6379, db=0)

while True:
  time.sleep(2)
  name = names.get_full_name()
  x = r.publish('names',name)
  print(x,name)
</code></pre>
</li>
</ul>
<h4 id="432-基于-redis-py-消费者开发">4.3.2 基于 redis-py 消费者开发</h4>
<ul>
<li>
<p>订阅发布用户名的频道，有新的消息时打印到屏幕上</p>
</li>
<li>
<p>SUBSCRIBE</p>
<pre><code class="language-python"># consumer2.py
import redis

r = redis.StrictRedis(host='localhost', port=6379, db=0)

ps = r.pubsub()
ps.subscribe('names')

for item in ps.listen():
  print(item)
</code></pre>
</li>
</ul>
<h2 id="5-kafka">5. Kafka</h2>
<h3 id="51-kafka简介与应用场景">5.1 Kafka简介与应用场景</h3>
<ol>
<li>
<p>Kafka简介-关键词</p>
<ul>
<li>磁盘消息队列</li>
<li>高性能</li>
<li>分布式</li>
<li>Zookeeper</li>
<li>实时流处理</li>
<li>重复消费</li>
</ul>
</li>
<li>
<p>Kafka简介--API</p>
<ul>
<li>生产者API</li>
<li>消费者API</li>
<li>流处理器API</li>
<li>连接器API</li>
</ul>
</li>
<li>
<p>Kafka简介--生产者</p>
<ul>
<li>异步通信，所有网路请求异步发送</li>
<li>批量发送，通过设置batch size或者timeout一次发送多个消息</li>
<li>线程安全，多个线程之间可以共享单个生产者实例</li>
<li>负载均衡，采用内部默认机制或者自定义负载均衡策略</li>
<li>返回结果，返回消息的topic，offset等元数据</li>
</ul>
</li>
<li>
<p>Kafka简介--消费者</p>
<ul>
<li>统一API，不在区分high-level consumer API和low-level consumer API</li>
<li>多次消费，不会删除已消费的信息，允许重复消费</li>
<li>负载均衡，基于partition和consumer group自动负载均衡</li>
<li>流量控制，允许开发者控制每次请求返回消息的条数</li>
</ul>
</li>
<li>
<p>Kafka简介--安全</p>
<ul>
<li>连接认证，连接到服务器的生产者和消费者客户端使用SSL或者SASL进行验证</li>
<li>权限管理，broker连接Zookeeper进行权限管理</li>
<li>加密传输，数据传输进行加密</li>
<li>授权管理，客户端读、写操作可以进行授权管理</li>
</ul>
</li>
<li>
<p>Kafka简介--连接器<br>
<img src="https://Zu3zz.github.io/post-images/1571396233737.jpeg" alt=""></p>
</li>
</ol>
<h3 id="52-kafka常见应用场景">5.2 Kafka常见应用场景</h3>
<ul>
<li>消息服务器</li>
<li>网站活动跟踪</li>
<li>实时数据流聚合</li>
<li>日志聚合</li>
</ul>
<h3 id="53-kafka安装与简单用例">5.3 Kafka安装与简单用例</h3>
<h4 id="531-安装">5.3.1 安装</h4>
<ol>
<li>
<p>下载并且解压缩kafka安装包 这里选择当前最近的2.3.0版本</p>
<pre><code class="language-shell">$ wget http://apache.website-solution.net/kafka/2.3.0/kafka-2.3.0-src.tgz
$ tar -xzf kafka_2.12-2.3.0.tgz
$ cd kafka_2.12-2.3.0
</code></pre>
</li>
<li>
<p>安装Gradle和Zookeeper，为了方便管理 这里统一使用</p>
<pre><code class="language-shell">$ brew install gradle
$ brew install zookeeper
</code></pre>
</li>
</ol>
<h4 id="532-启动服务">5.3.2 启动服务</h4>
<ol>
<li>
<p>启动Zookeeper</p>
<pre><code class="language-shell">$ bin/zookeeper-server-start.sh config/zookeeper.properties
如果是使用homebrew 直接使用下面两条命令即可
$ zkServer
$ zkCli
</code></pre>
</li>
<li>
<p>启动kafka</p>
<pre><code class="language-bash">$ bin/kafka-server-start.sh config/server.properties
</code></pre>
</li>
</ol>
<h4 id="533-创建一个topic">5.3.3 创建一个Topic</h4>
<ol>
<li>
<p>创建一个测试用Topic</p>
<pre><code class="language-shell">$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test
</code></pre>
</li>
<li>
<p>查看当前已经创建的测试Topic</p>
<pre><code class="language-shell">$ bin/kafka-topics.sh --list --bootstrap-server localhost:9092
test1
</code></pre>
<p>输出test1 说明创建成功</p>
</li>
</ol>
<h4 id="534-消息的生产与消费">5.3.4 消息的生产与消费</h4>
<ol>
<li>
<p>通过producer产生消息</p>
<pre><code class="language-shell">$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
&gt;this is message one
&gt;this is message two
</code></pre>
</li>
<li>
<p>通过comsumer拿到所有当前消息</p>
<pre><code class="language-shell">$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
this is message one
this is message two
</code></pre>
<p>成功输出输入的消息</p>
</li>
</ol>
<h3 id="54-kafka的配置">5.4 Kafka的配置</h3>
<h4 id="541-kafka服务器配置文件">5.4.1 Kafka服务器配置文件</h4>
<ul>
<li>
<p>配置文件位于config/server.properties下</p>
</li>
<li>
<p>通过如下命令将配置文件导出至broker.properties中</p>
<pre><code class="language-shell">$ grep '^[^#]' server.properties &gt; broker.properties
</code></pre>
</li>
<li>
<p>查看broker.properties中去除了注释的内容</p>
<pre><code class="language-shell">broker.id=0
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/tmp/kafka-logs
num.partitions=1
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=6000
group.initial.rebalance.delay.ms=0
</code></pre>
</li>
</ul>
<h4 id="542-kafka默认配置项">5.4.2 Kafka默认配置项</h4>
<blockquote>
<p>加粗的为重要配置</p>
</blockquote>
<ol>
<li>
<p>Disk IO</p>
<ul>
<li>num.io.threads = 8 设置broker处理磁盘IO的线程数</li>
<li>num.partitions = 1 设置每个topic的分区个数</li>
<li>num.recovery.threads.per.data.dir = 1 配置每个数据恢复时的线程数</li>
<li><strong>log.dirs = /tmp/kafka-logs kafka数据的存放目录</strong></li>
<li>log.retention.hours = 168 消息的存储时间 单位是小时</li>
<li>log.segment.bytes = 1073741824 topic的partition是以segment的形式存储的</li>
<li>log.retention.check.interval.ms=300000 定时检查文件大小</li>
</ul>
</li>
<li>
<p>Network</p>
<ul>
<li>num.network.threads = 3 设置broker处理网络请求的最大线程数 一般可设置为CPU的核数</li>
<li>socket.send.buffer.bytes=102400 socket发送缓冲区的大小；-1即为操作系统默认值</li>
<li>socket.receive.buffer.bytes=102400 接受缓冲区；-1为默认值</li>
<li>socket.request.max.bytes=104857600 每一个socket的最大字节数</li>
</ul>
</li>
<li>
<p>Cluster</p>
<ul>
<li>
<p>zookeeper.connect=localhost:2181 zookeeper的集群地址；值可以有多个，中间用逗号分隔</p>
</li>
<li>
<p>zookeeper.connection.timeout.ms=6000 连接的超时时间</p>
</li>
<li>
<p><strong>broker.id=0 broker在集群中的唯一标识；如果没有，zookeeper从1001开始递增</strong></p>
</li>
</ul>
</li>
</ol>
<h3 id="55-kafka相关概念">5.5 Kafka相关概念</h3>
<h4 id="551-常见概念-集群">5.5.1 常见概念--集群</h4>
<ul>
<li>Cluster：集群</li>
<li>Broker：每个服务器都是一个Broker</li>
<li>Producer：生产者</li>
<li>Consumer：消费者</li>
<li>Consumer Group：实现topic广播的手段</li>
</ul>
<h4 id="552-常见概念-消息">5.5.2 常见概念--消息</h4>
<ul>
<li>Record：每一条消息</li>
<li>Topic：每一个消息都会有一个Topic</li>
<li>Partition：每个Topic包含一个或者多个Partition</li>
<li>Segment：每个Partition包含一个或者多个Segment</li>
<li>Offset：Partition中每个消息都有的序列号，可以唯一标识一条消息</li>
<li>Replication：副本；kafka支持以Partition为单位对消息进行备份</li>
<li>Leader：所有读写请求都由Leader来处理</li>
</ul>
<h4 id="553-常见概念-topic">5.5.3 常见概念--Topic</h4>
<ul>
<li>
<p>Topic可以被看做是一个队列</p>
<figure data-type="image" tabindex="5"><img src="https://Zu3zz.github.io/post-images/1571396270203.png" alt="topic"></figure>
</li>
</ul>
<h4 id="554-常见概念-partition">5.5.4 常见概念--Partition</h4>
<figure data-type="image" tabindex="6"><img src="https://Zu3zz.github.io/post-images/1571396287204.png" alt="partition"></figure>
<h4 id="555-常见概念-segment">5.5.5 常见概念--Segment</h4>
<figure data-type="image" tabindex="7"><img src="https://Zu3zz.github.io/post-images/1571396308603.png" alt="segment"></figure>
<h4 id="556-常见概念-集群">5.5.6 常见概念--集群</h4>
<figure data-type="image" tabindex="8"><img src="https://Zu3zz.github.io/post-images/1571396330337.png" alt="集群"></figure>
<h2 id="6-python与kafka">6. Python与Kafka</h2>
<ul>
<li>由于有很多python库都可以使用kafka</li>
<li>知名的有kafka-python；pykafka；faust；这里使用kafka-python</li>
</ul>
<blockquote>
<p>使用pip安装</p>
<p>pip install kafka-python</p>
</blockquote>
<ol>
<li>
<p>创建kafkaProducer.py</p>
<pre><code class="language-python">from kafka import KafkaProducer
import names

producer = KafkaProducer()

for _ in range(10):
    name = names.get_full_name()
    future = producer.send('test', bytes(name,'utf-8'))
    result = future.get(60)
    print(result)
</code></pre>
</li>
<li>
<p>创建kafkaConsumer.py</p>
<pre><code class="language-python">from kafka import KafkaConsumer

consumer = KafkaConsumer('test',group_id='test01')

for msg in consumer:
    print(msg)
</code></pre>
</li>
<li>
<p>进行数据的产生与消费</p>
<pre><code class="language-shell">$ python kafkaProducer.py 
RecordMetadata(topic='test', partition=0, topic_partition=TopicPartition(topic='test', partition=0), offset=14, timestamp=1571317704074, checksum=None, serialized_key_size=-1, serialized_value_size=15, serialized_header_size=-1)
RecordMetadata(topic='test', partition=0, topic_partition=TopicPartition(topic='test', partition=0), offset=15, timestamp=1571317704182, checksum=None, serialized_key_size=-1, serialized_value_size=12, serialized_header_size=-1)
...
</code></pre>
<pre><code class="language-shell">$ python kafkaConsumer.py 
ConsumerRecord(topic='test', partition=0, offset=14, timestamp=1571317704074, timestamp_type=0, key=None, value=b'Margaret Medina', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=15, serialized_header_size=-1)
ConsumerRecord(topic='test', partition=0, offset=15, timestamp=1571317704182, timestamp_type=0, key=None, value=b'Mark Mccarty', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=12, serialized_header_size=-1)
...
</code></pre>
<p>可以看到，通过Producer产生的数据已经被Consumer成功拿到。</p>
</li>
</ol>
<h2 id="7-框架整合">7. 框架整合</h2>
<h3 id="71-采集日志">7.1 采集日志</h3>
<h4 id="711-日志采集方案介绍">7.1.1 日志采集方案介绍</h4>
<ul>
<li>flume</li>
<li>rsyslog</li>
<li>heka</li>
<li>logstash</li>
</ul>
<h4 id="712-nginx访问日志格式配置">7.1.2 nginx访问日志格式配置</h4>
<ul>
<li>
<p>自定义日志格式</p>
</li>
<li>
<p>直接输出json</p>
<pre><code class="language-shell">log_format json '{&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,'
								'&quot;host&quot;:&quot;$server_addr&quot;,'
								'&quot;clientip&quot;:&quot;$remote_addr&quot;,'
								'&quot;size&quot;:$body_bytes_sent,'
								'&quot;responsetime&quot;:$request_time,'
								'&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,'
								'&quot;upstreamhost&quot;:&quot;$upstream_addr&quot;,'
								'&quot;http_host&quot;:&quot;$host&quot;,'
								'&quot;url&quot;:&quot;$url&quot;,'
								'&quot;xff&quot;:&quot;$http_x_forwarded_for&quot;,'
								'&quot;refer&quot;:&quot;$http_referer&quot;,'
								'&quot;agent&quot;:&quot;$http_user_agent&quot;,'
								'&quot;status&quot;:&quot;$status&quot;}';
asscess_log /tmp/nginx/access.log json;
</code></pre>
</li>
<li>
<p>可以看到nginx的access日志就是json格式的了</p>
</li>
</ul>
<h3 id="72-logstash配置">7.2 logstash配置</h3>
<ul>
<li>logstash配置主要分为以下三个部分
<ol>
<li>input</li>
<li>filter</li>
<li>output</li>
</ol>
</li>
</ul>
<h4 id="721-kafka插件">7.2.1 kafka插件</h4>
<ul>
<li>
<p>配置logstash output kafka插件</p>
<pre><code class="language-shell">input {
    file {
        path =&gt; &quot;/pot/openresty/nginx/logs/access.log&quot;
        codec =&gt; json
    }
}
filter {
    # 对useragent进行预处理
    useragent {
        source =&gt; &quot;agent&quot;
        target =&gt; &quot;user_agent&quot;
        remove_field =&gt; &quot;agent&quot;
    }
    # 对ip地址的地区进行预处理
    geoip {
        source =&gt; &quot;clientip&quot;
        target =&gt; &quot;geoip&quot;
    }
}
output {
    # debug用
    stdout {
        codec =&gt; rubydebug
    }
    # logstash output kafka插件
    kafka {
        codec =&gt; json
        topic_id =&gt; &quot;nginx&quot;
        # kafka对外地址
        bootstrap_servers =&gt; &quot;localhost:9092&quot;
    }
}
</code></pre>
</li>
</ul>
<h3 id="73-启动日志">7.3 启动日志</h3>
<p><strong>依次运行一下应用</strong></p>
<ul>
<li>nginx</li>
<li>logstash</li>
<li>kafka</li>
<li>Zookeeper</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringBoot系列(2)----配置文件]]></title>
        <id>https://Zu3zz.github.io/post/springboot-2</id>
        <link href="https://Zu3zz.github.io/post/springboot-2">
        </link>
        <updated>2019-10-10T09:11:01.000Z</updated>
        <summary type="html"><![CDATA[<center>🎏那么又到了我们SpringBoot系列的第二篇！！</center>
<center>🎀这一篇，我们来讲讲SpringBoot中的配置相关问题，依旧有源码级别的分析哦！</center>]]></summary>
        <content type="html"><![CDATA[<center>🎏那么又到了我们SpringBoot系列的第二篇！！</center>
<center>🎀这一篇，我们来讲讲SpringBoot中的配置相关问题，依旧有源码级别的分析哦！</center>
<!-- more -->
<h2 id="1-配置文件">1、配置文件</h2>
<ul>
<li>
<p>Spring Boot使用一个全局的配置文件，配置文件名是固定的</p>
<ol>
<li>application.properties</li>
<li>application.yml</li>
</ol>
</li>
<li>
<p>配置文件的作用：修改SpringBoot自动默认的设置值；SpringBoot在底层都给我们自动配置好了</p>
</li>
<li>
<p>YAML( YAML Ain't Markup Language)</p>
<ul>
<li>
<p>YAML A Markup Language: 是一个标记语言</p>
</li>
<li>
<p>YAML isn't Markup Language: 不是一个标记语言</p>
</li>
<li>
<p>标记语言：</p>
<ul>
<li>
<p>以前的配置文件：大多使用 <strong>xxxx.xml</strong>文件；</p>
</li>
<li>
<p>YAML: <strong>以数据为中心</strong>，比JSON、XML更适合做配置文件</p>
</li>
<li>
<p>YAML配置例子</p>
<pre><code class="language-yml">server:
	port: 8081
</code></pre>
</li>
<li>
<p>XML:</p>
<pre><code class="language-xml">&lt;server&gt;
  &lt;port&gt;8081&lt;/port&gt;
&lt;/server&gt;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2-yaml语法">2、 YAML语法</h2>
<h3 id="1-基本语法">1、基本语法</h3>
<ul>
<li>
<p>k:(空格)v : 表示一对键值对(空格必须有)；</p>
</li>
<li>
<p>以<strong>空格</strong>的缩进来控制层级关系；只要是左对齐的一列数据，都是同一层级的</p>
<pre><code class="language-text">server:
	port: 8081
	path: /hello
</code></pre>
</li>
<li>
<p>属性和值也是大小写敏感的</p>
</li>
</ul>
<h3 id="2-值的写法">2、值的写法</h3>
<h4 id="字面量普通的值数字-字符串-布尔值">字面量：普通的值(数字、字符串、布尔值)</h4>
<ul>
<li>
<p>K：V：字面直接来写</p>
</li>
<li>
<p>字符串默认不用加上单引号或者双引号</p>
<ul>
<li>&quot;&quot;:双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表达的意思</li>
</ul>
<pre><code class="language-yaml">name: &quot;zhangsan \n lisi&quot;
输出： zhangsan 换行 lisi
</code></pre>
<ul>
<li>:单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据</li>
</ul>
<pre><code class="language-yaml">name: 'zhangsan \n lisi'
输出： zhangsan \n  lisi
</code></pre>
</li>
</ul>
<h4 id="对象-map属性和值键值对">对象、Map(属性和值)(键值对)：</h4>
<ul>
<li>
<p>k：v：在下一行来写对象的属性和值的关系；注意缩进</p>
<ul>
<li>对象还是k：v的方式</li>
</ul>
<pre><code class="language-yaml">friends:
	lastname: zhangsan
	age: 20
</code></pre>
<ul>
<li>行内写法</li>
</ul>
<pre><code class="language-yaml">fridends: {lastName: zhangsan, age: 18}
</code></pre>
</li>
</ul>
<h4 id="数组list-set">数组（List、Set）：</h4>
<ul>
<li>
<p>用- 值表示数组中的一个元素</p>
<pre><code class="language-yaml">pets:
 - cat
 - dot
 - pig
</code></pre>
</li>
<li>
<p>行内写法</p>
<pre><code class="language-yaml">pets: [cat,dog,pig]
</code></pre>
</li>
</ul>
<h2 id="3-配置文件值注入">3、配置文件值注入</h2>
<ul>
<li>配置文件</li>
</ul>
<pre><code class="language-yaml">person:
	lastName: hello
	age: 18
	boss: false
	birth: 2019/10/7
	maps: {k1:v1, k2:v2}
	lists:
	 - lisi
	 - zhaoliu
	dog:
		name: 小狗
		age: 12
</code></pre>
<ul>
<li>JavaBean</li>
</ul>
<pre><code class="language-java">/**
 * 将配置文件中配置的每一个属性的值，映射到这个组件中
 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定；
 *      prefix = &quot;person&quot;：配置文件中哪个下面的所有属性进行一一映射
 * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能；
 */
@Component
@ConfigurationProperties(prefix = &quot;person&quot;)
public class Person {
    private String lastName;
    private Integer age;
    private Boolean boss;
    private Date birth;
  
    private Map&lt;String,Object&gt; maps;
    private List&lt;Object&gt; lists;
    private Dog dog;
}
</code></pre>
<ul>
<li>我们可以导入配置文件处理器，以后编写配置就有提示了</li>
</ul>
<pre><code class="language-xml">&lt;!--导入配置文件处理器，配置文件进行绑定就会有提示--&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;
  &lt;optional&gt;true&lt;/optional&gt;
&lt;/dependency&gt;
</code></pre>
<ul>
<li>SpringBoot中的测试类</li>
</ul>
<pre><code class="language-java">/**
 * SpringBoot单元测试
 * 可以在测试期间很方便的类似编码一样进行自动注入等容器的功能
 */
@RunWith(SpringRunner.class)
@SpringBootTest
public class DemoApplicationTests {}
</code></pre>
<h4 id="1-properties配置文件在idea中默认utf-8可能会乱码">1、properties配置文件在idea中默认utf-8可能会乱码</h4>
<ul>
<li>调整IDEA中的默认编码<img src="https://Zu3zz.github.io/post-images/1571315004181.png" alt="调整IDEA"></li>
</ul>
<h4 id="2-value获取值和configurationproperties获取值比较">2、@Value获取值和@ConfigurationProperties获取值比较</h4>
<table>
<thead>
<tr>
<th></th>
<th>@ConfigurationProperties</th>
<th>@Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>功能</td>
<td>批量注入配置文件中的属性</td>
<td>一个个指定</td>
</tr>
<tr>
<td>松散绑定(驼峰与下划线转换)</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>SpEL(Spring表达式)</td>
<td>不支持</td>
<td>支持</td>
</tr>
<tr>
<td>JSR303数据校验(@Validated)</td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td>复杂类型封装</td>
<td>支持</td>
<td>不支持</td>
</tr>
</tbody>
</table>
<ul>
<li>配置文件yml还是properties他们都能获取到值；</li>
<li>如果说，我们只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value；</li>
<li>如果说，我们专门编写了一个JavaBean来和配置文件进行映射，我们就直接使用@ConfigurationProperties；</li>
</ul>
<h4 id="3-配置文件注入值数据校验">3、配置文件注入值数据校验</h4>
<pre><code class="language-java">@Component
@ConfigurationProperties(prefix = &quot;person&quot;)
@Validated
public class Person {
    /**
     * &lt;bean class=&quot;Person&quot;&gt;
     * 	&lt;property name=&quot;lastName&quot; value=&quot;字面量/${key}从环境变量、配置文件中获取值/#{Spring表达式}&quot;&gt;
     	&lt;/property&gt;
     * &lt;bean/&gt;
     */
   	// lastName必须是邮箱格式
    @Email
    // @Value(&quot;${person.last-name}&quot;)
    private String lastName;
    // @Value(&quot;#{11*2}&quot;)
    private Integer age;
    // @Value(&quot;true&quot;)
    private Boolean boss;
    private Date birth;
    private Map&lt;String,Object&gt; maps;
    private List&lt;Object&gt; lists;
    private Dog dog;
}
</code></pre>
<h4 id="4-propertysource-importresource">4、@PropertySource &amp; @ImportResource</h4>
<ul>
<li><strong>@PropertySource:</strong> ： 加载指定的配置文件</li>
</ul>
<pre><code class="language-java">/**
 * 将配置文件中配置的每一个属性的值，映射到这个组件中
 * @ConfigurationProperties：告诉SpringBoot将本类中的所有属性和配置文件中相关的配置进行绑定；
 *      prefix = &quot;person&quot;：配置文件中哪个下面的所有属性进行一一映射
 * 只有这个组件是容器中的组件，才能容器提供的@ConfigurationProperties功能；
 *  @ConfigurationProperties(prefix = &quot;person&quot;)默认从全局配置文件中获取值；
 */
@PropertySource(value = {&quot;classpath:person.properties&quot;})
@Component
@ConfigurationProperties(prefix = &quot;person&quot;)
//@Validated
public class Person {
   	//lastName必须是邮箱格式
   	// @Email
    //@Value(&quot;${person.last-name}&quot;)
    private String lastName;
    //@Value(&quot;#{11*2}&quot;)
    private Integer age;
    //@Value(&quot;true&quot;)
    private Boolean boss;
}
</code></pre>
<ul>
<li>
<p><strong>@ImportResource</strong>: 导入Sprinig的配置文件，让配置文件里面的内容生效</p>
</li>
<li>
<p>Spring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别；</p>
</li>
<li>
<p>想让Spring的配置文件生效，加载进来，@<strong>ImportResource</strong>标注在一个配置类上</p>
<pre><code class="language-java">@ImportResource(locations = {&quot;classpath:beans.xml&quot;})
导入Spring的配置文件让其生效
</code></pre>
</li>
<li>
<p>不来编写Spring中的配置文件</p>
<pre><code class="language-xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;
       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;
    &lt;bean id=&quot;helloService&quot; class=&quot;com.zth.springboot.service.HelloService&quot;&gt;&lt;/bean&gt;
&lt;/beans&gt;
</code></pre>
</li>
<li>
<p><strong>上面这种方式在SpringBoot2.x中已经不再使用！！！！！！！</strong></p>
</li>
<li>
<p>SpringBoot推荐给容器中添加组件的方式：推荐使用全注解的方式</p>
<ol>
<li>配置类**@Configuration** -----&gt; Spring配置文件</li>
<li>使用**@Bean**给容器中添加组件</li>
</ol>
<pre><code class="language-java">/**
 * @Configuration：指明当前类是一个配置类；就是来替代之前的Spring配置文件
 * 在配置文件中用&lt;bean&gt;&lt;bean/&gt;标签添加组件
 */
@Configuration
public class MyAppConfig {
    //将方法的返回值添加到容器中；容器中这个组件默认的id就是方法名
    @Bean
    public HelloService helloService02(){
        System.out.println(&quot;配置类@Bean给容器中添加组件了...&quot;);
        return new HelloService();
    }
}
</code></pre>
</li>
</ul>
<h2 id="4-配置文件占位符">4、配置文件占位符</h2>
<h3 id="1-随机数">1、随机数</h3>
<pre><code class="language-java">${random.value}、${random.int}、${random.long}
${random.int(10)}、${random.int[1024,65536]}
</code></pre>
<h3 id="2-占位符获取之前配置的值如果没有可以是用指定默认值">2、占位符获取之前配置的值，如果没有可以是用:指定默认值</h3>
<pre><code class="language-properties">person.last-name=张三${random.uuid}
person.age=${random.int}
person.birth=2017/12/15
person.boss=false
person.maps.k1=v1
person.maps.k2=14
person.lists=a,b,c
person.dog.name=${person.hello:hello}_dog
person.dog.age=15
</code></pre>
<h2 id="5-profile">5、Profile</h2>
<h3 id="1-多profile文件">1、多Profile文件</h3>
<ul>
<li>我们在主配置文件编写的时候，文件名可以是   application-{profile}.properties/yml</li>
<li>当创建多个文件时，默认使用application.properties的配置；</li>
</ul>
<h3 id="2-yml支持多文档块方式">2、yml支持多文档块方式</h3>
<ul>
<li>yml更简洁一些</li>
</ul>
<pre><code class="language-yml">server:
  port: 8081
spring:
  profiles:
    active: prod # 激活prod环境
---
server:
  port: 8083
spring:
  profiles: dev
---
server:
  port: 8084
spring:
  profiles: prod  #指定属于哪个环境
</code></pre>
<h3 id="3-激活指定profile">3、激活指定profile</h3>
<p>1、在配置文件中指定  spring.profiles.active=dev（yml中配置如上）</p>
<p>2、命令行：</p>
<pre><code class="language-shell">java -jar spring-boot-02-config-0.0.1-SNAPSHOT.jar --spring.profiles.active=dev；
</code></pre>
<ul>
<li>可以直接在测试的时候，配置传入命令行参数，在IDEA中配置**--spring.profiles.active=dev**</li>
</ul>
<p>3、虚拟机参数；</p>
<pre><code class="language-shell">-Dspring.profiles.active=dev
</code></pre>
<h2 id="6-配置文件加载位置">6、配置文件加载位置</h2>
<ul>
<li>
<p>springboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件</p>
<ol>
<li>–file:./config/</li>
<li>–file:./</li>
<li>–classpath:/config/</li>
<li>–classpath:/</li>
</ol>
</li>
<li>
<p>优先级由高到底，高优先级的配置会覆盖低优先级的配置；</p>
</li>
<li>
<p>SpringBoot会从这四个位置全部加载主配置文件；<strong>互补配置</strong>；</p>
</li>
<li>
<p>我们还可以通过spring.config.location来改变默认的配置文件位置</p>
</li>
<li>
<p><strong>项目打包好以后，我们可以使用命令行参数的形式，启动项目的时候来指定配置文件的新位置；指定配置文件和默认加载的这些配置文件共同起作用形成互补配置；</strong></p>
<pre><code class="language-shell">java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --spring.config.location=G:/application.properties
</code></pre>
</li>
</ul>
<h2 id="7-外部配置加载顺序">7、外部配置加载顺序</h2>
<ul>
<li>
<p><strong>SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置</strong></p>
<ol>
<li><strong>命令行参数</strong></li>
</ol>
<ul>
<li>
<p>所有的配置都可以在命令行上进行指定</p>
<pre><code class="language-shell">java -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --server.port=8087  --server.context-path=/abc
</code></pre>
</li>
<li>
<p>多个配置用空格分开； --配置项=值</p>
</li>
</ul>
<ol start="2">
<li>来自java: comp/env的JNDI属性</li>
<li>Java系统属性(System.getProperties())</li>
<li>操作系统环境变量</li>
<li>RandomValuePropertySource配置的random.*属性值</li>
<li><strong>jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件</strong></li>
<li><strong>jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件</strong></li>
</ol>
</li>
<li>
<p><strong>再来加载不带profile</strong></p>
<ol start="8">
<li><strong>jar包外部的application.properties或application.yml(不带spring.profile)配置文件</strong></li>
<li><strong>jar包内部的application.properties或application.yml(不带spring.profile)配置文件</strong></li>
<li>@Configuration注解类上的@PropertySource</li>
<li>通过SpringApplication.setDefaultProperties指定的默认属性</li>
</ol>
</li>
<li>
<p>所有支持的配置加载来源；<a href="https://docs.spring.io/spring-boot/docs/1.5.9.RELEASE/reference/htmlsingle/#boot-features-external-config">参考官方文档</a></p>
</li>
</ul>
<h2 id="8-自动配置原理">8、自动配置原理</h2>
<ul>
<li>配置文件到底能写什么？怎么写？自动配置原理？</li>
<li><a href="https://docs.spring.io/spring-boot/docs/1.5.9.RELEASE/reference/htmlsingle/#common-application-properties">配置文件能配置的属性参照</a></li>
</ul>
<h3 id="1-自动配置原理">1、<strong>自动配置原理</strong></h3>
<ol>
<li>
<p>SpringBoot启动的时候加载主配置类，开启了自动配置功能**@EnableAutoConfiguration**</p>
</li>
<li>
<p><strong>@EnableAutoConfiguration的作用</strong></p>
<ul>
<li>
<p>利用EnableAutoConfigurationImportSelector给容器中导入一些组件？</p>
</li>
<li>
<p>可以查看selectImports()方法的内容</p>
</li>
<li>
<p>List<String> configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置</p>
<pre><code class="language-java">SpringFactoriesLoader.loadFactoryNames()
扫描所有jar包类路径下  META-INF/spring.factories
把扫描到的这些文件的内容包装成properties对象
从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中
</code></pre>
</li>
<li>
<p>将类路径下的 META-INF/spring.factories里面配置的所有EnableAutoConfiguration的值加入到了容器中；</p>
</li>
</ul>
<pre><code class="language-properties"># Auto Configure
org.springframework.boot.autoconfigure.EnableAutoConfiguration=\
org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\
org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\
org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\
org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\
org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\...
# 省略若干
</code></pre>
<ul>
<li>每一个这样的  xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置；</li>
</ul>
</li>
<li>
<p>每一个自动配置类进行自动配置功能；</p>
</li>
<li>
<p>以**HttpEncodingAutoConfiguration（Http编码自动配置）**为例解释自动配置原理；</p>
<pre><code class="language-java">// 表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件
@Configuration

// 启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpProperties绑定起来；并把HttpProperties加入到ioc容器中
@EnableConfigurationProperties(HttpProperties.class)

// Spring底层@Conditional注解，根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效；判断当前应用是否是web应用，如果是，当前配置类生效
@ConditionalOnWebApplication(type = ConditionalOnWebApplication.Type.SERVLET)

// 判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器；
@ConditionalOnClass(CharacterEncodingFilter.class)

// 判断配置文件中是否存在某个配置  spring.http.encoding.enabled；如果不存在，判断也是成立的
@ConditionalOnProperty(prefix = &quot;spring.http.encoding&quot;, value = &quot;enabled&quot;, matchIfMissing = true)
public class HttpEncodingAutoConfiguration {
  // 这里与SpringBoot配置文件进行映射
  private final HttpProperties.Encoding properties;
  
  // 只有一个有参构造器的情况下，参数的值就会从容器中拿
	public HttpEncodingAutoConfiguration(HttpProperties properties) {
		this.properties = properties.getEncoding();
	}
	@Bean // 给容器中添加一个组件，这个组件的某些值需要从properties中获取
	@ConditionalOnMissingBean // 判断容器中是否有这个组件
	public CharacterEncodingFilter characterEncodingFilter() {
		CharacterEncodingFilter filter = new OrderedCharacterEncodingFilter();
		filter.setEncoding(this.properties.getCharset().name());
		filter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST));
		filter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE));
		return filter;
	}
}
</code></pre>
<ul>
<li>
<p>根据当前不同的条件判断，决定这个配置类，是否生效</p>
</li>
<li>
<p>一旦这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性都是在xxxxProperties类中封装着；配置文件能配置什么就可以参照某个功能对应的这个配置类</p>
<pre><code class="language-java">// 从配置文件中获取指定的值和bean的属性进行绑定
@ConfigurationProperties(prefix = &quot;spring.http&quot;)
public class HttpProperties {
  public static class Encoding {
    public static final Charset DEFAULT_CHARSET = StandardCharsets.UTF_8;
  }
}
</code></pre>
</li>
</ul>
</li>
</ol>
<ul>
<li>
<p><strong>SpringBoot的精髓</strong></p>
<ol>
<li><strong>SpringBoot会加载大量的自动配置类</strong></li>
<li><strong>我们看我们需要的功能有没有SpringBoot默认写好的自动配置类</strong></li>
<li><strong>我们再来看这个自动配置类中到底配置了哪些组件；(只要我们要用的组件有，我们就不需要再来配置了)</strong></li>
<li><strong>给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值；</strong></li>
</ol>
</li>
<li>
<p>xxxxAutoConfigurartion：自动配置类；</p>
</li>
<li>
<p>给容器中添加组件</p>
</li>
<li>
<p>xxxxProperties:封装配置文件中相关属性；</p>
</li>
</ul>
<h3 id="2-细节">2、细节</h3>
<h4 id="1-conditional派生注解spring中原生的conditional作用">1、@Conditional派生注解(Spring中原生的@Conditional作用)</h4>
<ul>
<li>
<p>作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置里面的所有内容才生效；</p>
<table>
<thead>
<tr>
<th>@Conditional扩展注解</th>
<th>作用（判断是否满足当前指定条件）</th>
</tr>
</thead>
<tbody>
<tr>
<td>@ConditionalOnJava</td>
<td>系统的java版本是否符合要求</td>
</tr>
<tr>
<td>@ConditionalOnBean</td>
<td>容器中存在指定Bean；</td>
</tr>
<tr>
<td>@ConditionalOnMissingBean</td>
<td>容器中不存在指定Bean；</td>
</tr>
<tr>
<td>@ConditionalOnExpression</td>
<td>满足SpEL表达式指定</td>
</tr>
<tr>
<td>@ConditionalOnClass</td>
<td>系统中有指定的类</td>
</tr>
<tr>
<td>@ConditionalOnMissingClass</td>
<td>系统中没有指定的类</td>
</tr>
<tr>
<td>@ConditionalOnSingleCandidate</td>
<td>容器中只有一个指定的Bean，或者这个Bean是首选Bean</td>
</tr>
<tr>
<td>@ConditionalOnProperty</td>
<td>系统中指定的属性是否有指定的值</td>
</tr>
<tr>
<td>@ConditionalOnResource</td>
<td>类路径下是否存在指定资源文件</td>
</tr>
<tr>
<td>@ConditionalOnWebApplication</td>
<td>当前是web环境</td>
</tr>
<tr>
<td>@ConditionalOnNotWebApplication</td>
<td>当前不是web环境</td>
</tr>
<tr>
<td>@ConditionalOnJndi</td>
<td>JNDI存在指定项</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h4 id="2-自动配置类必须在一定的条件下才能生效">2、自动配置类必须在一定的条件下才能生效</h4>
<ul>
<li>
<p>我们怎么知道哪些自动配置类生效？</p>
</li>
<li>
<p><strong>我们可以通过启用  debug=true属性；来让控制台打印自动配置报告</strong>，这样我们可以很方便的知道哪些哪些自动配置类生效</p>
<pre><code class="language-shell">============================
CONDITIONS EVALUATION REPORT
============================
Positive matches: （自动配置类启用的）
-----------------
CodecsAutoConfiguration matched:
    - @ConditionalOnClass found required class 'org.springframework.http.codec.CodecConfigurer' (OnClassCondition)

CodecsAutoConfiguration.JacksonCodecConfiguration matched:
    - @ConditionalOnClass found required class 'com.fasterxml.jackson.databind.ObjectMapper' (OnClassCondition)
    下面若干省略...
    
Negative matches:（没有启动，没有匹配成功的自动配置类）
-----------------
ActiveMQAutoConfiguration:
    Did not match:
        - @ConditionalOnClass did not find required classes 'javax.jms.ConnectionFactory', 'org.apache.activemq.ActiveMQConnectionFactory' (OnClassCondition)

AopAutoConfiguration:
    Did not match:
        - @ConditionalOnClass did not find required classes 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice' (OnClassCondition)
</code></pre>
</li>
</ul>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpringBoot系列(1)----SpringBoot入门]]></title>
        <id>https://Zu3zz.github.io/post/springboot-1</id>
        <link href="https://Zu3zz.github.io/post/springboot-1">
        </link>
        <updated>2019-10-07T08:53:18.000Z</updated>
        <summary type="html"><![CDATA[<center>🎉开新坑啦 Java后端系列之SpringBoot篇第一章：SpringBoot入门🎉</center>
<center>🎊🎊有源码级别的分析哦~~🎊🎊</center>]]></summary>
        <content type="html"><![CDATA[<center>🎉开新坑啦 Java后端系列之SpringBoot篇第一章：SpringBoot入门🎉</center>
<center>🎊🎊有源码级别的分析哦~~🎊🎊</center>
<!-- more -->
<h2 id="1-spring-boot-入门">1、Spring Boot 入门</h2>
<h3 id="1-spring-boot-简介">1、 Spring Boot 简介</h3>
<blockquote>
<p>简化Spring应用开发的一个框架;</p>
<p>整个Spring技术栈的一个大整合;</p>
<p>J2EE开发的一站式解决方案;</p>
</blockquote>
<h2 id="2-微服务">2、微服务</h2>
<p>2014，martin fowler</p>
<p>微服务：架构风格（服务微化）</p>
<p>一个应用应该是一组小型服务；可以通过HTTP的方式进行互通；</p>
<p>单体应用：ALL IN ONE</p>
<p>微服务：每一个功能元素最终都是一个可独立替换和独立升级的软件单元；</p>
<p><a href="https://martinfowler.com/articles/microservices.html#MicroservicesAndSoa">详细参照微服务文档</a></p>
<h2 id="3-环境准备">3、环境准备</h2>
<p>环境约束</p>
<ul>
<li>jdk1.8：Spring Boot 推荐jdk1.7及以上；java version &quot;1.8.0_112&quot;</li>
<li>maven3.x：maven 3.3以上版本；Apache Maven 3.3.9</li>
<li>IntelliJIDEA2017：IntelliJ IDEA 2017.2.2 x64、STS</li>
<li>SpringBoot 1.5.9.RELEASE：1.5.9；</li>
</ul>
<p>统一环境；</p>
<h3 id="1-maven设置">1、MAVEN设置</h3>
<p>给maven 的settings.xml配置文件的profiles标签添加</p>
<pre><code class="language-xml">&lt;profile&gt;
  &lt;id&gt;jdk-1.8&lt;/id&gt;
  &lt;activation&gt;
    &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
    &lt;jdk&gt;1.8&lt;/jdk&gt;
  &lt;/activation&gt;
  &lt;properties&gt;
    &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
    &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt;
  &lt;/properties&gt;
&lt;/profile&gt;
</code></pre>
<h3 id="2-idea设置">2、IDEA设置</h3>
<p>整合maven</p>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io/post-images/1570438626151.png" alt="IDEA"></figure>
<figure data-type="image" tabindex="2"><img src="https://Zu3zz.github.io/post-images/1570438637757.png" alt="IDEA_MAVEN"></figure>
<h2 id="4-spring-boot-helloworld">4、Spring Boot HelloWorld</h2>
<p>一个功能：</p>
<p>浏览器发送hello请求，服务器接收请求并处理，相应HelloWorld字符串;</p>
<h3 id="1-创建一个maven工程jar">1、创建一个maven工程；(jar)</h3>
<h3 id="2-导入spring-boot相关的依赖">2、导入spring boot相关的依赖</h3>
<pre><code class="language-xml">&lt;parent&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
  &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;
&lt;/parent&gt;
&lt;dependencies&gt;
  &lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
  &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>
<h3 id="3-编写一个主程序启动spring-boot应用">3、编写一个主程序：启动Spring Boot应用</h3>
<pre><code class="language-java">/**
 *  @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用
 */
@SpringBootApplication
public class HelloWorldMainApplication {
  public static void main(String[] args) {
    // Spring应用启动起来
    SpringApplication.run(HelloWorldMainApplication.class,args);
  }
}
</code></pre>
<h3 id="4-编写相关的controller-service">4、编写相关的Controller、Service</h3>
<pre><code class="language-java">@Controller
public class HelloController{
  @ResponseBody
  @RequestMapping(&quot;/hello&quot;)
  public String hello(){
    return &quot;Hello World&quot;;
  }
}
</code></pre>
<h3 id="5-运行主程序测试">5、运行主程序测试</h3>
<h3 id="6-简化部署">6、简化部署</h3>
<pre><code class="language-xml">&lt;!-- 这个插件，可以将应用打包成一个可执行的jar包；--&gt;
&lt;build&gt;
  &lt;plugins&gt;
    &lt;plugin&gt;
      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
      &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;
    &lt;/plugin&gt;
  &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>
<p>将这个应用打成jar包，直接使用java -jar的命令进行执行；</p>
<h2 id="5-hello-world-探究">5、Hello World 探究</h2>
<h3 id="1-pom文件">1、POM文件</h3>
<h4 id="1-父项目">1、父项目</h4>
<pre><code class="language-xml">&lt;parent&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
  &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;
&lt;/parent&gt;

他的父项目是
&lt;parent&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt;
  &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;
  &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt;
&lt;/parent&gt;
他来真正管理Spring Boot应用里面的所有依赖版本；
</code></pre>
<p>Spring Boot的版本仲裁中心;</p>
<p>以后我们导入依赖默认是不需要写版本的;(没有在dependencies里面管理的依赖自然需要声明版本号)</p>
<h4 id="2-启动器">2、启动器</h4>
<pre><code class="language-xml">&lt;dependency&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
&lt;/dependency&gt;
</code></pre>
<p><strong>spring-boot-starter-web</strong></p>
<p>spring-boot-starter：spring-boot场景启动器；帮我们导入了web模块正常运行所依赖的组件；</p>
<p>Spring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器</p>
<h3 id="2-主程序类主入口类">2、主程序类，主入口类</h3>
<pre><code class="language-java">/**
 *  @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用
 */
@SpringBootApplication
public class HelloWorldMainApplication {
  public static void main(String[] args) {
    // Spring应用启动起来
    SpringApplication.run(HelloWorldMainApplication.class,args);
  }
}
</code></pre>
<p>@<strong>SpringBootApplication</strong>:    Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法来启动SpringBoot应用；</p>
<pre><code class="language-java">@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Inherited
@SpringBootConfiguration
@EnableAutoConfiguration
@ComponentScan(excludeFilters = {
  @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),
  @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })
public @interface SpringBootApplication {}
</code></pre>
<ul>
<li>
<p>@<strong>SpringBootConfiguration</strong>:Spring Boot的配置类；</p>
<ul>
<li>标注在某个类上，表示这是一个Spring Boot的配置类；</li>
<li>@<strong>Configuration</strong>:配置类上来标注这个注解；
<ul>
<li>配置类 -----  配置文件；配置类也是容器中的一个组件；@Component</li>
</ul>
</li>
</ul>
</li>
<li>
<p>@<strong>EnableAutoConfiguration</strong>：开启自动配置功能；</p>
<ul>
<li>
<p>以前我们需要配置的东西，Spring Boot帮我们自动配置；@<strong>EnableAutoConfiguration</strong>告诉SpringBoot开启自动配置功能；这样自动配置才能生效；</p>
<pre><code class="language-java">@AutoConfigurationPackage
@Import(EnableAutoConfigurationImportSelector.class)
public @interface EnableAutoConfiguration {}
</code></pre>
<ul>
<li>
<p>@<strong>AutoConfigurationPackage</strong>：自动配置包</p>
</li>
<li>
<p>@<strong>Import</strong>(AutoConfigurationPackages.Registrar.class)：</p>
<ul>
<li>
<p>Spring的底层注解@Import，给容器中导入一个组件；导入的组件由AutoConfigurationPackages.Registrar.class来实现</p>
</li>
<li>
<p>将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器；</p>
</li>
<li>
<p>@<strong>Import</strong>(EnableAutoConfigurationImportSelector.class)；</p>
<p>给容器导入组件</p>
<ul>
<li>
<p><strong>EnableAutoConfigurationImportSelector</strong>：导入哪些组件的选择器；</p>
</li>
<li>
<p>将所有需要导入的组件以全类名的方式返回；这些组件就会被添加到容器中；</p>
</li>
<li>
<p>会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；就是给容器中导入这个场景需要的所有组件，并配置好这些组件；		<img src="https://Zu3zz.github.io/post-images/1570438503004.png" alt="自动配置类"></p>
</li>
<li>
<p>有了自动配置类，免去了我们手动编写配置注入功能组件等的工作；</p>
</li>
</ul>
<pre><code class="language-java">  SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class,classLoader)
</code></pre>
<ul>
<li>
<p>Spring Boot在启动的时候从类路径下的META-INF/spring.factories中获取<strong>EnableAutoConfiguration</strong>指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作；<img src="https://Zu3zz.github.io/post-images/1570438722722.png" alt="SpringFactories"></p>
</li>
<li>
<p>以前我们需要自己配置的东西，现在自动配置类都帮我们实现了；</p>
</li>
<li>
<p>J2EE的整体整合解决方案和自动配置都在spring-boot-autoconfigure-2.x.x.RELEASE.jar；</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="6-使用spring-initializer快速创建spring-boot项目">6、使用Spring Initializer快速创建Spring Boot项目</h2>
<h3 id="1-idea-使用spring-initializer快速创建项目">1、IDEA: 使用Spring Initializer快速创建项目</h3>
<ul>
<li>IDE都支持使用Spring的项目创建想到快速创建一个Spring Boot项目；</li>
<li>选择我们需要的模块；向导会联网创建Spring Boot项目；</li>
<li>默认生成的Spring Boot项目：
<ul>
<li>主程序已经写好，我们只需要写自己的逻辑</li>
<li>resources文件夹中的目录结构
<ul>
<li>static: 保存所有的静态资源：js、css、images</li>
<li>templates：保存所有的模板页面；(Spring Boot默认jar包使用嵌入式的Tomcat， 默认不支持JSP页面)；可以使用模板引擎（freemarker、thymeleaf）；</li>
<li>application.properties：Spring Boot应用的配置文件；可以修改一些默认配置</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-sts使用-spring-starter-project快速创建项目">2、STS使用 Spring Starter Project快速创建项目</h3>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeetCode-double-week-10]]></title>
        <id>https://Zu3zz.github.io/post/leetcode-double-week-10</id>
        <link href="https://Zu3zz.github.io/post/leetcode-double-week-10">
        </link>
        <updated>2019-10-06T11:56:00.000Z</updated>
        <summary type="html"><![CDATA[<center>又到了每周拼手速的时刻了！(大雾</center>
<center>👏👏👏👏👏👏👏👏👏👏👏👏</center>
<center>这周题目难度适中 冲！</center>]]></summary>
        <content type="html"><![CDATA[<center>又到了每周拼手速的时刻了！(大雾</center>
<center>👏👏👏👏👏👏👏👏👏👏👏👏</center>
<center>这周题目难度适中 冲！</center>
<!-- more -->
<h2 id="第一题">第一题</h2>
<h3 id="5079三个有序数组的交集">5079.三个有序数组的交集</h3>
<ul>
<li>
<p>给出三个均为 严格递增排列 的整数数组 arr1，arr2 和 arr3。</p>
</li>
<li>
<p>返回一个由 仅 在这三个数组中 同时出现 的整数所构成的有序数组。</p>
</li>
</ul>
<ol>
<li>示例：</li>
</ol>
<pre><code class="language-shell">输入: arr1 = [1,2,3,4,5], arr2 = [1,2,5,7,9], arr3 = [1,3,4,5,8]
输出: [1,5]
解释: 只有 1 和 5 同时在这三个数组中出现.
</code></pre>
<h3 id="思路">思路</h3>
<p>直接创建一个比较大的数组，遍历三遍，每次遍历到的数字都在相应位置上+1，如果数组中有值为3，说明三个数组中都有<br>
时间复杂度O(n) 比较快<br>
当然也可以直接循环一遍查看是否这个数都在两个数组中，但是这样就是O(nlogn)，会相对慢一些</p>
<h3 id="python-solution">Python solution</h3>
<pre><code class="language-python">class Solution:
    def arraysIntersection(self, arr1: List[int], arr2: List[int], arr3: List[int]) -&gt; List[int]:
        res = [0] * 2010
        res1 = []
        for i in arr1:
            res[i]+=1
        for i in arr2:
            res[i]+=1
        for i in arr3:
            res[i]+=1
        for j in range(len(res)):
            if res[j] == 3:
                res1.append(j)
        return res1
</code></pre>
<h2 id="第二题">第二题</h2>
<h3 id="5080-查找两棵二叉搜索树之和">5080. 查找两棵二叉搜索树之和</h3>
<ul>
<li>
<p>给出两棵二叉搜索树，请你从两棵树中各找出一个节点，使得这两个节点的值之和等于目标值 Target。</p>
</li>
<li>
<p>如果可以找到返回 True，否则返回 False。</p>
</li>
</ul>
<ol>
<li>示例 1：</li>
</ol>
<pre><code class="language-shell">输入：root1 = [2,1,4], root2 = [1,0,3], target = 5
输出：true
解释：2 加 3 和为 5 。
</code></pre>
<ol start="2">
<li>示例 2：</li>
</ol>
<pre><code class="language-shell">输入：root1 = [0,-10,10], root2 = [5,1,7,0,2], target = 18
输出：false
</code></pre>
<h3 id="思路-2">思路</h3>
<p>暂时没有想到更好的了<br>
首先，把两个二叉搜索树变成set，所使用到的函数就如上图t2s和_t2s所示。然后在两个set中进行类似twosum的操作<br>
但是可以利用二叉搜索树的性质，即所有左子树的节点值都小于根节点，右子树的节点的值都大于根节点。但是这道题好像这样做就麻烦了，索性转成数组省事一点。</p>
<h3 id="python-solution-2">Python Solution</h3>
<pre><code class="language-python"># Definition for a binary tree node.
# class TreeNode:
#     def __init__(self, x):
#         self.val = x
#         self.left = None
#         self.right = None

class Solution:
    def _t2s(self, root, s):
        if root:
            s.add(root.val)
            self._t2s(root.left, s)
            self._t2s(root.right, s)
    def t2s(self, root):
        s = set()
        self._t2s(root,s)
        return s
    def twoSumBSTs(self, root1: TreeNode, root2: TreeNode, target: int) -&gt; bool:
        t1 = self.t2s(root1)
        t2 = self.t2s(root2)
        for i in t1:
            if target - i in t2:
                return True
        return False
</code></pre>
<h2 id="第三题">第三题</h2>
<h3 id="5081-步进数">5081. 步进数</h3>
<ul>
<li>
<p>如果一个整数上的每一位数字与其相邻位上的数字的绝对差都是 1，那么这个数就是一个「步进数」。</p>
</li>
<li>
<p>例如， <strong>321</strong> 是一个步进数，而 <strong>421</strong> 不是。</p>
</li>
<li>
<p>给你两个整数，low 和 high，请你找出在 [low, high] 范围内的所有步进数，并返回 排序后 的结果。</p>
</li>
</ul>
<ol>
<li>实例：</li>
</ol>
<pre><code class="language-shell">输入：low = 0, high = 21
输出：[0,1,2,3,4,5,6,7,8,9,10,12,21]
</code></pre>
<h3 id="思路-3">思路</h3>
<p>找出从low到high的范围里满足进步数要求的数字。<br>
使用一个queue来存储所有数字，具体实现就是从1开始，queue中添加10，01；对于2，添加12，21.....<br>
并且要满足所有添加的数字小于high.<br>
最后sorted一遍就可以了<br>
不知道python里的queue咋用，还是用回老本行cpp吧。</p>
<h3 id="cpp-solution">Cpp Solution</h3>
<pre><code class="language-cpp">class Solution {
public:
    vector&lt;int&gt; countSteppingNumbers(int low, int high) {
        vector&lt;int&gt; res;
        queue&lt;int&gt; q;
        int i,j;
        for(i=1;i&lt;10&amp;&amp;i&lt;=high;i++)q.push(i);
        if(!low)res.push_back(0);
        while(!q.empty())
        {
            i=q.front();
            q.pop();
            if(i&gt;=low) res.push_back(i);
            j=i%10;
            if(j&amp;&amp;i*10LL+j-1&lt;=high)q.push(i*10+j-1);
            if(j&lt;9&amp;&amp;i*10LL+j+1&lt;=high)q.push(i*10+j+1);
        }
        sort(res.begin(),res.end());
        return res;
    }
};
</code></pre>
<h2 id="第四题">第四题</h2>
<h3 id="5099验证回文字符串iii">5099.验证回文字符串III</h3>
<ul>
<li>
<p>给出一个字符串 s 和一个整数 k，请你帮忙判断这个字符串是不是一个「K 回文」。</p>
</li>
<li>
<p>所谓「K 回文」：如果可以通过从字符串中删去最多 k 个字符将其转换为回文，那么这个字符串就是一个「K 回文」。</p>
</li>
</ul>
<ol>
<li>示例：</li>
</ol>
<pre><code class="language-shell">输入：s = &quot;abcdeca&quot;, k = 2
输出：true
解释：删除字符 “b” 和 “e”。
</code></pre>
<h3 id="思路-4">思路</h3>
<p>首先先把给定的字符串s逆序得到s2,把s与s2进行比较。<br>
dp方程就是用来记录到了最后一位，s与s2有多少位能够相等，如果加上了k还能够大于字符串本身的长度，那么就是“K回文”。<br>
用python写好像很方便 索性又换回python了</p>
<h3 id="python-solution-3">Python Solution</h3>
<pre><code class="language-python">class Solution:
    def isValidPalindrome(self, s: str, k: int) -&gt; bool:
        s2 = s[::-1]
        n = len(s)
        dp = [[0] * (n+1) for i in range(n+1)]
        for i in range(1,n+1):
            for j in range(1,n+1):
                if s[i-1] == s2[j-1]:
                    dp[i][j] = dp[i-1][j-1] +1
                else:
                    dp[i][j] = max(dp[i-1][j],dp[i][j-1])
        if dp[-1][-1] +k &gt;= n:
            return True
        return False
</code></pre>
<h3 id="总结">总结</h3>
<p>每周的双周赛难度适中，比较适合新手打。不过这一周香港局势动荡，学校也停课了，在家呆着真窝心啊。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDFS系列(1)----安装]]></title>
        <id>https://Zu3zz.github.io/post/hdfs-3</id>
        <link href="https://Zu3zz.github.io/post/hdfs-3">
        </link>
        <updated>2019-09-13T10:54:04.000Z</updated>
        <summary type="html"><![CDATA[<center>这篇文章咱们就来讲讲如何安装HDFS</center>]]></summary>
        <content type="html"><![CDATA[<center>这篇文章咱们就来讲讲如何安装HDFS</center>
<!-- more -->
<h1 id="hdfs-概述">HDFS 概述</h1>
<ol>
<li>分布式</li>
<li>commodity hardware</li>
<li>fault-tolerant 容错</li>
<li>high throughput</li>
<li>large data sets</li>
</ol>
<h2 id="hdfs-是一个分布式的文件系统">HDFS 是一个分布式的文件系统</h2>
<ul>
<li>文件系统：Linux、Windows、Mac....
<ul>
<li>目录结构: C /</li>
<li>存放的是文件或者文件夹</li>
<li>对外提供服务：创建、修改、删除、查看、移动等等</li>
<li>普通文件系统 vs 分布式文件系统
<ul>
<li>单机</li>
<li>分布式文件系统能够横跨 N 个机器</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="hdfs-前提和设计目标">HDFS 前提和设计目标</h2>
<ul>
<li>Hardware Failure 硬件错误
<ul>
<li>每个机器只存储文件的部分数据，blocksize=128M</li>
<li>block 存放在不同的机器上的，由于容错，HDFS 默认采用 3 副本机制</li>
</ul>
</li>
<li>Streaming Data Access 流式数据访问
<ul>
<li>The emphasis is on high throughput of data access</li>
<li>rather than low latency of data access.</li>
</ul>
</li>
<li>Large Data Sets 大规模数据集</li>
<li>Moving Computation is Cheaper than Moving Data 移动计算比移动数据更划算</li>
</ul>
<h2 id="hdfs-的架构">HDFS 的架构</h2>
<ol>
<li>NameNode(master) and DataNodes(slave)</li>
<li>master/slave 的架构</li>
<li>NN:</li>
</ol>
<ul>
<li>the file system namespace
<ul>
<li>/home/hadoop/software</li>
<li>/home/hadoop/app</li>
<li>regulates access to files by clients</li>
</ul>
</li>
</ul>
<ol start="4">
<li>
<p>DN：storage</p>
</li>
<li>
<p>HDFS exposes a file system namespace and allows user data to be stored in files.</p>
</li>
<li>
<p>a file is split into one or more blocks</p>
<ul>
<li>blocksize: 128M</li>
<li>150M 拆成 2 个 block</li>
</ul>
</li>
<li>
<p>blocks are stored in a set of DataNodes</p>
<ul>
<li>为什么？ 容错！！！</li>
</ul>
</li>
<li>
<p>NameNode executes file system namespace operations：CRUD</p>
</li>
<li>
<p>determines the mapping of blocks to DataNodes</p>
<ul>
<li>
<p>a.txt 150M blocksize=128M</p>
</li>
<li>
<p>a.txt 拆分成 2 个 block 一个是 block1：128M 另一个是 block2：22M<br>
block1 存放在哪个 DN？block2 存放在哪个 DN？</p>
</li>
<li>
<p>a.txt<br>
_ block1：128M, 192.168.199.1<br>
_ block2：22M, 192.168.199.2<br>
_ get a.txt<br>
_ 这个过程对于用户来说是不感知的</p>
</li>
</ul>
</li>
<li>
<p>通常情况下：1 个 Node 部署一个组件</p>
</li>
</ol>
<h2 id="课程环境介绍">课程环境介绍：</h2>
<h3 id="本课程录制的系统是-mac所以我采用的-linux-客户端是-mac-自带的-shell">本课程录制的系统是 Mac，所以我采用的 linux 客户端是 mac 自带的 shell</h3>
<ul>
<li>如果你们是 win：xshell、crt</li>
<li>服务器/linux 地址：192.168.199.233</li>
<li>连接到 linux 环境<br>
_ 登陆：ssh hadoop@192.168.199.233<br>
_ 登陆成功以后：[hadoop@hadoop000 ~]$
<ul>
<li>linux机器：用户名hadoop、密码123456、hostname是* * hadoop000</li>
<li>创建课程中所需要的目录（合适的文件存放在合适的目录）
<ol>
<li>[hadoop@hadoop000 ~]$ mkdir software 存放课程所使用的软件安装包 2. [hadoop@hadoop000 ~]$ mkdir app       存放课程所有软件的安装目录</li>
<li>[hadoop@hadoop000 ~]$ mkdir data 存放课程中使用的数据 4. [hadoop@hadoop000 ~]$ mkdir lib       存放课程中开发过的作业jar存放的目录</li>
<li>[hadoop@hadoop000 ~]$ mkdir shell 存放课程中相关的脚本 6. [hadoop@hadoop000 ~]$ mkdir maven_resp 存放课程中使用到的 maven 依赖包存放的目录</li>
</ol>
</li>
</ul>
</li>
<li>学员问：root 密码 1. 切换 hadoop 到 root 用户：[hadoop@hadoop000 ~]$ sudo -i 2. 切换 root 到 hadoop 用户：[root@hadoop000 ~]# su hadoop 3. 我 OOTB 环境中创建的 hadoop 用户是有 sudo 权限：sudo vi /etc/hosts</li>
<li>Linux 版本： * 以前的课程是 centos6.4，本次课程升级成 centos7</li>
</ul>
<h3 id="hadoop-环境搭建">Hadoop 环境搭建</h3>
<ul>
<li>使用的 Hadoop 相关版本：CDH</li>
<li>CDH 相关软件包下载地址：http://archive.cloudera.com/cdh5/cdh/5/</li>
<li>Hadoop 使用版本：hadoop-2.6.0-cdh5.15.1</li>
<li>Hadoop 下载：wget http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</li>
<li>Hive 使用版本：hive-1.1.0-cdh5.15.1</li>
</ul>
<h3 id="hadoophivespark-相关框架的学习">Hadoop/Hive/Spark 相关框架的学习：</h3>
<ul>
<li>
<p>使用单机版足够</p>
</li>
<li>
<p>如果使用集群学习会导致：从入门到放弃</p>
</li>
<li>
<p>使用 Linux/Mac 学习</p>
</li>
<li>
<p>一定不要使用 Windows 搭建 Hadoop 环境</p>
</li>
<li>
<p>所以 Linux 基础是要会的</p>
</li>
</ul>
<h3 id="hadoop-安装前置要求">Hadoop 安装前置要求</h3>
<ul>
<li>Java 1.8+</li>
<li>ssh</li>
</ul>
<h3 id="安装-java">安装 Java</h3>
<ul>
<li>拷贝本地软件包到服务器：scp jdk-8u91-linux-x64.tar.gz hadoop@192.168.199.233:~/software/</li>
<li>解压 jdk 到~/app/：tar -zvxf jdk-8u91-linux-x64.tar.gz -C ~/app/</li>
<li>把 jdk 配置系统环境变量中： ~/.bash_profile - export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91 - export PATH=<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>J</mi><mi>A</mi><mi>V</mi><msub><mi>A</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi><mi mathvariant="normal">/</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>:</mo></mrow><annotation encoding="application/x-tex">JAVA_HOME/bin:</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.09618em;">J</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord">/</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span></span></span></span>PATH</li>
<li>使得配置修改生效：source .bash_profile</li>
<li>验证：java -version</li>
</ul>
<h3 id="安装-ssh-无密码登陆">安装 ssh 无密码登陆</h3>
<ul>
<li>
<p>ls</p>
</li>
<li>
<p>ls -a</p>
</li>
<li>
<p>ls -la 并没有发现一个.ssh 的文件夹</p>
</li>
<li>
<p>ssh-keygen -t rsa 一路回车</p>
</li>
<li>
<p>cd ~/.ssh</p>
</li>
<li>
<p>[hadoop@hadoop000 .ssh]$ ll</p>
</li>
<li>
<p>总用量 12</p>
<pre><code class="language-shell">-rw------- 1 hadoop hadoop 1679 10月 15 02:54 id_rsa  私钥
-rw-r--r-- 1 hadoop hadoop  398 10月 15 02:54 id_rsa.pub 公钥
-rw-r--r-- 1 hadoop hadoop  358 10月 15 02:54 known_hosts
cat id_rsa.pub &gt;&gt; authorized_keys
chmod 600 authorized_keys
</code></pre>
</li>
</ul>
<h3 id="hadoophdfs安装">Hadoop(HDFS)安装</h3>
<ul>
<li>
<p>下载<br>
_ 解压：~/app<br>
_ 添加 HADOOP_HOME/bin 到系统环境变量<br>
_ 修改 Hadoop 配置文件<br>
_ hadoop-env.sh<br>
_ export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91<br>
_ core-site.xml</p>
<pre><code class="language-xml">  &lt;property&gt;
      &lt;name&gt;fs.defaultFS&lt;/name&gt;
      &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
  &lt;/property&gt;

  hdfs-site.xml
  &lt;property&gt;
      &lt;name&gt;dfs.replication&lt;/name&gt;
      &lt;value&gt;1&lt;/value&gt;
  &lt;/property&gt;

  &lt;property&gt;
      &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
      &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
<ul>
<li>slaves<br>
_ hadoop000<br>
_ 启动 HDFS：<br>
_ 第一次执行的时候一定要格式化文件系统，不要重复执行: hdfs namenode -format<br>
_ 启动集群：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mi>A</mi><mi>D</mi><mi>O</mi><mi>O</mi><msub><mi>P</mi><mi>H</mi></msub><mi>O</mi><mi>M</mi><mi>E</mi><mi mathvariant="normal">/</mi><mi>s</mi><mi>b</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">/</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>t</mi><mo>−</mo><mi>d</mi><mi>f</mi><mi>s</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>h</mi><mo>∗</mo><mi mathvariant="normal">验</mi><mi mathvariant="normal">证</mi><mo>:</mo><mn>1.</mn><mo>[</mo><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mi mathvariant="normal">@</mi><mi>h</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>o</mi><mi>p</mi><mn>000</mn><mi>s</mi><mi>b</mi><mi>i</mi><mi>n</mi><mo>]</mo></mrow><annotation encoding="application/x-tex">HADOOP_HOME/sbin/start-dfs.sh
	* 验证:
		1. [hadoop@hadoop000 sbin]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">A</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.08125em;">H</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord">/</span><span class="mord mathdefault">s</span><span class="mord mathdefault">t</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">d</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">s</span><span class="mord">.</span><span class="mord mathdefault">s</span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord cjk_fallback">验</span><span class="mord cjk_fallback">证</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mopen">[</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">@</span><span class="mord mathdefault">h</span><span class="mord mathdefault">a</span><span class="mord mathdefault">d</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault">p</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord mathdefault">s</span><span class="mord mathdefault">b</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mclose">]</span></span></span></span> jps 2. 60002 DataNode 3. 60171 SecondaryNameNode 4. 59870 NameNode 5. http://192.168.199.233:50070 6. 如果发现 jps ok，但是浏览器不 OK？ 十有八九是防火墙问题 7. 查看防火墙状态：sudo firewall-cmd --state 8. 关闭防火墙: sudo systemctl stop firewalld.service 9. 进制防火墙开机启动：</li>
</ul>
</li>
</ul>
<h3 id="hadoop-软件包常见目录说明">hadoop 软件包常见目录说明</h3>
<ul>
<li>bin：hadoop 客户端名单</li>
<li>etc/hadoop：hadoop 相关的配置文件存放目录</li>
<li>sbin：启动 hadoop 相关进程的脚本</li>
<li>share：常用例子</li>
</ul>
<h3 id="注意">注意：</h3>
<ul>
<li>
<p>start/stop-dfs.sh 与 hadoop-daemons.sh 的关系</p>
</li>
<li>
<p>start-dfs.sh</p>
<pre><code class="language-shell">hadoop-daemons.sh start namenode
hadoop-daemons.sh start datanode
hadoop-daemons.sh start secondarynamenode
</code></pre>
</li>
<li>
<p>stop-dfs.sh =</p>
</li>
<li>
<p>hadoop 常用命令：</p>
<ol>
<li>hadoop fs -ls /</li>
<li>hadoop fs -put</li>
<li>hadoop fs -copyFromLocal</li>
<li>hadoop fs -moveFromLocal</li>
<li>hadoop fs -cat</li>
<li>hadoop fs -text</li>
<li>hadoop fs -get</li>
<li>hadoop fs -mkdir</li>
<li>hadoop fs -mv 移动/改名</li>
<li>hadoop fs -getmerge</li>
<li>hadoop fs -rm</li>
<li>hadoop fs -rmdir</li>
<li>hadoop fs -rm -r</li>
</ol>
</li>
</ul>
<h3 id="hdfs-存储扩展">HDFS 存储扩展：</h3>
<ul>
<li>put: 1file ==&gt; 1...n block ==&gt; 存放在不同的节点上的</li>
<li>get: 去 nn 上查找这个 file 对应的元数据信息</li>
<li>了解底层的存储机制这才是我们真正要学习的东西，掌握 API 那是毛毛雨</li>
</ul>
<h3 id="使用-hdfs-api-的方式来操作-hdfs-文件系统">使用 HDFS API 的方式来操作 HDFS 文件系统</h3>
<ul>
<li>IDEA/Eclipse</li>
<li>Java
<ul>
<li>使用 Maven 来管理项目</li>
<li>拷贝 jar 包</li>
<li>我的所有课程都是使用 maven 来进行管理的</li>
</ul>
</li>
</ul>
<pre><code class="language-shell">Caused by: org.apache.hadoop.ipc.RemoteException
(org.apache.hadoop.security.AccessControlException):
Permission denied: user=rocky, access=WRITE,
inode=&quot;/&quot;:hadoop:supergroup:drwxr-xr-x
</code></pre>
<h3 id="hdfs-操作shell-java-api">HDFS 操作：shell + Java API</h3>
<ul>
<li>综合性的 HDFS 实战：使用 HDFS Java API 才完成 HDFS 文件系统上的文件的词频统计</li>
<li>词频统计：wordcount
<ul>
<li>/path/1.txt</li>
<li>hello world hello</li>
<li>/path/2.txt</li>
<li>hello world hello
<ul>
<li>==&gt; (hello,4) (world,2)</li>
</ul>
</li>
</ul>
</li>
<li>将统计完的结果输出到 HDFS 上去。</li>
</ul>
<h3 id="假设有的小伙伴了解过-mr-spark-等等觉得这个操作很简单">假设：有的小伙伴了解过 mr、spark 等等，觉得这个操作很简单</h3>
<h3 id="本实战的要求只允许使用-hdfs-api-进行操作">本实战的要求：只允许使用 HDFS API 进行操作</h3>
<h3 id="目的">目的</h3>
<ol>
<li>掌握 HDFS API 的操作</li>
<li>通过这个案例，让你们对于后续要学习的 mr 有一个比较好的认识</li>
</ol>
<h3 id="硬编码-非常忌讳的">硬编码 ： 非常忌讳的</h3>
<h3 id="可配置">==&gt; 可配置</h3>
<h3 id="可插拔的开发管理方式-plugin">可插拔的开发/管理方式 plugin</h3>
<h3 id="副本摆放策略">副本摆放策略</h3>
<ul>
<li>
<p>1-本 rack 的一个节点上</p>
</li>
<li>
<p>2-另外一个 rack 的节点上</p>
</li>
<li>
<p>3-与 2 相同的 rack 的另外一个节点上</p>
</li>
<li>
<p>1-本 rack 的一个节点上</p>
</li>
<li>
<p>2-本 rack 的另外一个节点上</p>
</li>
<li>
<p>3-不同 rack 的一个节点上</p>
</li>
</ul>
<h3 id="hdfs-的元数据管理">HDFS 的元数据管理</h3>
<ul>
<li>元数据：HDFS 的目录结构以及每个文件的 BLOCK 信息(id，副本系数、block 存放在哪个 DN 上)</li>
<li>存在什么地方：对应配置 ${hadoop.tmp.dir}/name/......</li>
<li>元数据存放在文件中：</li>
</ul>
<pre><code class="language-shell">/test1
/test1/a.txt
/test2
/test2/1.txt
/test2/2.txt
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yarn系列(1)----基础、介绍、流程]]></title>
        <id>https://Zu3zz.github.io/post/yarn-1</id>
        <link href="https://Zu3zz.github.io/post/yarn-1">
        </link>
        <updated>2019-09-10T07:31:48.000Z</updated>
        <summary type="html"><![CDATA[<center>简单介绍一下资源调度工具YARN</center>
<center>Yet Another Resource Negotiator</center>]]></summary>
        <content type="html"><![CDATA[<center>简单介绍一下资源调度工具YARN</center>
<center>Yet Another Resource Negotiator</center>
<!-- more -->
<h1 id="yarn-产生背景">YARN 产生背景</h1>
<ul>
<li>
<p>MapReduce1.x ==&gt; MapReduce2.x</p>
<ul>
<li>master/slave : JobTracker/TaskTracker</li>
<li>JobTracker：单点、压力大</li>
<li>仅仅只能够支持 mapreduce 作业</li>
</ul>
</li>
<li>
<p>资源利用率</p>
<ul>
<li>所有的计算框架运行在一个集群中，共享一个集群的资源，按需分配！</li>
</ul>
</li>
</ul>
<pre><code class="language-txt">master: resource management：ResourceManager (RM)
job scheduling/monitoring：per-application ApplicationMaster (AM)
slave: NodeManager (NM)
</code></pre>
<h2 id="yarn-架构">YARN 架构</h2>
<ul>
<li>Client、ResourceManager、NodeManager、ApplicationMaster</li>
<li>master/slave: RM/NM</li>
</ul>
<h2 id="client-向-rm-提交任务-杀死任务等">Client: 向 RM 提交任务、杀死任务等</h2>
<ul>
<li>
<p>ApplicationMaster：</p>
<ul>
<li>每个应用程序对应一个 AM</li>
<li>AM 向 RM 申请资源用于在 NM 上启动对应的 Task</li>
<li>数据切分</li>
<li>为每个 task 向 RM 申请资源（container）</li>
<li>NodeManager 通信</li>
<li>任务的监控</li>
</ul>
</li>
<li>
<p>NodeManager： 多个</p>
<ul>
<li>干活</li>
<li>向 RM 发送心跳信息、任务的执行情况</li>
<li>接收来自 RM 的请求来启动任务</li>
<li>处理来自 AM 的命令</li>
</ul>
</li>
<li>
<p>ResourceManager:集群中同一时刻对外提供服务的只有 1 个，负责资源相关</p>
<ul>
<li>处理来自客户端的请求：提交、杀死</li>
<li>启动/监控 AM</li>
<li>监控 NM</li>
<li>资源相关</li>
</ul>
</li>
<li>
<p>container：任务的运行抽象</p>
<ul>
<li>memory、cpu....</li>
<li>task 是运行在 container 里面的</li>
<li>可以运行 am、也可以运行 map/reduce task</li>
</ul>
</li>
</ul>
<h2 id="yarn-执行流程">yarn 执行流程</h2>
<figure data-type="image" tabindex="1"><img src="https://Zu3zz.github.io/post-images/1568100871914.png" alt="yarn执行流程"></figure>
<h2 id="提交自己开发的-mr-作业到-yarn-上运行的步骤">提交自己开发的 MR 作业到 YARN 上运行的步骤：</h2>
<ol>
<li>mvn clean package -DskipTests 打包 jar 包<br>
windows/Mac/Linux ==&gt; Maven</li>
<li>把编译出来的 jar 包(项目根目录/target/...jar)以及测试数据上传到服务器<br>
scp xxxx hadoop@hostname:directory</li>
<li>把数据上传到 HDFS<br>
hadoop fs -put xxx hdfspath</li>
<li>执行作业<br>
hadoop jar xxx.jar 完整的类名(包名+类名) args.....</li>
<li>到 YARN UI(8088) 上去观察作业的运行情况</li>
<li>到输出目录去查看对应的输出结果</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hadoop系列(1)----多集群配置流程]]></title>
        <id>https://Zu3zz.github.io/post/hadoop-1</id>
        <link href="https://Zu3zz.github.io/post/hadoop-1">
        </link>
        <updated>2019-09-09T13:12:27.000Z</updated>
        <summary type="html"><![CDATA[<center>终于到了激动人心的伪(划掉分布式环节 如何将HDFS配置在多机器的集群上</center>
<center>这篇文章会详细告诉你~</center>]]></summary>
        <content type="html"><![CDATA[<center>终于到了激动人心的伪(划掉分布式环节 如何将HDFS配置在多机器的集群上</center>
<center>这篇文章会详细告诉你~</center>
<!-- more -->
<h1 id="hadoop-集群规划">Hadoop 集群规划</h1>
<ul>
<li>
<p>HDFS: NN DN</p>
</li>
<li>
<p>YARN: RM NM</p>
</li>
<li>
<p>hadoop000 192.168.199.234</p>
<ul>
<li>NN RM</li>
<li>DN NM</li>
</ul>
</li>
<li>
<p>hadoop001 192.168.199.235</p>
<ul>
<li>DN NM</li>
</ul>
</li>
<li>
<p>hadoop002 192.168.199.236</p>
<ul>
<li>DN NM</li>
</ul>
</li>
</ul>
<h2 id="详细步骤">详细步骤</h2>
<ul>
<li>
<p>对每台机器</p>
<ul>
<li>
<p>修改 host 配置</p>
<ul>
<li>
<p>在/etc/hostname 下修改 hostname(hadoop000/hadoop001/hadoop002)</p>
</li>
<li>
<p>在/etc/hosts 下修改 ip 和 hostname 的映射关系</p>
<ol>
<li>192.168.199.234 hadoop000</li>
<li>192.168.199.235 hadoop001</li>
<li>192.168.199.236 hadoop002</li>
<li>192.168.199.23x localhost(试机器而定)</li>
</ol>
</li>
</ul>
</li>
<li>
<p>前置安装 ssh 进行免密码登录操作</p>
</li>
</ul>
<pre><code class="language-shell">ssh-keygen -t rsa
</code></pre>
<p>在每台 hadoop 机器上进行操作</p>
<pre><code class="language-shell">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001
ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002
</code></pre>
<ul>
<li>
<p>安装 java jkd</p>
<ul>
<li>首先在每台 hadoop 集群机器上部署 jdk</li>
<li>将 jkd bin 配置到系统环境变量(~/.bash_profile)</li>
<li>将 jdk 以及环境变量配置拷贝到其他节点上去(start with hadoop000)</li>
</ul>
<pre><code class="language-shell">scp -r jdk1.8.0_91 hadoop@hadoop001:~/app/
scp -r jdk1.8.0_91 hadoop@hadoop002:~/app/

scp ~/.bash_profile hadoop@hadoop001:~/
scp ~/.bash_profile hadoop@hadoop002:~/
</code></pre>
</li>
<li>
<p>Hadoop 部署</p>
<ul>
<li>hadoop-env.sh 配置: JAVA_HOME</li>
<li>hdfs-site.xml 配置:</li>
</ul>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/app/tmp/dfs/name&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
  &lt;value&gt;/home/hadoop/app/tmp/dfs/data&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>yarn-site.xml</li>
</ul>
<pre><code class="language-xml">&lt;!--只在hadoop000机器中进行配置--&gt;
&lt;property&gt;
  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;hadoop000&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>mapred-site.xml: 这个文件只有模板 需要自己创建</li>
</ul>
<pre><code class="language-xml">&lt;property&gt;
  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
  &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>配置 slaves</li>
<li>分发 hadoop 到其他机器</li>
</ul>
<pre><code class="language-shell">scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:~/app/
scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:~/app/

scp ~/.bash_profile hadoop@hadoop001:~/
scp ~/.bash_profile hadoop@hadoop002:~/
</code></pre>
<ul>
<li>NN 格式化</li>
</ul>
<pre><code class="language-shell">hadoop namenode -format
</code></pre>
<ul>
<li>在每个机器上启动 HDFS</li>
<li>在每个机器上启动 YRAN</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hive系列(2)----内部表、外部表、实战]]></title>
        <id>https://Zu3zz.github.io/post/hive-2</id>
        <link href="https://Zu3zz.github.io/post/hive-2">
        </link>
        <updated>2019-09-09T07:48:10.000Z</updated>
        <summary type="html"><![CDATA[<center>这节咱们来点真实的</center>
<center>hive如何创建表以及读取数据</center>]]></summary>
        <content type="html"><![CDATA[<center>这节咱们来点真实的</center>
<center>hive如何创建表以及读取数据</center>
<!-- more -->
<h1 id="hive-外部表-内部表">Hive 外部表、内部表</h1>
<h2 id="内部表">内部表</h2>
<p>可以通过 formatted 查看表的属性</p>
<pre><code class="language-sql">desc formattede mp2

可以看到一系列属性 其中有属性如下

Table Type: MANAGED_TABLE

MANAGED_TABLE就代emp2是一个内部表

删除emp2
drop table emp2;
</code></pre>
<p>删除表: HDFS 上的数据被删除 &amp; Meta 也被删除</p>
<h2 id="外部表">外部表</h2>
<p>创建外部表</p>
<pre><code class="language-sql">CREATE EXTERNNAL TABLE emp_external(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
location '/external/emp/';

此时表是空表 还需要加载数据
LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' ONVERWRITE INTO TABLE emp_external

此时通过desc查看表的属性
Table Type: EXTERNAL_TABLE
是一个外部表

drop table emp_external
</code></pre>
<p>删除表: HDFS 上的数据不被删除 &amp; Meta 上被删除<br>
安全性更好</p>
<h2 id="分区表">分区表</h2>
<p>使用分区表创建</p>
<pre><code class="language-sql">CREATE EXTERNAL TABLE track_info(
ip string,
country string,
province string,
city string,
url string,
time string,
page string
) partitioned by (day string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
location '/project/trackinfo/';
</code></pre>
<p>此时通过执行 jar 包生成 ETL 文件</p>
<p>将这个文件存储到 hive 中</p>
<pre><code class="language-sql">LOAD DATA INPATH 'hdfs://localhost:8020/project/input/etl'
OVERWRITE INTO TABLE track_info partition(day='2013-07-21');

统计总数
select count(*) from track_info where day='2013-07-21';

统计省份的个数
select province,count(*) from track_info where day='2013-07-21' group by province;

为了方便展示 创建一张表用来存储省份的信息
create table track_info_province_stat(
province string,
cnt bigint
) partitioned by (day string)
row format delimited fields terminated by '\t';

通过sql语句直接写入数据
insert overwrite table track_info_province_stat partition(day='2013-07-21') select province, count(*) as cnt from track_info where day='2013-07-21' group by province;

统计页面访问情况
select page,count(*) from track_info where day = '2013-07-21' group by page;

创建一张表用来存储页面访问信息
create table track_info_page_stat(
province string,
cnt bigint
) partitioned by (day string)
row format delimited fields terminated by '\t';

写入数据
insert overwrite table track_info_page_stat partition(day='2013-07-21') select page, count(*) as cnt from track_info where day='2013-07-21' group by page;
</code></pre>
<p>到现在为止，我们统计的数据已经在 Hive 表 track_info_province_stat<br>
而且这个表是一个分区表，后续统计报表的数据可以直接从这个表中查询<br>
也可以将 hive 表的数据导出到 RDBMS（sqoop<br>
总结一下所有的操作</p>
<ol>
<li>ETL</li>
<li>把 ETL 输出的数据加载到 track_info 分区表里</li>
<li>各个维度统计结果的数据输出到各自维度的表里 （如 track_info_province_stat）</li>
<li>将数据导出 (optional)</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hive系列(1)----概念、部署、语法]]></title>
        <id>https://Zu3zz.github.io/post/hive-1</id>
        <link href="https://Zu3zz.github.io/post/hive-1">
        </link>
        <updated>2019-09-07T12:35:32.000Z</updated>
        <summary type="html"><![CDATA[<p>慢慢学习路 一步一脚印<br>
今天带来hive学习笔记系列的第一篇</p>
]]></summary>
        <content type="html"><![CDATA[<p>慢慢学习路 一步一脚印<br>
今天带来hive学习笔记系列的第一篇</p>
<!-- more -->
<h1 id="hive-笔记">Hive 笔记</h1>
<h2 id="hive-概念">Hive 概念</h2>
<ul>
<li>
<p>统一元数据管理:</p>
<ul>
<li>Hive 数据是存放在 HDFS</li>
<li>元数据信息(记录数据的数据)是存放在 MySQL 中</li>
<li>SQL on Hadoop： Hive、Spark SQL、impala....</li>
</ul>
</li>
<li>
<p>Hive 体系架构</p>
<ul>
<li>client: shell、thrift/jdbc(server/jdbc)、WebUI(HUE/Zeppelin)</li>
<li>metastore：==&gt; MySQL<br>
database：name、location、owner....<br>
table：name、location、owner、column name/type ....</li>
</ul>
</li>
<li>
<p>Hive 部署</p>
<ol>
<li>下载（官网）</li>
<li>解压到~/app</li>
<li>添加 HIVE_HOME 到系统环境变量</li>
<li>修改配置<br>
hive-env.sh<br>
hive-site.xml</li>
<li>拷贝 MySQL 驱动包 mysql-java-connector.jar 包到$HIVE_HOME/lib 下</li>
<li>前提是要准备安装一个 MySQL 数据库，利用 yum install 安装一个 MySQL 数据库 https://www.cnblogs.com/julyme/p/5969626.html</li>
</ol>
<pre><code class="language-xml">&lt;!--hive-site.xml配置--&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://hadoop000:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;我是用户名&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;我是密码&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
</li>
</ul>
<h2 id="hive-的-sql-语言">Hive 的 sql 语言</h2>
<h3 id="ddl">DDL</h3>
<p>Hive Data Definition Language</p>
<pre><code class="language-shell">create、delete、alter...
</code></pre>
<p>Hive 数据抽象/结构:</p>
<pre><code class="language-ASCII">+-- database  HDFS一个目录
|   +-- table HDFS一个目录
    |   +-- data  文件
    |   +-- partition 分区表  HDFS一个文件
        |   +-- data  文件
        |   +-- bucket  分桶  HDFS一个文件
</code></pre>
<p>Hive 具体 ddl 操作</p>
<h3 id="创建数据库">创建数据库</h3>
<pre><code class="language-sql">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

CREATE DATABASE IF NOT EXISTS hive;

CREATE DATABASE IF NOT EXISTS hive2 LOCATION '/test/location';

CREATE DATABASE IF NOT EXISTS hive3
WITH DBPROPERTIES('creator'='pk');
</code></pre>
<p>/user/hive/warehouse 是 Hive 默认的存储在 HDFS 上的路径</p>
<h3 id="创建表">创建表</h3>
<pre><code class="language-sql">CREATE TABLE emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
</code></pre>
<h3 id="读取本地数据">读取本地数据</h3>
<pre><code class="language-sql">LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]

LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;
</code></pre>
<p>LOCAL：本地系统，如果没有 local 那么就是指的 HDFS 的路径<br>
OVERWRITE：是否数据覆盖，如果没有那么就是数据追加</p>
<pre><code class="language-sql">LOAD DATA LOCAL INPATH '/home/hadoop/data/emp.txt' OVERWRITE INTO TABLE emp;

LOAD DATA INPATH 'hdfs://hadoop000:8020/data/emp.txt' INTO TABLE emp;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/hive/'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select empno,ename,sal,deptno from emp;
</code></pre>
<h3 id="基本统计">基本统计</h3>
<pre><code class="language-sql">select * from emp where sal between 800 and 1500 (limit(5));

select * from emp where ename in ('SMITH','KING');

select * from emp where sal is (not) null;
</code></pre>
<h3 id="聚合操作">聚合操作</h3>
<p>max/min/sum/avg</p>
<pre><code class="language-sql">select count(1) from emp where deptno = 10;

select max(sal),min(sal),sum(sal),avg(sal) from emp;
</code></pre>
<h3 id="分组函数">分组函数</h3>
<p>group by</p>
<p>出现在 select 中的字段，如果没有出现在聚合函数里，那么一定要实现在 group by 里</p>
<pre><code class="language-sql">求每个部门的平均工资
select deptno, avg(sal) from emp group by deptno;

求每个部门、工作岗位的平均工资
select deptno,job,avg(sal) from emp group by deptno, job;

求每个部门的平均工资大于2000的部门
select deptno, avg(sal) avg_sal from emp group by deptno where avg_sal &gt; 2000; 错误！！！

group by 不能同时和 where 使用
应该使用having替代where

select deptno, avg(sal) abg_sal from emp group by deptno having avg_sal &gt; 2000;
</code></pre>
<h3 id="多表操作">多表操作</h3>
<p>join</p>
<p>两个表:</p>
<ol>
<li>emp</li>
<li>dept</li>
</ol>
<pre><code class="language-sql">创建表
CREATE TABLE dept(
deptno int,
dname string,
loc string
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
加载数据
LOAD DATA LOCAL INPATH '/home/hadoop/data/dept.txt' OVERWRITE INTO TABLE dept;

select
e.empno,e.ename,e.sal,e.deptno,d.name
from emp e join dept d
on e.deptno=d.deptno
</code></pre>
<h4 id="关于-stage-0-stage-3-stage-4">关于 stage-0、stage-3、stage-4</h4>
<p>由于join是一个复杂操作，所以需要分步骤进行查询</p>
<pre><code class="language-sql">explain EXTENDED
select
e.empno,e.ename,e.sal,e.deptno,d.dname
from emp e join dept d
on e.deptno=d.deptno;
</code></pre>
<ul>
<li>stage-4 is root stage</li>
<li>stage-3 depends on stage-4</li>
<li>stage-0 depends on stage-3</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[python基础操作]]></title>
        <id>https://Zu3zz.github.io/post/python-basic</id>
        <link href="https://Zu3zz.github.io/post/python-basic">
        </link>
        <updated>2019-09-04T13:23:26.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  分享Python的一些骚操作</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  分享Python的一些骚操作</p>
<!-- more -->
<h2 id="file">file</h2>
<pre><code class="language-python">file1 = read(&quot;test.txt&quot;)
# 找到当前位置的指针
file1.tell()
# 让文件的指示指针进行偏移 第一个参数代表偏移位置 第二个参数 0代表从文件开头偏移 1代表从当前位置进行偏移 2代表从文件结尾进行偏移
file1.seek(5,0)
</code></pre>
<h2 id="error处理">error处理</h2>
]]></content>
    </entry>
</feed>